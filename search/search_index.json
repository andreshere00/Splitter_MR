{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SplitterMR","text":"<p>SplitterMR is a library for chunking data into convenient text blocks compatible with your LLM applications.</p> <p> </p> <p>Important</p> <p>Version 1.0.0 released \u2013 First Stable Release!</p> <p>We are excited to announce the first stable release of SplitterMR (v1.0.0)! Install it with the following command:</p> <pre><code>pip install splitter-mr\n</code></pre> <p>Highlights:</p> <ul> <li>\ud83d\ude80 Stable API consolidating all v0.x features.</li> <li>\ud83d\udcd6 Readers: Plug-and-play support for Vanilla, MarkItDown, and Docling, covering formats like text, Office, JSON/YAML, images, HTML, and more.</li> <li>\ud83e\ude93 Splitters: Extensive library of split strategies, including character, word, sentence, paragraph, token, paged, row/column, JSON, semantic, HTML tag, header, and code splitters.</li> <li>\ud83e\udde0 Models: Multimodal Vision-Language support for OpenAI, Azure, Grok, HuggingFace, Gemini, Claude, and more.</li> <li>\ud83d\uddfa\ufe0f Embeddings: Fully integrated embeddings from OpenAI, Azure, HuggingFace, Gemini, and Claude (via Voyage).</li> <li>\ud83c\udf9b\ufe0f Extras system: Install the minimal core, or extend with <code>markitdown</code>, <code>docling</code>, <code>multimodal</code>, or <code>all</code> for a batteries-included setup.</li> <li>\ud83d\udcda Docs: New API reference, real executed notebook examples, and updated architecture diagrams.</li> <li>\ud83d\udd27 Developer Experience: CI/CD pipeline, PyPI publishing, pre-commit checks, and improved cleaning instructions.</li> <li>\ud83d\udc1b Bugfixes: Improved NLTK tokenizers, more robust splitters, and new utilities for HTML =&gt; Markdown conversion.</li> </ul> <p>Check out the updated documentation, new examples, and join us in making text splitting and document parsing easier than ever!</p> <p>Version 1.0.1 released - <code>KeywordSplitter</code></p> <p>This Splitter allows to divide text based on specific regex patterns or keywords. See documentation here.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#different-input-formats","title":"Different input formats","text":"<p>SplitterMR can read data from multiples sources and files. To read the files, it uses the Reader components, which inherits from a Base abstract class, <code>BaseReader</code>. This object allows you to read the files as a properly formatted string, or convert the files into another format (such as <code>markdown</code> or <code>json</code>). </p> <p>Currently, there are supported three readers: <code>VanillaReader</code>, and <code>MarkItDownReader</code> and <code>DoclingReader</code>. These are the differences between each Reader component:</p> Reader Unstructured files &amp; PDFs MS Office suite files Tabular data Files with hierarchical schema Image files Markdown conversion <code>VanillaReader</code> <code>txt</code>, <code>md</code>, <code>pdf</code> <code>xlsx</code>, <code>docx</code>, <code>pptx</code> <code>csv</code>, <code>tsv</code>, <code>parquet</code> <code>json</code>, <code>yaml</code>, <code>html</code>, <code>xml</code> <code>jpg</code>, <code>png</code>, <code>webp</code>, <code>gif</code> Yes <code>MarkItDownReader</code> <code>txt</code>, <code>md</code>, <code>pdf</code> <code>docx</code>, <code>xlsx</code>, <code>pptx</code> <code>csv</code>, <code>tsv</code> <code>json</code>, <code>html</code>, <code>xml</code> <code>jpg</code>, <code>png</code>, <code>pneg</code> Yes <code>DoclingReader</code> <code>txt</code>, <code>md</code>, <code>pdf</code> <code>docx</code>, <code>xlsx</code>, <code>pptx</code> \u2013 <code>html</code>, <code>xhtml</code> <code>png</code>, <code>jpeg</code>, <code>tiff</code>, <code>bmp</code>, <code>webp</code> Yes"},{"location":"#several-splitting-methods","title":"Several splitting methods","text":"<p>SplitterMR allows you to split files in many different ways depending on your needs. The available splitting methods are described in the following table:</p> Splitting Technique Description Character Splitter Splits text into chunks based on a specified number of characters. Supports overlapping by character count or percentage.  Parameters: <code>chunk_size</code> (max chars per chunk), <code>chunk_overlap</code> (overlapping chars: int or %).  Compatible with: Text. Word Splitter Splits text into chunks based on a specified number of words. Supports overlapping by word count or percentage.  Parameters: <code>chunk_size</code> (max words per chunk), <code>chunk_overlap</code> (overlapping words: int or %).  Compatible with: Text. Sentence Splitter Splits text into chunks by a specified number of sentences. Allows overlap defined by a number or percentage of words from the end of the previous chunk. Customizable sentence separators (e.g., <code>.</code>, <code>!</code>, <code>?</code>).  Parameters: <code>chunk_size</code> (max sentences per chunk), <code>chunk_overlap</code> (overlapping words: int or %), <code>sentence_separators</code> (list of characters).  Compatible with: Text. Paragraph Splitter Splits text into chunks based on a specified number of paragraphs. Allows overlapping by word count or percentage, and customizable line breaks.  Parameters: <code>chunk_size</code> (max paragraphs per chunk), <code>chunk_overlap</code> (overlapping words: int or %), <code>line_break</code> (delimiter(s) for paragraphs).  Compatible with: Text. Recursive Splitter Recursively splits text based on a hierarchy of separators (e.g., paragraph, sentence, word, character) until chunks reach a target size. Tries to preserve semantic units as long as possible.  Parameters: <code>chunk_size</code> (max chars per chunk), <code>chunk_overlap</code> (overlapping chars), <code>separators</code> (list of characters to split on, e.g., <code>[\"\\n\\n\", \"\\n\", \" \", \"\"]</code>).  Compatible with: Text. NEW Keyword Splitter Splits text into chunks around matches of specified keywords, using one or more regex patterns. Supports precise boundary control\u2014matched keywords can be included <code>before</code>, <code>after</code>, <code>both</code> sides, or omitted from the split. Each keyword can have a custom name (via <code>dict</code>) for metadata counting. Secondary soft-wrapping by <code>chunk_size</code> is supported.  Parameters: <code>patterns</code> (list of regex patterns, or <code>dict</code> mapping names to patterns), <code>include_delimiters</code> (<code>\"before\"</code>, <code>\"after\"</code>, <code>\"both\"</code>, or <code>\"none\"</code>), <code>flags</code> (regex flags, e.g. <code>re.MULTILINE</code>), <code>chunk_size</code> (max chars per chunk, soft-wrapped).  Compatible with: Text. Token Splitter Splits text into chunks based on the number of tokens, using various tokenization models (e.g., tiktoken, spaCy, NLTK). Useful for ensuring chunks are compatible with LLM context limits.  Parameters: <code>chunk_size</code> (max tokens per chunk), <code>model_name</code> (tokenizer/model, e.g., <code>\"tiktoken/cl100k_base\"</code>, <code>\"spacy/en_core_web_sm\"</code>, <code>\"nltk/punkt\"</code>), <code>language</code> (for NLTK).  Compatible with: Text. Paged Splitter Splits text by pages for documents that have page structure. Each chunk contains a specified number of pages, with optional word overlap.  Parameters: <code>num_pages</code> (pages per chunk), <code>chunk_overlap</code> (overlapping words).  Compatible with: Word, PDF, Excel, PowerPoint. Row/Column Splitter For tabular formats, splits data by a set number of rows or columns per chunk, with possible overlap. Row-based and column-based splitting are mutually exclusive.  Parameters: <code>num_rows</code>, <code>num_cols</code> (rows/columns per chunk), <code>overlap</code> (overlapping rows or columns).  Compatible with: Tabular formats (csv, tsv, parquet, flat json). JSON Splitter Recursively splits JSON documents into smaller sub-structures that preserve the original JSON schema.  Parameters: <code>max_chunk_size</code> (max chars per chunk), <code>min_chunk_size</code> (min chars per chunk).  Compatible with: JSON. Semantic Splitter Splits text into chunks based on semantic similarity, using an embedding model and a max tokens parameter. Useful for meaningful semantic groupings.  Parameters: <code>embedding_model</code> (model for embeddings), <code>max_tokens</code> (max tokens per chunk).  Compatible with: Text. HTML Tag Splitter Splits HTML content based on a specified tag, or automatically detects the most frequent and shallowest tag if not specified. Each chunk is a complete HTML fragment for that tag.  Parameters: <code>chunk_size</code> (max chars per chunk), <code>tag</code> (HTML tag to split on, optional).  Compatible with: HTML. Header Splitter Splits Markdown or HTML documents into chunks using header levels (e.g., <code>#</code>, <code>##</code>, or <code>&lt;h1&gt;</code>, <code>&lt;h2&gt;</code>). Uses configurable headers for chunking.  Parameters: <code>headers_to_split_on</code> (list of headers and semantic names), <code>chunk_size</code> (unused, for compatibility).  Compatible with: Markdown, HTML. Code Splitter Splits source code files into programmatically meaningful chunks (functions, classes, methods, etc.), aware of the syntax of the specified programming language (e.g., Python, Java, Kotlin). Uses language-aware logic to avoid splitting inside code blocks.  Parameters: <code>chunk_size</code> (max chars per chunk), <code>language</code> (programming language as string, e.g., <code>\"python\"</code>, <code>\"java\"</code>).  Compatible with: Source code files (Python, Java, Kotlin, C++, JavaScript, Go, etc.)."},{"location":"#architecture","title":"Architecture","text":"<p>SplitterMR is designed around a modular pipeline that processes files from raw data all the way to chunked, LLM-ready text. There are three main components: Readers, Models and Splitters.</p> <ul> <li>Readers<ul> <li>The <code>BaseReader</code> components read a file and optionally converts to other formats to subsequently conduct a splitting strategy.</li> <li>Supported readers (e.g., <code>VanillaReader</code>, <code>MarkItDownReader</code>, <code>DoclingReader</code>) produce a <code>ReaderOutput</code> dictionary containing:<ul> <li>Text content (in <code>markdown</code>, <code>text</code>, <code>json</code> or another format).</li> <li>Document metadata.</li> <li>Conversion method.</li> </ul> </li> </ul> </li> <li>Models:<ul> <li>The <code>BaseModel</code> component is used to read non-text content using a Visual Language Model (VLM).</li> <li>Supported models are <code>AzureOpenAI</code>, <code>OpenAI</code> and <code>Grok</code>, but more models will be available soon.</li> <li>All the models have a <code>analyze_content</code> method which returns the LLM response based on a prompt, the client and the model parameters.</li> </ul> </li> <li>Splitters<ul> <li>The <code>BaseSplitter</code> components take the <code>ReaderOutput</code> text content and divide that text into meaningful chunks for LLM or other downstream use.</li> <li>Splitter classes (e.g., <code>CharacterSplitter</code>, <code>SentenceSplitter</code>, <code>RecursiveCharacterSplitter</code>, etc.) allow flexible chunking strategies with optional overlap and rich configuration.</li> </ul> </li> <li>Embedders<ul> <li>The <code>BaseEmbedder</code> components are used to encode the text into embeddings. These embeddings are used to split text by semantic similarity.</li> <li>Supported models are <code>AzureOpenAI</code> and <code>OpenAI</code>, but more models will be available soon.</li> <li>All the models have a <code>encode_text</code> method which returns the embeddings based on a text, the client and the model parameters.</li> </ul> </li> </ul>"},{"location":"#how-to-install","title":"How to install","text":"<p>Package is published on PyPi.  </p> <p>By default, only the core dependencies are installed. If you need additional features (e.g., MarkItDown, Docling, multimodal processing), you can install the corresponding extras.</p>"},{"location":"#core-install","title":"Core install","text":"<p>Installs the basic text splitting and file parsing features (lightweight, fast install):</p> <pre><code>pip install splitter-mr\n</code></pre>"},{"location":"#optional-extras","title":"Optional extras","text":"Extra Description Example install command <code>markitdown</code> Adds MarkItDown support for rich-text document parsing (HTML, DOCX, etc.). <code>pip install \"splitter-mr[markitdown]\"</code> <code>docling</code> Adds Docling support for high-quality PDF/document to Markdown conversion. <code>pip install \"splitter-mr[docling]\"</code> <code>multimodal</code> Enables computer vision, OCR, and audio features \u2014 includes PyTorch, EasyOCR, OpenCV, Transformers, etc. <code>pip install \"splitter-mr[multimodal]\"</code> <code>all</code> Installs everything above (MarkItDown + Docling + Multimodal + Azure). Heavy install (\\~GBs). <code>pip install \"splitter-mr[all]\"</code>"},{"location":"#multiple-extras","title":"Multiple extras","text":"<p>You can combine extras by separating them with commas:</p> <pre><code>pip install \"splitter-mr[markitdown,docling]\"\n</code></pre>"},{"location":"#using-other-package-managers","title":"Using other package managers","text":"<p>You can also install it with <code>uv</code>, <code>conda</code> or <code>poetry</code>:</p> <pre><code>uv add splitter-mr\n</code></pre> <p>Note</p> <p>Python 3.11 or greater is required to use this library.</p>"},{"location":"#how-to-use","title":"How to use","text":""},{"location":"#read-files","title":"Read files","text":"<p>Firstly, you need to instantiate an object from a BaseReader class, for example, <code>VanillaReader</code>.</p> <pre><code>from splitter_mr.reader import VanillaReader\n\nreader = VanillaReader()\n</code></pre> <p>To read any file, provide the file path within the <code>read()</code> method. If you use <code>DoclingReader</code> or <code>MarkItDownReader</code>, your files will be automatically parsed to markdown text format. The result of this reader will be a <code>ReaderOutput</code> object, a dictionary with the following shape:</p> <p><pre><code>reader_output = reader.read('https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/lorem_ipsum.txt')\nprint(reader_output)\n</code></pre> <pre><code>text='Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum sit amet ultricies orci. Nullam et tellus dui.', \ndocument_name='lorem_ipsum.txt',\ndocument_path='https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/lorem_ipsum.txt', \ndocument_id='732b9530-3e41-4a1a-a4ea-1d9d6fe815d3', \nconversion_method='txt', \nreader_method='vanilla', \nocr_method=None, \npage_placeholder=None,\nmetadata={}\n</code></pre></p> <p>Note</p> <p>Note that you can read from an URL, a variable and from a <code>file_path</code>. See Developer guide.</p>"},{"location":"#split-text","title":"Split text","text":"<p>To split the text, first import the class that implements your desired splitting strategy (e.g., by characters, recursively, by headers, etc.). Then, create an instance of this class and call its <code>split</code> method, which is defined in the <code>BaseSplitter</code> class.</p> <p>For example, we will split by characters with a maximum chunk size of 50, with an overlap between chunks:</p> <p><pre><code>from splitter_mr.splitter import CharacterSplitter\n\nchar_splitter = CharacterSplitter(chunk_size=50, chunk_overlap = 10)\nsplitter_output = char_splitter.split(reader_output)\nprint(splitter_output)\n</code></pre> <pre><code>chunks=['Lorem ipsum dolor sit amet, consectetur adipiscing', 'adipiscing elit. Vestibulum sit amet ultricies orc', 'ricies orci. Nullam et tellus dui.'], \nchunk_id=['db454a9b-32aa-4fdc-9aab-8770cae99882', 'e67b427c-4bb0-4f28-96c2-7785f070d1c1', '6206a89d-efd1-4586-8889-95590a14645b'], \ndocument_name='lorem_ipsum.txt', \ndocument_path='https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/lorem_ipsum.txt', \ndocument_id='732b9530-3e41-4a1a-a4ea-1d9d6fe815d3', \nconversion_method='txt', \nreader_method='vanilla', \nocr_method=None, \nsplit_method='character_splitter', \nsplit_params={'chunk_size': 50, 'chunk_overlap': 10}, \nmetadata={}\n</code></pre></p> <p>The returned object is a <code>SplitterOutput</code> dataclass, which provides all the information you need to further process your data. You can easily add custom metadata, and you have access to details such as the document name, path, and type. Each chunk is uniquely identified by an UUID, allowing for easy traceability throughout your LLM workflow.</p>"},{"location":"#compatibility-with-vision-tools-for-image-processing-and-annotations","title":"Compatibility with vision tools for image processing and annotations","text":"<p>Pass a VLM model to any Reader via the <code>model</code> parameter:</p> <pre><code>from splitter_mr.reader import VanillaReader\nfrom splitter_mr.model.models import AzureOpenAIVisionModel\n\nmodel = AzureOpenAIVisionModel()\nreader = VanillaReader(model=model)\noutput = reader.read(file_path=\"data/sample_pdf.pdf\")\nprint(output.text)\n</code></pre> <p>These VLMs can be used for captioning, annotation or text extraction. In fact, you can use these models to process the files as you want using the <code>prompt</code> parameter in the <code>read</code> method for every class which inherits from <code>BaseReader</code>. </p> <p>Note</p> <p>To see more details, consult documentation here.</p>"},{"location":"#updates","title":"Updates","text":""},{"location":"#next-features","title":"Next features","text":"<ul> <li> NEW Provide a MCP server to make queries about the chunked documents.</li> <li> Add examples on how to implement SplitterMR in RAGs, MCPs and Agentic RAGs.</li> <li> Add a method to read PDFs using Textract.</li> <li> Add a new <code>BaseVisionModel</code> class to support generic API-provided models.</li> <li> Add asynchronous methods for Splitters and Readers.</li> <li> Add batch methods to process several documents at once.</li> <li> Add support to read formulas.</li> <li> Add classic OCR models: <code>easyocr</code> and <code>pytesseract</code>.</li> <li> Add support to generate output in <code>markdown</code> for all data types in VanillaReader.</li> <li> Add methods to support Markdown, JSON and XML data types when returning output.</li> </ul>"},{"location":"#previously-implemented-v100","title":"Previously implemented (<code>^v1.0.0</code>)","text":"<ul> <li> Add <code>KeywordSplitter</code> support.</li> </ul>"},{"location":"#previously-implemented-up-to-v100","title":"Previously implemented (up to <code>v1.0.0</code>)","text":"<ul> <li> Add embedding model support.<ul> <li> Add OpenAI embeddings model support.</li> <li> Add Azure OpenAI embeddings model support.</li> <li> Add HuggingFace embeddings model support.</li> <li> Add Gemini embeddings model support.</li> <li> Add Claude Anthropic embeddings model support.</li> </ul> </li> <li> Add Vision models:<ul> <li> Add OpenAI vision model support.</li> <li> Add Azure OpenAI embeddings model support.</li> <li> Add Grok VLMs model support.</li> <li> Add HuggingFace VLMs model support.</li> <li> Add Gemini VLMs model support.</li> <li> Add Claude Anthropic VLMs model support.</li> </ul> </li> <li> Modularize library into several sub-libraries.</li> <li> Implement a method to split by embedding similarity: <code>SemanticSplitter</code>.</li> <li> Add new supported formats to be analyzed with OpenAI and AzureOpenAI models.</li> <li> Add support to read images using <code>VanillaReader</code>. </li> <li> Add support to read <code>xlsx</code>, <code>docx</code> and <code>pptx</code> files using <code>VanillaReader</code>. </li> <li> Add support to read images using <code>VanillaReader</code>.</li> <li> Implement a method to split a document by pages (<code>PagedSplitter</code>).</li> <li> Add support to read PDF as scanned pages.</li> <li> Add support to change image placeholders.</li> <li> Add support to change page placeholders.</li> <li> Add Pydantic models to define Reader and Splitter outputs.</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>If you want to collaborate, please contact me through the following media: </p> <ul> <li>My mail.</li> <li>My LinkedIn</li> <li>PyPI package</li> </ul>"},{"location":"CHANGELOG/","title":"v1.0.x","text":"<p>Version 1.0.0: First stable release with full Reader, Splitter, Embedding and Vision model support.</p>"},{"location":"CHANGELOG/#v101","title":"v1.0.1","text":"<p>Add KeywordSplitter to split by regex patterns or specific keywords.</p>"},{"location":"CHANGELOG/#features","title":"Features","text":"<p>Add new Splitter: KeywordSplitter. This Splitter allows to chunk based on regular expressions and patterns.</p>"},{"location":"CHANGELOG/#documentation","title":"Documentation","text":"<p>Update documentation server to provide more examples</p>"},{"location":"CHANGELOG/#developer-features","title":"Developer features","text":"<p>Update pre-commit hooks to sync dependencies when executing the tests.</p>"},{"location":"CHANGELOG/#v100","title":"v1.0.0","text":""},{"location":"CHANGELOG/#features_1","title":"Features","text":"<ul> <li>Consolidated all features introduced in v0.x series into a stable API.</li> <li>Readers: <code>VanillaReader</code>, <code>MarkItDownReader</code>, <code>DoclingReader</code> with support for multiple formats (text, Office, JSON/YAML, images, HTML, etc.).</li> <li>Splitters: character, word, sentence, paragraph, recursive, token, paged, row/column, JSON, semantic, HTML tag, header, and code splitting strategies.</li> <li>Models: support for multimodal Vision-Language Models (OpenAI, Azure, Grok, HuggingFace, Gemini, Claude).</li> <li>Embeddings: OpenAI, Azure, HuggingFace, Gemini, Claude (via Voyage) supported.</li> </ul>"},{"location":"CHANGELOG/#developer-features_1","title":"Developer features","text":"<ul> <li>Optional extras system: install lightweight core by default, extend with <code>markitdown</code>, <code>docling</code>, <code>multimodal</code>, <code>azure</code>, or <code>all</code>.</li> <li>CI/CD pipeline, PyPI release, and pre-commit checks in place.</li> </ul>"},{"location":"CHANGELOG/#documentation_1","title":"Documentation","text":"<p>Extensive documentation with API reference, examples, and architecture diagrams.</p>"},{"location":"CHANGELOG/#new-improvements-bug-fixes","title":"New improvements + Bug fixes","text":"<ul> <li>Fix: NLTK tokenizers in <code>TokenSplitter</code> are now correct base tokenizer when using <code>nltk</code> tokenizers.</li> <li>Fix: RecursiveJSONSplitter could not produce outputs since it did not validate correct data type.</li> <li>Now the examples are based on real Jupyter Notebooks executions to ensure that the behavior is the expected one.</li> <li>Added the Notebooks which are used as examples in the <code>notebooks</code> section.</li> <li>Update <code>clean</code> instruction with <code>poe</code>.</li> <li>New constans have been defined.</li> <li>Add new class to transform HTML to Markdown.</li> </ul>"},{"location":"CHANGELOG/#v06x","title":"v0.6.x","text":"<p>[!IMPORTANT] Breaking Change! Version v0.6.0</p> <p>Dependencies are now split into core (installed by default) and optional extras for heavy or specialized features. - Example: to use MarkItDown and Docling readers, install with:   <pre><code>pip install \"splitter-mr[markitdown,docling]\"\n</code></pre> - To install all optional features:   <pre><code>pip install \"splitter-mr[all]\"\n</code></pre></p> <ul> <li>This change reduces install time and keeps core installs lightweight.</li> </ul>"},{"location":"CHANGELOG/#v065","title":"v0.6.5","text":"<p>Hotfix: dependency isolation was not guaranteed.</p>"},{"location":"CHANGELOG/#features_2","title":"Features","text":"<ul> <li>Add a util class to convert HTML to Markdown content.</li> <li>Improve Header Splitter to always return its content in markdown format.</li> <li>Add the option to return text in markdown format for HTMLTagSplitter.</li> <li>Add the option to batch content when using HTMLTagSplitter: If <code>batch=True</code>, it returns the chunks grouped by tags up to the numbers of characters specified by <code>chunk_size</code>. If False, it will return one register per tag.</li> </ul>"},{"location":"CHANGELOG/#bug-fixes","title":"Bug fixes","text":"<ul> <li>Dependency isolation was not guaranteed: implement safe lazy imports in all the <code>__init__</code> methods.</li> <li>Raise test coverage up to 90%. </li> </ul>"},{"location":"CHANGELOG/#064","title":"0.6.4","text":"<p>Version 0.6.4:</p> <p>SplitterMR now supports Anthropic Claude as a backend for both embedding (via Voyage AI) and vision models.</p>"},{"location":"CHANGELOG/#features_3","title":"Features","text":"<ul> <li>Add new Vision Model: Claude Anthropic models.</li> <li>Add new Embedding Model: Voyage Anthropic models.</li> </ul>"},{"location":"CHANGELOG/#documentation_2","title":"Documentation","text":"<ul> <li>Change font type to Documentation server.</li> <li>Update API reference guide with new links and resources.</li> </ul>"},{"location":"CHANGELOG/#v063","title":"v0.6.3","text":"<p>Version 0.6.3: SplitterMR now supports Gemini as a backend for both embedding and vision models.</p> <p>To use HuggingFace, Gemini, Claude or Grok models, you must install SplitterMR with the <code>multimodal</code> extra:</p> <pre><code>pip install \"splitter-mr[multimodal]\"\n</code></pre>"},{"location":"CHANGELOG/#features_4","title":"Features","text":"<ul> <li>Add <code>GeminiVisionModel</code> class to Vision models.</li> <li>Add <code>GeminiEmbedding</code> class to embedding models.</li> <li>Apply lazy import strategy to classes which require <code>extra</code>s to be installed (e.g., <code>docling</code>, <code>markitdown</code>, etc.).</li> </ul>"},{"location":"CHANGELOG/#documentation_3","title":"Documentation","text":"<ul> <li>Update documentation.</li> </ul>"},{"location":"CHANGELOG/#v062","title":"v0.6.2","text":"<p>Version 0.6.2: SplitterMR now supports HuggingFace as a backend for both embedding and vision models:</p> <ul> <li>HuggingFaceEmbedding: Use any Sentence Transformers model (local or from Hugging Face Hub) for fast, local, or cloud embeddings.</li> <li>HuggingFaceVisionModel: Leverage Hugging Face\u2019s vision-language models for image-to-text and image captioning.</li> </ul> <p>To use HuggingFace, Gemini, Claude or Grok models, you must install SplitterMR with the <code>multimodal</code> extra:</p> <pre><code>pip install \"splitter-mr[multimodal]\"\n</code></pre> <p>Add HuggingFace Model and Embedding support.</p>"},{"location":"CHANGELOG/#features_5","title":"Features","text":"<ul> <li>Add <code>HuggingFaceVisionModel</code> class. Note that the support is limited until now.</li> <li>Add <code>HuggingFaceEmbedding</code> class.</li> </ul>"},{"location":"CHANGELOG/#documentation_4","title":"Documentation","text":"<ul> <li>Add <code>HuggingFaceVisionModel</code> to documentation.</li> <li>Update architecture diagram.</li> <li>Update <code>README.md</code>.</li> </ul>"},{"location":"CHANGELOG/#developer-features_2","title":"Developer features","text":"<ul> <li>Add new dependencies to multimodal group.</li> </ul>"},{"location":"CHANGELOG/#v061","title":"v0.6.1","text":"<p>Add Grok Vision Model.</p> <p>Version 0.6.1: SplitterMR now supports <code>GrokVisionModel</code>. See documentation here.</p> <p>To use HuggingFace, Gemini, Claude or Grok models, you must install SplitterMR with the <code>multimodal</code> extra:</p> <p><code>``bash pip install \"splitter-mr[multimodal]\"</code></p>"},{"location":"CHANGELOG/#features_6","title":"Features","text":"<ul> <li>Add <code>GrokVisionModel</code>. </li> <li>Redefine constants.</li> <li>Add new tests.</li> </ul>"},{"location":"CHANGELOG/#documentation_5","title":"Documentation","text":"<ul> <li>Add <code>GrokVisionModel</code> documentation.</li> <li>Fix format bugs.</li> <li>Add new documentation in Readers about how to install necessary dependencies.</li> <li>Add plugin to read formulas appropiately.</li> </ul>"},{"location":"CHANGELOG/#v060","title":"v0.6.0","text":"<p>Divide library into sub packages.</p>"},{"location":"CHANGELOG/#features_7","title":"Features","text":"<ul> <li>Divide the library into sub-modules.</li> </ul>"},{"location":"CHANGELOG/#developer-features_3","title":"Developer features","text":"<ul> <li>Add new steps to Dockerfile images.</li> <li>Change <code>requirements.txt</code> to don't save editable builds as dependencies.</li> <li>Change how the <code>splitter_mr</code> library is installed within Dockerfiles.</li> <li>Lighten the weight of the library by making some dependencies optional.</li> <li>Change how <code>poe test</code> is executed.</li> </ul>"},{"location":"CHANGELOG/#documentation_6","title":"Documentation","text":"<ul> <li>Fix Embedding models not showing on Developer Guide overview page.</li> </ul>"},{"location":"CHANGELOG/#v05x","title":"v0.5.x","text":"<p>[!IMPORTANT] New version v0.5.0</p> <ul> <li>Add embedding models to encode the text into distributed vectorized representations. See documentation here. </li> <li>Add support for chunking files based on semantic similarity between sentences. See documentation here.</li> </ul>"},{"location":"CHANGELOG/#v050","title":"v0.5.0","text":"<p>Add SemanticSplitter first implementation</p>"},{"location":"CHANGELOG/#features_8","title":"Features","text":"<ul> <li>Add <code>embedding</code> module.</li> <li>Add <code>AzureOpenAI embeddings</code>.</li> <li>Add <code>OpenAI embeddings</code>.</li> <li>Add <code>BaseEmbeddings</code>, to create your own class.</li> <li>Add <code>SemanticSplitter</code> class.</li> </ul>"},{"location":"CHANGELOG/#fixes","title":"Fixes","text":"<ul> <li>Fix <code>SentenceSplitter</code> class to be more robust and flexible (separators can be customized using regex pattern).</li> </ul>"},{"location":"CHANGELOG/#developer-features_4","title":"Developer features","text":"<ul> <li>Update tests.</li> </ul>"},{"location":"CHANGELOG/#documentation_7","title":"Documentation","text":"<ul> <li>Update documentation with new embedding module.</li> <li>Fix some format errors in Documentation server.</li> <li>Add new example documentation page for <code>SemanticSplitter</code>.</li> </ul>"},{"location":"CHANGELOG/#v04x","title":"v0.4.x","text":""},{"location":"CHANGELOG/#v040","title":"v0.4.0","text":"<p>PagedSplitter full implementation</p> <p>[!IMPORTANT] New version v0.4.0</p> <p>Add support for reading files and splitting them by pages using <code>PageSplitter</code>. Add support to read more files with <code>VanillaReader</code>.</p> <p>\u27a1\ufe0f See documentation.</p>"},{"location":"CHANGELOG/#features_9","title":"Features","text":"<ul> <li>Add support to read a PDF by pages using <code>MarkItDownReader</code> without LLM.</li> <li>Add method to read <code>xlsx</code>, <code>pptx</code>, <code>docx</code> files using <code>VanillaReader</code>.</li> <li>Add method to read several image formats using <code>VanillaReader</code>.</li> <li>Add support to read excel and parquet files using different engines in <code>VanillaReader</code>.</li> <li>Add support to analyze content in several file types using AzureOpenAI and OpenAI models.</li> </ul>"},{"location":"CHANGELOG/#documentation_8","title":"Documentation","text":"<ul> <li>Update documentation.</li> <li>Fix some hyperlinks in README.</li> </ul>"},{"location":"CHANGELOG/#v03x","title":"v0.3.x","text":""},{"location":"CHANGELOG/#v033","title":"v0.3.3","text":""},{"location":"CHANGELOG/#features_10","title":"Features","text":"<ul> <li>Add a method to convert variables to a <code>ReaderOutput</code> object.</li> <li>Add a <code>page_placeholder</code> attribute to the <code>ReaderOutput</code> object to distinguish when a file has been read by pages and which placeholder is.</li> <li>Add an splitter method which split by pages for supported documents: <code>PagedSplitter</code>. </li> </ul>"},{"location":"CHANGELOG/#developer-features_5","title":"Developer features","text":"<ul> <li>Refactor the <code>VanillaReader</code> class to be more decoupled.</li> </ul>"},{"location":"CHANGELOG/#documentation_9","title":"Documentation","text":"<ul> <li>Update examples in documentation server.</li> </ul>"},{"location":"CHANGELOG/#v032","title":"v0.3.2","text":""},{"location":"CHANGELOG/#features_11","title":"Features","text":"<ul> <li>Add <code>Pydantic</code> models to validate inputs and outputs for <code>BaseReader</code> and <code>BaseSplitter</code> objects.</li> <li>Refactor models to modularize into constants, pydantic models and prompts.</li> </ul>"},{"location":"CHANGELOG/#documentation_10","title":"Documentation","text":"<ul> <li>Update <code>README.md</code> to handle notes and warnings.</li> </ul>"},{"location":"CHANGELOG/#v031","title":"v0.3.1","text":""},{"location":"CHANGELOG/#features_12","title":"Features","text":"<ul> <li>Add support to read and scan PDF by pages for all the readers, using the parameter <code>scan_pdf_images = True</code>. </li> <li>Add support to use different placeholders for images in Vanilla and Docling Readers.</li> <li>Add support to split by pages for PDFs. </li> <li>Add three different pipelines to DoclingReader to process the document as PageImages, using VLM to provide image captioning and regularly.</li> <li>Add three different pipelines to VanillaReader to process the document as PageImages, using VLM to provide image captioning and regularly.</li> </ul>"},{"location":"CHANGELOG/#bugs","title":"Bugs","text":"<ul> <li>Change how the arguments are passed to every Reader to enhance robutsness.</li> <li>Add new test cases.</li> </ul>"},{"location":"CHANGELOG/#documentation_11","title":"Documentation","text":"<ul> <li>Update examples.</li> <li>Change MkDocs server to support both light and dark modes.</li> </ul>"},{"location":"CHANGELOG/#v030","title":"v0.3.0","text":"<p>[!IMPORTANT] Vision Language Model (VLM) support!</p> <p>You can now use vision-capable models (OpenAI Vision, Azure OpenAI Vision) to extract image descriptions and OCR text during file reading. Pass a VLM model to any Reader class via the <code>model</code> parameter. </p> <p>\u27a1\ufe0f See documentation.</p>"},{"location":"CHANGELOG/#features_13","title":"Features","text":"<ul> <li>Implement <code>AzureOpenAI</code> and <code>OpenAI</code> Vision Models to analyze graphical resources in PDF files.</li> <li>Add support to read PDF files to VanillaReader using <code>PDFPlumber</code>.</li> </ul>"},{"location":"CHANGELOG/#documentation_12","title":"Documentation","text":"<ul> <li>Update examples.</li> <li>Add new examples to documentation.</li> <li>Add Reading methods with PDF documentation.</li> <li>Add information about implementing VLMs in your reading pipeline.</li> <li>Change file names on data folder to be more descriptive.</li> <li>Update <code>README.md</code> and <code>CHANGELOG</code>.</li> </ul>"},{"location":"CHANGELOG/#fixes_1","title":"Fixes","text":"<ul> <li>Update tests.</li> <li>Update docstrings.</li> <li>Update <code>TokenSplitter</code> to raise Exceptions if no valid models are provided.</li> <li>Update <code>TokenSplitter</code> to take as default argument a valid tiktoken model.</li> <li>Change <code>HTMLTagSplitter</code> to take the headers if a table is provided.</li> <li>Change <code>HeaderSplitter</code> to preserve headers in chunks.</li> </ul>"},{"location":"CHANGELOG/#v02x","title":"v0.2.x","text":""},{"location":"CHANGELOG/#v022","title":"v0.2.2","text":""},{"location":"CHANGELOG/#features_14","title":"Features","text":"<ul> <li>Implement <code>TokenSplitter</code>: split text into chunks by number of tokens, using selectable tokenizers (<code>tiktoken</code>, <code>spacy</code>, <code>nltk</code>).</li> <li><code>MarkItDownReader</code> now supports more file extensions: PDFs, audio, etc.</li> </ul>"},{"location":"CHANGELOG/#fixes_2","title":"Fixes","text":"<ul> <li><code>HTMLTagSplitter</code> does not correctly chunking the document as desired.</li> <li>Change <code>docstring</code> documentation.</li> </ul>"},{"location":"CHANGELOG/#documentation_13","title":"Documentation","text":"<ul> <li>Updated Splitter strategies documentation to include <code>TokenSplitter</code>.</li> <li>Expanded example scripts and test scripts for end-to-end manual and automated verification of all Splitter strategies.</li> <li>New examples in documentation server for <code>HTMLTagSplitter</code>. </li> </ul>"},{"location":"CHANGELOG/#developer-features_6","title":"Developer Features","text":"<ul> <li>Remove <code>Pipfile</code> and <code>Pipfile.lock</code>. </li> <li>Update to <code>poethepoet</code> as task runner tool </li> </ul>"},{"location":"CHANGELOG/#fixes_3","title":"Fixes","text":""},{"location":"CHANGELOG/#v021","title":"v0.2.1","text":""},{"location":"CHANGELOG/#features_15","title":"Features","text":"<p>Implement <code>CodeSplitter</code>.</p>"},{"location":"CHANGELOG/#fixes_4","title":"Fixes","text":"<ul> <li>Change <code>docstring</code> for <code>BaseSplitter</code> to update with current parameters.</li> <li>Some minor warnings in documentations when deploying Mkdocs server.</li> </ul>"},{"location":"CHANGELOG/#v020","title":"v0.2.0","text":"<p>[!IMPORTANT] Breaking change!</p> <ul> <li>All Readers now return <code>ReaderOutput</code> dataclass objects.</li> <li>All Splitters now return <code>SplitterOutput</code> dataclass objects.</li> </ul> <p>You must access fields using dot notation (e.g., <code>result.text</code>, <code>result.chunks</code>), not dictionary keys.</p>"},{"location":"CHANGELOG/#features_16","title":"Features","text":"<p>New splitting strategy: <code>RowColumnSplitter</code> for flexible splitting of tabular data.</p> <p>New reader_method attribute in output dataclasses.</p>"},{"location":"CHANGELOG/#migration","title":"Migration","text":"<p>Update all code/tests to use attribute access for results from Readers and Splitters.</p> <p>Use <code>.to_dict()</code> on output if a dictionary is required.</p> <p>Update any custom splitter/reader implementations to use the new output dataclasses.</p>"},{"location":"CHANGELOG/#v01x","title":"v0.1.x","text":""},{"location":"CHANGELOG/#v013","title":"v0.1.3","text":""},{"location":"CHANGELOG/#features_17","title":"Features","text":"<ul> <li>Add a new splitting strategy: <code>RowColumnSplitter</code>.</li> </ul>"},{"location":"CHANGELOG/#fixes_5","title":"Fixes","text":"<ul> <li>Change Readers to properly handle JSON files.</li> </ul>"},{"location":"CHANGELOG/#documentation_14","title":"Documentation","text":"<ul> <li>Update documentation.</li> </ul>"},{"location":"CHANGELOG/#v012","title":"v0.1.2","text":""},{"location":"CHANGELOG/#features_18","title":"Features","text":"<ul> <li>Now <code>VanillaReader</code> can read from multiple sources: URL, text, file_path and dictionaries.</li> </ul>"},{"location":"CHANGELOG/#fixes_6","title":"Fixes","text":"<ul> <li>By default, the document_id was <code>None</code> instead of and <code>uuid</code> in <code>VanillaReader</code>. </li> <li>Some name changes for <code>splitter_method</code> attribute in <code>SplitterOutput</code> method.</li> </ul>"},{"location":"CHANGELOG/#developer-features_7","title":"Developer features","text":"<ul> <li>Extend CI/CD lifecycle. Now it uses Dockerfile to check tests and deploy docs.</li> <li>Automate versioning for the Python project.</li> <li>The project has been published to PyPI.org. New versions will be deployed using CI/CD script.</li> <li><code>requirements.txt</code> has been added in the root of the project.</li> <li>A new stage in <code>pre-commit</code> has been introduced for generating the <code>requirements.txt</code> file.</li> <li><code>Makefile</code> extended: new commands to serve <code>mkdocs</code>. Now make clean remove more temporary files.</li> </ul>"},{"location":"CHANGELOG/#documentation_15","title":"Documentation","text":"<ul> <li>Update documentation in examples for the Documentation server.</li> <li>Documentation server now can be served using Make.</li> </ul>"},{"location":"CHANGELOG/#v011","title":"v0.1.1","text":"<p>Some bug fixes in HeaderSplitter and RecursiveCharacterSplitter, and documentation updates.</p>"},{"location":"CHANGELOG/#bug-fixes_1","title":"Bug fixes","text":"<ul> <li><code>chunk_overlap</code> (between 0 and 1) was not working in the <code>split</code> method from <code>RecursiveCharacterSplitter</code>.</li> <li>Some markdown code was not properly formatted in <code>README.md</code>.</li> <li>Reformat examples from docstring documentation in every Reader and Splitter classes.</li> <li><code>HeaderSplitter</code> was not properly handling the headers in some <code>markdown</code> and <code>HTML</code> files.</li> </ul>"},{"location":"CHANGELOG/#documentation_16","title":"Documentation","text":"<ul> <li>Some examples have been provided in the documentation (<code>docs/</code>, and in the documentation server).</li> <li>New examples in docstrings.</li> </ul>"},{"location":"CHANGELOG/#v010","title":"v0.1.0","text":"<p>First version of the project</p>"},{"location":"CHANGELOG/#functional-features","title":"Functional features","text":"<ul> <li>Add first readers, <code>VanillaReader</code>: reader which reads the files and format them into a string.</li> <li><code>DoclingReader</code>: reader which uses the docling package to read the files.</li> <li><code>MarkItDownReader</code>: reader which uses the markitdown package to read the files.</li> <li>Add first splitters: <code>CharacterSplitter</code>, <code>RecursiveCharacterSplitter</code>, <code>WordSplitter</code>, <code>SentenceSplitter</code>, <code>ParagraphSplitter</code>, <code>HTMLTagSplitter</code>, <code>RecursiveJSONSplitter</code>, <code>HeaderSplitter</code>: </li> <li>The package can be installed using pip.</li> <li><code>README.md</code> has been updated.</li> <li>Tests cases for main functionalities are available.</li> <li>Some data has been added for testing purposes.</li> <li>A documentation server is deployed with up-to-date information.</li> </ul>"},{"location":"CHANGELOG/#developer-features_8","title":"Developer features","text":"<ul> <li>Update <code>pyproject.toml</code> project information.</li> <li>Add pre-commit configurations (<code>flake8</code>, check commit messages, run test coverage, and update documentation).</li> <li>Add first Makefile commands (focused on developers):</li> <li><code>make help</code>: Provide a list with all the Make commands.</li> <li><code>make clean</code>: Clean temporal files and cache</li> <li><code>make shell</code>: Run a <code>uv</code> shell.</li> <li><code>make install</code>: Install uv CLI and pre-commit.</li> <li><code>make precommit</code>: Install pre-commit hooks.</li> <li><code>make format</code>: Run pyupgrade, isort, black, and flake8 for code style.</li> </ul>"},{"location":"api_reference/api_reference/","title":"Developer guide","text":"<p>Welcome to the SplitterMR Python API reference.</p> <p></p> <p></p>"},{"location":"api_reference/api_reference/#documentation","title":"Documentation","text":""},{"location":"api_reference/api_reference/#vision-model-component","title":"Vision Model component","text":"<p>Extend the reader capabilities using VLMs (Visual Language Models) to analyze visual content from your documents.</p>"},{"location":"api_reference/api_reference/#reader-component","title":"Reader component","text":"<p>Use different reading methods to process your files before splitting them.</p>"},{"location":"api_reference/api_reference/#splitter-component","title":"Splitter component","text":"<p>Implement several splitting strategies based on the type of document and use case.</p>"},{"location":"api_reference/api_reference/#embedding-component","title":"Embedding component","text":"<p>Implement encoder models from different providers to perform Semantic Splitting.</p>"},{"location":"api_reference/embedding/","title":"Embedding Models","text":""},{"location":"api_reference/embedding/#overview","title":"Overview","text":"<p>Encoder models are the engines which produce embeddings. These embeddings are distributed and vectorized representations of a text. These embeddings allows to capture relationships between semantic units (commonly words, but can be sentences, or even multimodal content such as images).  </p> <p>These embeddings can be used in a variety of tasks, such as:</p> <ul> <li>Measuring how relevant a word is within a text.  </li> <li>Comparing the similarity between two pieces of text.  </li> <li>Power searching, clustering, and recommendation systems building.  </li> </ul> <p></p> <p>SplitterMR takes advantage of these models in <code>SemanticSplitter</code>. These representations are used to break text into chunks based on meaning, not just size. Sentences with similar context end up together, regardless of length or position.</p>"},{"location":"api_reference/embedding/#which-embedder-should-i-use","title":"Which embedder should I use?","text":"<p>All embedders inherit from BaseEmbedding and expose the same interface for generating embeddings. Choose based on your cloud provider, credentials, and compliance needs.</p> Model When to use Requirements Features OpenAIEmbedding You have an OpenAI API key and want to use OpenAI\u2019s hosted embeddings <code>OPENAI_API_KEY</code> Production-ready text embeddings; simple setup; broad ecosystem/tooling support. AzureOpenAIEmbedding Your organization uses Azure OpenAI Services <code>AZURE_OPENAI_API_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_DEPLOYMENT</code> Enterprise controls, Azure compliance &amp; data residency; integrates with Azure identity. GeminiEmbedding You want Google\u2019s Gemini text embeddings <code>GEMINI_API_KEY</code> + Multimodal extra: <code>pip install 'splitter-mr[multimodal]'</code> Google Gemini API; modern, high-quality text embeddings. AnthropicEmbeddings You want embeddings aligned with Anthropic guidance (via Voyage AI) <code>VOYAGE_API_KEY</code> + Multimodal extra: <code>pip install 'splitter-mr[multimodal]'</code> Voyage AI embeddings (general, code, finance, law, multimodal); supports <code>input_type</code> for query/document asymmetry. HuggingFaceEmbedding Prefer local/open-source models (Sentence-Transformers); offline capability Multimodal extra: <code>pip install 'splitter-mr[multimodal]'</code> (optional: <code>HF_ACCESS_TOKEN</code>, only for required models) No API key; huge model zoo; CPU/GPU/MPS; optional L2 normalization for cosine similarity. BaseEmbedding Abstract base, not used directly \u2013 Implement to plug in a custom or self-hosted embedder. <p>Note</p> <p>In case that you want to bring your own embedding provider, you can easily implement the class using <code>BaseEmbedding</code>.</p>"},{"location":"api_reference/embedding/#embedders","title":"Embedders","text":""},{"location":"api_reference/embedding/#baseembedding","title":"BaseEmbedding","text":""},{"location":"api_reference/embedding/#src.splitter_mr.embedding.base_embedding.BaseEmbedding","title":"<code>BaseEmbedding</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for text embedding providers.</p> <p>Implementations wrap specific backends (e.g., OpenAI, Azure OpenAI, local models) and expose a consistent interface to convert text into numeric vectors suitable for similarity search, clustering, and retrieval-augmented generation.</p> Source code in <code>src/splitter_mr/embedding/base_embedding.py</code> <pre><code>class BaseEmbedding(ABC):\n    \"\"\"\n    Abstract base for text embedding providers.\n\n    Implementations wrap specific backends (e.g., OpenAI, Azure OpenAI, local\n    models) and expose a consistent interface to convert text into numeric\n    vectors suitable for similarity search, clustering, and retrieval-augmented\n    generation.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, model_name: str) -&gt; Any:\n        \"\"\"Initialize the embedding backend.\n\n        Args:\n            model_name (str): Identifier of the embedding model (e.g.,\n                ``\"text-embedding-3-large\"`` or a local model alias/path).\n\n        Raises:\n            ValueError: If required configuration or credentials are missing.\n        \"\"\"\n\n    @abstractmethod\n    def get_client(self) -&gt; Any:\n        \"\"\"Return the underlying client or handle.\n\n        Returns:\n            Any: A client/handle used to perform embedding calls (e.g., an SDK\n                client instance, session object, or local runner). May be ``None``\n                for pure-local implementations that do not require a client.\n        \"\"\"\n\n    @abstractmethod\n    def embed_text(\n        self,\n        text: str,\n        **parameters: Dict[str, Any],\n    ) -&gt; List[float]:\n        \"\"\"\n        Compute an embedding vector for the given text.\n\n        Args:\n            text (str): Input text to embed. Implementations may apply\n                normalization or truncation according to model limits.\n            **parameters (Dict[str, Any]): Additional backend-specific options\n                forwarded to the implementation (e.g., user tags, request IDs).\n\n        Returns:\n            A single embedding vector representing ``text``.\n\n        Raises:\n            ValueError: If ``text`` is empty or exceeds backend constraints.\n            RuntimeError: If the embedding call fails or returns an unexpected\n                response shape.\n        \"\"\"\n\n    def embed_documents(\n        self,\n        texts: List[str],\n        **parameters: Dict[str, Any],\n    ) -&gt; List[List[float]]:\n        \"\"\"Compute embeddings for multiple texts (default loops over `embed_text`).\n\n        Implementations are encouraged to override for true batch performance.\n\n        Args:\n            texts: List of input strings to embed.\n            **parameters: Backend-specific options.\n\n        Returns:\n            List of embedding vectors, one per input string.\n\n        Raises:\n            ValueError: If `texts` is empty or any element is empty.\n        \"\"\"\n        if not texts:\n            raise ValueError(\"`texts` must be a non-empty list of strings.\")\n        return [self.embed_text(t, **parameters) for t in texts]\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.base_embedding.BaseEmbedding.__init__","title":"<code>__init__(model_name)</code>  <code>abstractmethod</code>","text":"<p>Initialize the embedding backend.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Identifier of the embedding model (e.g., <code>\"text-embedding-3-large\"</code> or a local model alias/path).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If required configuration or credentials are missing.</p> Source code in <code>src/splitter_mr/embedding/base_embedding.py</code> <pre><code>@abstractmethod\ndef __init__(self, model_name: str) -&gt; Any:\n    \"\"\"Initialize the embedding backend.\n\n    Args:\n        model_name (str): Identifier of the embedding model (e.g.,\n            ``\"text-embedding-3-large\"`` or a local model alias/path).\n\n    Raises:\n        ValueError: If required configuration or credentials are missing.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.base_embedding.BaseEmbedding.get_client","title":"<code>get_client()</code>  <code>abstractmethod</code>","text":"<p>Return the underlying client or handle.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A client/handle used to perform embedding calls (e.g., an SDK client instance, session object, or local runner). May be <code>None</code> for pure-local implementations that do not require a client.</p> Source code in <code>src/splitter_mr/embedding/base_embedding.py</code> <pre><code>@abstractmethod\ndef get_client(self) -&gt; Any:\n    \"\"\"Return the underlying client or handle.\n\n    Returns:\n        Any: A client/handle used to perform embedding calls (e.g., an SDK\n            client instance, session object, or local runner). May be ``None``\n            for pure-local implementations that do not require a client.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.base_embedding.BaseEmbedding.embed_text","title":"<code>embed_text(text, **parameters)</code>  <code>abstractmethod</code>","text":"<p>Compute an embedding vector for the given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to embed. Implementations may apply normalization or truncation according to model limits.</p> required <code>**parameters</code> <code>Dict[str, Any]</code> <p>Additional backend-specific options forwarded to the implementation (e.g., user tags, request IDs).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>A single embedding vector representing <code>text</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>text</code> is empty or exceeds backend constraints.</p> <code>RuntimeError</code> <p>If the embedding call fails or returns an unexpected response shape.</p> Source code in <code>src/splitter_mr/embedding/base_embedding.py</code> <pre><code>@abstractmethod\ndef embed_text(\n    self,\n    text: str,\n    **parameters: Dict[str, Any],\n) -&gt; List[float]:\n    \"\"\"\n    Compute an embedding vector for the given text.\n\n    Args:\n        text (str): Input text to embed. Implementations may apply\n            normalization or truncation according to model limits.\n        **parameters (Dict[str, Any]): Additional backend-specific options\n            forwarded to the implementation (e.g., user tags, request IDs).\n\n    Returns:\n        A single embedding vector representing ``text``.\n\n    Raises:\n        ValueError: If ``text`` is empty or exceeds backend constraints.\n        RuntimeError: If the embedding call fails or returns an unexpected\n            response shape.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.base_embedding.BaseEmbedding.embed_documents","title":"<code>embed_documents(texts, **parameters)</code>","text":"<p>Compute embeddings for multiple texts (default loops over <code>embed_text</code>).</p> <p>Implementations are encouraged to override for true batch performance.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of input strings to embed.</p> required <code>**parameters</code> <code>Dict[str, Any]</code> <p>Backend-specific options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>List of embedding vectors, one per input string.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>texts</code> is empty or any element is empty.</p> Source code in <code>src/splitter_mr/embedding/base_embedding.py</code> <pre><code>def embed_documents(\n    self,\n    texts: List[str],\n    **parameters: Dict[str, Any],\n) -&gt; List[List[float]]:\n    \"\"\"Compute embeddings for multiple texts (default loops over `embed_text`).\n\n    Implementations are encouraged to override for true batch performance.\n\n    Args:\n        texts: List of input strings to embed.\n        **parameters: Backend-specific options.\n\n    Returns:\n        List of embedding vectors, one per input string.\n\n    Raises:\n        ValueError: If `texts` is empty or any element is empty.\n    \"\"\"\n    if not texts:\n        raise ValueError(\"`texts` must be a non-empty list of strings.\")\n    return [self.embed_text(t, **parameters) for t in texts]\n</code></pre>"},{"location":"api_reference/embedding/#openaiembedding","title":"OpenAIEmbedding","text":""},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.openai_embedding.OpenAIEmbedding","title":"<code>OpenAIEmbedding</code>","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Encoder provider using OpenAI's embeddings API.</p> <p>This class wraps OpenAI's embeddings endpoint, providing convenience methods for both single-text and batch embeddings. It also adds token counting and validation to avoid exceeding model limits.</p> Example <pre><code>from splitter_mr.embedding import OpenAIEmbedding\n\nembedder = OpenAIEmbedding(model_name=\"text-embedding-3-large\")\nvector = embedder.embed_text(\"hello world\")\nprint(vector)\n</code></pre> Source code in <code>src/splitter_mr/embedding/embeddings/openai_embedding.py</code> <pre><code>class OpenAIEmbedding(BaseEmbedding):\n    \"\"\"\n    Encoder provider using OpenAI's embeddings API.\n\n    This class wraps OpenAI's embeddings endpoint, providing convenience\n    methods for both single-text and batch embeddings. It also adds token\n    counting and validation to avoid exceeding model limits.\n\n    Example:\n        ```python\n        from splitter_mr.embedding import OpenAIEmbedding\n\n        embedder = OpenAIEmbedding(model_name=\"text-embedding-3-large\")\n        vector = embedder.embed_text(\"hello world\")\n        print(vector)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"text-embedding-3-large\",\n        api_key: Optional[str] = None,\n        tokenizer_name: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the OpenAI embeddings provider.\n\n        Args:\n            model_name (str):\n                The OpenAI embedding model name (e.g., `\"text-embedding-3-large\"`).\n            api_key (Optional[str]):\n                API key for OpenAI. If not provided, reads from the\n                `OPENAI_API_KEY` environment variable.\n            tokenizer_name (Optional[str]):\n                Optional explicit tokenizer name for `tiktoken`. If provided,\n                this overrides automatic model-to-tokenizer mapping.\n\n        Raises:\n            ValueError: If the API key is not provided or the `OPENAI_API_KEY` environment variable is not set.\n        \"\"\"\n        if api_key is None:\n            api_key = os.getenv(\"OPENAI_API_KEY\")\n            if not api_key:\n                raise ValueError(\n                    \"OpenAI API key not provided or 'OPENAI_API_KEY' env var is not set.\"\n                )\n        self.client = OpenAI(api_key=api_key)\n        self.model_name = model_name\n        self._tokenizer_name = tokenizer_name\n\n    def get_client(self) -&gt; OpenAI:\n        \"\"\"\n        Get the configured OpenAI client.\n\n        Returns:\n            OpenAI: The OpenAI API client instance.\n        \"\"\"\n        return self.client\n\n    def _get_encoder(self):\n        \"\"\"\n        Retrieve the `tiktoken` encoder for the configured model.\n\n        If a `tokenizer_name` is explicitly provided, it is used. Otherwise,\n        attempts to use `tiktoken.encoding_for_model`. If that fails, falls\n        back to the default tokenizer defined by `OPENAI_EMBEDDING_MODEL_FALLBACK`.\n\n        Returns:\n            tiktoken.Encoding: The encoding object for tokenizing text.\n\n        Raises:\n            ValueError: If neither the model-specific nor fallback encoder\n            can be loaded.\n        \"\"\"\n        if self._tokenizer_name:\n            return tiktoken.get_encoding(self._tokenizer_name)\n        try:\n            return tiktoken.encoding_for_model(self.model_name)\n        except Exception:\n            return tiktoken.get_encoding(OPENAI_EMBEDDING_MODEL_FALLBACK)\n\n    def _count_tokens(self, text: str) -&gt; int:\n        \"\"\"\n        Count the number of tokens in the given text.\n\n        Args:\n            text (str): The text to tokenize.\n\n        Returns:\n            int: Number of tokens.\n        \"\"\"\n        encoder = self._get_encoder()\n        return len(encoder.encode(text))\n\n    def _validate_token_length(self, text: str) -&gt; None:\n        \"\"\"\n        Ensure the text does not exceed the model's token limit.\n\n        Args:\n            text (str): The text to check.\n\n        Raises:\n            ValueError: If the token count exceeds `OPENAI_EMBEDDING_MAX_TOKENS`.\n        \"\"\"\n        if self._count_tokens(text) &gt; OPENAI_EMBEDDING_MAX_TOKENS:\n            raise ValueError(\n                f\"Input text exceeds maximum allowed length of {OPENAI_EMBEDDING_MAX_TOKENS} tokens.\"\n            )\n\n    def embed_text(self, text: str, **parameters: Any) -&gt; List[float]:\n        \"\"\"\n        Compute an embedding vector for a single text string.\n\n        Args:\n            text (str):\n                The text to embed. Must be non-empty and within the model's\n                token limit.\n            **parameters:\n                Additional keyword arguments forwarded to\n                `client.embeddings.create(...)`.\n\n        Returns:\n            List[float]: The computed embedding vector.\n\n        Raises:\n            ValueError: If `text` is empty or exceeds the token limit.\n        \"\"\"\n        if not text:\n            raise ValueError(\"`text` must be a non-empty string.\")\n        self._validate_token_length(text)\n\n        response = self.client.embeddings.create(\n            input=text,\n            model=self.model_name,\n            **parameters,\n        )\n        return response.data[0].embedding\n\n    def embed_documents(self, texts: List[str], **parameters: Any) -&gt; List[List[float]]:\n        \"\"\"\n        Compute embeddings for multiple texts in one API call.\n\n        Args:\n            texts (List[str]):\n                List of text strings to embed. All must be non-empty and within\n                the model's token limit.\n            **parameters:\n                Additional keyword arguments forwarded to\n                `client.embeddings.create(...)`.\n\n        Returns:\n            A list of embedding vectors, one per input string.\n\n        Raises:\n            ValueError:\n                - If `texts` is empty.\n                - If any text is empty or not a string.\n                - If any text exceeds the token limit.\n        \"\"\"\n        if not texts:\n            raise ValueError(\"`texts` must be a non-empty list of strings.\")\n        if any(not isinstance(t, str) or not t for t in texts):\n            raise ValueError(\"All items in `texts` must be non-empty strings.\")\n\n        encoder = self._get_encoder()\n        for t in texts:\n            if len(encoder.encode(t)) &gt; OPENAI_EMBEDDING_MAX_TOKENS:\n                raise ValueError(\n                    f\"An input exceeds the maximum allowed length of {OPENAI_EMBEDDING_MAX_TOKENS} tokens.\"\n                )\n\n        response = self.client.embeddings.create(\n            input=texts,\n            model=self.model_name,\n            **parameters,\n        )\n        return [data.embedding for data in response.data]\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.openai_embedding.OpenAIEmbedding.__init__","title":"<code>__init__(model_name='text-embedding-3-large', api_key=None, tokenizer_name=None)</code>","text":"<p>Initialize the OpenAI embeddings provider.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The OpenAI embedding model name (e.g., <code>\"text-embedding-3-large\"</code>).</p> <code>'text-embedding-3-large'</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for OpenAI. If not provided, reads from the <code>OPENAI_API_KEY</code> environment variable.</p> <code>None</code> <code>tokenizer_name</code> <code>Optional[str]</code> <p>Optional explicit tokenizer name for <code>tiktoken</code>. If provided, this overrides automatic model-to-tokenizer mapping.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API key is not provided or the <code>OPENAI_API_KEY</code> environment variable is not set.</p> Source code in <code>src/splitter_mr/embedding/embeddings/openai_embedding.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"text-embedding-3-large\",\n    api_key: Optional[str] = None,\n    tokenizer_name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the OpenAI embeddings provider.\n\n    Args:\n        model_name (str):\n            The OpenAI embedding model name (e.g., `\"text-embedding-3-large\"`).\n        api_key (Optional[str]):\n            API key for OpenAI. If not provided, reads from the\n            `OPENAI_API_KEY` environment variable.\n        tokenizer_name (Optional[str]):\n            Optional explicit tokenizer name for `tiktoken`. If provided,\n            this overrides automatic model-to-tokenizer mapping.\n\n    Raises:\n        ValueError: If the API key is not provided or the `OPENAI_API_KEY` environment variable is not set.\n    \"\"\"\n    if api_key is None:\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"OpenAI API key not provided or 'OPENAI_API_KEY' env var is not set.\"\n            )\n    self.client = OpenAI(api_key=api_key)\n    self.model_name = model_name\n    self._tokenizer_name = tokenizer_name\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.openai_embedding.OpenAIEmbedding.get_client","title":"<code>get_client()</code>","text":"<p>Get the configured OpenAI client.</p> <p>Returns:</p> Name Type Description <code>OpenAI</code> <code>OpenAI</code> <p>The OpenAI API client instance.</p> Source code in <code>src/splitter_mr/embedding/embeddings/openai_embedding.py</code> <pre><code>def get_client(self) -&gt; OpenAI:\n    \"\"\"\n    Get the configured OpenAI client.\n\n    Returns:\n        OpenAI: The OpenAI API client instance.\n    \"\"\"\n    return self.client\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.openai_embedding.OpenAIEmbedding.embed_text","title":"<code>embed_text(text, **parameters)</code>","text":"<p>Compute an embedding vector for a single text string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to embed. Must be non-empty and within the model's token limit.</p> required <code>**parameters</code> <code>Any</code> <p>Additional keyword arguments forwarded to <code>client.embeddings.create(...)</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: The computed embedding vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>text</code> is empty or exceeds the token limit.</p> Source code in <code>src/splitter_mr/embedding/embeddings/openai_embedding.py</code> <pre><code>def embed_text(self, text: str, **parameters: Any) -&gt; List[float]:\n    \"\"\"\n    Compute an embedding vector for a single text string.\n\n    Args:\n        text (str):\n            The text to embed. Must be non-empty and within the model's\n            token limit.\n        **parameters:\n            Additional keyword arguments forwarded to\n            `client.embeddings.create(...)`.\n\n    Returns:\n        List[float]: The computed embedding vector.\n\n    Raises:\n        ValueError: If `text` is empty or exceeds the token limit.\n    \"\"\"\n    if not text:\n        raise ValueError(\"`text` must be a non-empty string.\")\n    self._validate_token_length(text)\n\n    response = self.client.embeddings.create(\n        input=text,\n        model=self.model_name,\n        **parameters,\n    )\n    return response.data[0].embedding\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.openai_embedding.OpenAIEmbedding.embed_documents","title":"<code>embed_documents(texts, **parameters)</code>","text":"<p>Compute embeddings for multiple texts in one API call.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of text strings to embed. All must be non-empty and within the model's token limit.</p> required <code>**parameters</code> <code>Any</code> <p>Additional keyword arguments forwarded to <code>client.embeddings.create(...)</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>A list of embedding vectors, one per input string.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If <code>texts</code> is empty.</li> <li>If any text is empty or not a string.</li> <li>If any text exceeds the token limit.</li> </ul> Source code in <code>src/splitter_mr/embedding/embeddings/openai_embedding.py</code> <pre><code>def embed_documents(self, texts: List[str], **parameters: Any) -&gt; List[List[float]]:\n    \"\"\"\n    Compute embeddings for multiple texts in one API call.\n\n    Args:\n        texts (List[str]):\n            List of text strings to embed. All must be non-empty and within\n            the model's token limit.\n        **parameters:\n            Additional keyword arguments forwarded to\n            `client.embeddings.create(...)`.\n\n    Returns:\n        A list of embedding vectors, one per input string.\n\n    Raises:\n        ValueError:\n            - If `texts` is empty.\n            - If any text is empty or not a string.\n            - If any text exceeds the token limit.\n    \"\"\"\n    if not texts:\n        raise ValueError(\"`texts` must be a non-empty list of strings.\")\n    if any(not isinstance(t, str) or not t for t in texts):\n        raise ValueError(\"All items in `texts` must be non-empty strings.\")\n\n    encoder = self._get_encoder()\n    for t in texts:\n        if len(encoder.encode(t)) &gt; OPENAI_EMBEDDING_MAX_TOKENS:\n            raise ValueError(\n                f\"An input exceeds the maximum allowed length of {OPENAI_EMBEDDING_MAX_TOKENS} tokens.\"\n            )\n\n    response = self.client.embeddings.create(\n        input=texts,\n        model=self.model_name,\n        **parameters,\n    )\n    return [data.embedding for data in response.data]\n</code></pre>"},{"location":"api_reference/embedding/#azureopenaiembedding","title":"AzureOpenAIEmbedding","text":""},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.azure_openai_embedding.AzureOpenAIEmbedding","title":"<code>AzureOpenAIEmbedding</code>","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Encoder provider using Azure OpenAI Embeddings.</p> <p>This class wraps Azure OpenAI's embeddings API, handling both authentication and tokenization. It supports both direct embedding calls for a single text (<code>embed_text</code>) and batch embedding calls (<code>embed_documents</code>).</p> <p>Azure deployments use deployment names (e.g., <code>my-embedding-deployment</code>) instead of OpenAI's standard model names. Since <code>tiktoken</code> may not be able to map a deployment name to a tokenizer automatically, this class implements a fallback mechanism to use a known encoding (e.g., <code>cl100k_base</code>) when necessary.</p> Example <pre><code>from splitter_mr.embedding import AzureOpenAIEmbedding\n\nembedder = AzureOpenAIEmbedding(\n    azure_deployment=\"text-embedding-3-large\",\n    api_key=\"...\",\n    azure_endpoint=\"https://my-azure-endpoint.openai.azure.com/\"\n)\nvector = embedder.embed_text(\"Hello world\")\n</code></pre> Source code in <code>src/splitter_mr/embedding/embeddings/azure_openai_embedding.py</code> <pre><code>class AzureOpenAIEmbedding(BaseEmbedding):\n    \"\"\"\n    Encoder provider using Azure OpenAI Embeddings.\n\n    This class wraps Azure OpenAI's embeddings API, handling both authentication\n    and tokenization. It supports both direct embedding calls for a single text\n    (`embed_text`) and batch embedding calls (`embed_documents`).\n\n    Azure deployments use *deployment names* (e.g., `my-embedding-deployment`)\n    instead of OpenAI's standard model names. Since `tiktoken` may not be able to\n    map a deployment name to a tokenizer automatically, this class implements\n    a fallback mechanism to use a known encoding (e.g., `cl100k_base`) when necessary.\n\n    Example:\n        ```python\n        from splitter_mr.embedding import AzureOpenAIEmbedding\n\n        embedder = AzureOpenAIEmbedding(\n            azure_deployment=\"text-embedding-3-large\",\n            api_key=\"...\",\n            azure_endpoint=\"https://my-azure-endpoint.openai.azure.com/\"\n        )\n        vector = embedder.embed_text(\"Hello world\")\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: Optional[str] = None,\n        api_key: Optional[str] = None,\n        azure_endpoint: Optional[str] = None,\n        azure_deployment: Optional[str] = None,\n        api_version: Optional[str] = None,\n        tokenizer_name: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Azure OpenAI Embedding provider.\n\n        Args:\n            model_name (Optional[str]):\n                OpenAI model name (unused for Azure, but kept for API parity).\n                If `azure_deployment` is not provided, this will be used as the\n                deployment name.\n            api_key (Optional[str]):\n                API key for Azure OpenAI. If not provided, it will be read from\n                the environment variable `AZURE_OPENAI_API_KEY`.\n            azure_endpoint (Optional[str]):\n                The base endpoint for the Azure OpenAI service. If not provided,\n                it will be read from `AZURE_OPENAI_ENDPOINT`.\n            azure_deployment (Optional[str]):\n                Deployment name for the embeddings model in Azure OpenAI. If not\n                provided, it will be read from `AZURE_OPENAI_DEPLOYMENT` or\n                fallback to `model_name`.\n            api_version (Optional[str]):\n                Azure API version string. Defaults to `\"2025-04-14-preview\"`.\n                If not provided, it will be read from `AZURE_OPENAI_API_VERSION`.\n            tokenizer_name (Optional[str]):\n                Optional explicit tokenizer name for `tiktoken` (e.g.,\n                `\"cl100k_base\"`). If provided, it overrides the automatic mapping.\n\n        Raises:\n            ValueError: If any required parameter is missing or it is not found in environment variables.\n        \"\"\"\n        if api_key is None:\n            api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n            if not api_key:\n                raise ValueError(\n                    \"Azure OpenAI API key not provided or 'AZURE_OPENAI_API_KEY' env var is not set.\"\n                )\n\n        if azure_endpoint is None:\n            azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n            if not azure_endpoint:\n                raise ValueError(\n                    \"Azure endpoint not provided or 'AZURE_OPENAI_ENDPOINT' env var is not set.\"\n                )\n\n        if azure_deployment is None:\n            azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or model_name\n            if not azure_deployment:\n                raise ValueError(\n                    \"Azure deployment name not provided. Set 'azure_deployment', \"\n                    \"'AZURE_OPENAI_DEPLOYMENT', or pass `model_name`.\"\n                )\n\n        if api_version is None:\n            api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-04-14-preview\")\n\n        self.client = AzureOpenAI(\n            api_key=api_key,\n            azure_endpoint=azure_endpoint,\n            azure_deployment=azure_deployment,\n            api_version=api_version,\n        )\n        self.model_name = azure_deployment\n        self._tokenizer_name = tokenizer_name\n\n    def get_client(self) -&gt; AzureOpenAI:\n        \"\"\"\n        Get the underlying Azure OpenAI client.\n\n        Returns:\n            AzureOpenAI: The configured Azure OpenAI API client.\n        \"\"\"\n        return self.client\n\n    def _get_encoder(self):\n        \"\"\"\n        Retrieve the `tiktoken` encoder for this deployment.\n\n        This method ensures compatibility with Azure's deployment names, which\n        may not be directly recognized by `tiktoken`. If the user has explicitly\n        provided a tokenizer name, that is used. Otherwise, the method first\n        tries to look up the encoding via `tiktoken.encoding_for_model` using the\n        deployment name. If that fails, it falls back to the default encoding\n        defined by `OPENAI_EMBEDDING_MODEL_FALLBACK`.\n\n        Returns:\n            tiktoken.Encoding: A tokenizer encoding object.\n\n        Raises:\n            ValueError: If `tiktoken` fails to load the fallback encoding.\n        \"\"\"\n        if self._tokenizer_name:\n            return tiktoken.get_encoding(self._tokenizer_name)\n        try:\n            return tiktoken.encoding_for_model(self.model_name)\n        except Exception:\n            return tiktoken.get_encoding(OPENAI_EMBEDDING_MODEL_FALLBACK)\n\n    def _count_tokens(self, text: str) -&gt; int:\n        \"\"\"\n        Count the number of tokens in the given text.\n\n        Uses the encoder retrieved from `_get_encoder()` to tokenize the input\n        and returns the length of the resulting token list.\n\n        Args:\n            text (str): The text to tokenize.\n\n        Returns:\n            int: Number of tokens in the input text.\n        \"\"\"\n        encoder = self._get_encoder()\n        return len(encoder.encode(text))\n\n    def _validate_token_length(self, text: str) -&gt; None:\n        \"\"\"\n        Ensure the input text does not exceed the model's maximum token limit.\n\n        Args:\n            text (str): The text to check.\n\n        Raises:\n            ValueError: If the token count exceeds `OPENAI_EMBEDDING_MAX_TOKENS`.\n        \"\"\"\n        if self._count_tokens(text) &gt; OPENAI_EMBEDDING_MAX_TOKENS:\n            raise ValueError(\n                f\"Input text exceeds maximum allowed length of {OPENAI_EMBEDDING_MAX_TOKENS} tokens.\"\n            )\n\n    def embed_text(self, text: str, **parameters: Any) -&gt; List[float]:\n        \"\"\"\n        Compute an embedding vector for a single text string.\n\n        Args:\n            text (str):\n                The text to embed. Must be non-empty and within the model's\n                token limit.\n            **parameters:\n                Additional parameters to forward to the Azure OpenAI embeddings API.\n\n        Returns:\n            List[float]: The computed embedding vector.\n\n        Raises:\n            ValueError: If `text` is empty or exceeds the token limit.\n        \"\"\"\n        if not text:\n            raise ValueError(\"`text` must be a non-empty string.\")\n        self._validate_token_length(text)\n        response = self.client.embeddings.create(\n            model=self.model_name,\n            input=text,\n            **parameters,\n        )\n        return response.data[0].embedding\n\n    def embed_documents(self, texts: List[str], **parameters: Any) -&gt; List[List[float]]:\n        \"\"\"\n        Compute embeddings for multiple texts in a single API call.\n\n        Args:\n            texts (List[str]):\n                List of text strings to embed. All items must be non-empty strings\n                within the token limit.\n            **parameters:\n                Additional parameters to forward to the Azure OpenAI embeddings API.\n\n        Returns:\n            A list of embedding vectors, one per input text.\n\n        Raises:\n            ValueError:\n                - If `texts` is empty.\n                - If any text is empty or not a string.\n                - If any text exceeds the token limit.\n        \"\"\"\n        if not texts:\n            raise ValueError(\"`texts` must be a non-empty list of strings.\")\n        if any(not isinstance(t, str) or not t for t in texts):\n            raise ValueError(\"All items in `texts` must be non-empty strings.\")\n\n        encoder = self._get_encoder()\n        for t in texts:\n            if len(encoder.encode(t)) &gt; OPENAI_EMBEDDING_MAX_TOKENS:\n                raise ValueError(\n                    f\"An input exceeds the maximum allowed length of {OPENAI_EMBEDDING_MAX_TOKENS} tokens.\"\n                )\n\n        response = self.client.embeddings.create(\n            model=self.model_name,\n            input=texts,\n            **parameters,\n        )\n        return [data.embedding for data in response.data]\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.azure_openai_embedding.AzureOpenAIEmbedding.__init__","title":"<code>__init__(model_name=None, api_key=None, azure_endpoint=None, azure_deployment=None, api_version=None, tokenizer_name=None)</code>","text":"<p>Initialize the Azure OpenAI Embedding provider.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>Optional[str]</code> <p>OpenAI model name (unused for Azure, but kept for API parity). If <code>azure_deployment</code> is not provided, this will be used as the deployment name.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for Azure OpenAI. If not provided, it will be read from the environment variable <code>AZURE_OPENAI_API_KEY</code>.</p> <code>None</code> <code>azure_endpoint</code> <code>Optional[str]</code> <p>The base endpoint for the Azure OpenAI service. If not provided, it will be read from <code>AZURE_OPENAI_ENDPOINT</code>.</p> <code>None</code> <code>azure_deployment</code> <code>Optional[str]</code> <p>Deployment name for the embeddings model in Azure OpenAI. If not provided, it will be read from <code>AZURE_OPENAI_DEPLOYMENT</code> or fallback to <code>model_name</code>.</p> <code>None</code> <code>api_version</code> <code>Optional[str]</code> <p>Azure API version string. Defaults to <code>\"2025-04-14-preview\"</code>. If not provided, it will be read from <code>AZURE_OPENAI_API_VERSION</code>.</p> <code>None</code> <code>tokenizer_name</code> <code>Optional[str]</code> <p>Optional explicit tokenizer name for <code>tiktoken</code> (e.g., <code>\"cl100k_base\"</code>). If provided, it overrides the automatic mapping.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any required parameter is missing or it is not found in environment variables.</p> Source code in <code>src/splitter_mr/embedding/embeddings/azure_openai_embedding.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    api_key: Optional[str] = None,\n    azure_endpoint: Optional[str] = None,\n    azure_deployment: Optional[str] = None,\n    api_version: Optional[str] = None,\n    tokenizer_name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the Azure OpenAI Embedding provider.\n\n    Args:\n        model_name (Optional[str]):\n            OpenAI model name (unused for Azure, but kept for API parity).\n            If `azure_deployment` is not provided, this will be used as the\n            deployment name.\n        api_key (Optional[str]):\n            API key for Azure OpenAI. If not provided, it will be read from\n            the environment variable `AZURE_OPENAI_API_KEY`.\n        azure_endpoint (Optional[str]):\n            The base endpoint for the Azure OpenAI service. If not provided,\n            it will be read from `AZURE_OPENAI_ENDPOINT`.\n        azure_deployment (Optional[str]):\n            Deployment name for the embeddings model in Azure OpenAI. If not\n            provided, it will be read from `AZURE_OPENAI_DEPLOYMENT` or\n            fallback to `model_name`.\n        api_version (Optional[str]):\n            Azure API version string. Defaults to `\"2025-04-14-preview\"`.\n            If not provided, it will be read from `AZURE_OPENAI_API_VERSION`.\n        tokenizer_name (Optional[str]):\n            Optional explicit tokenizer name for `tiktoken` (e.g.,\n            `\"cl100k_base\"`). If provided, it overrides the automatic mapping.\n\n    Raises:\n        ValueError: If any required parameter is missing or it is not found in environment variables.\n    \"\"\"\n    if api_key is None:\n        api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"Azure OpenAI API key not provided or 'AZURE_OPENAI_API_KEY' env var is not set.\"\n            )\n\n    if azure_endpoint is None:\n        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n        if not azure_endpoint:\n            raise ValueError(\n                \"Azure endpoint not provided or 'AZURE_OPENAI_ENDPOINT' env var is not set.\"\n            )\n\n    if azure_deployment is None:\n        azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or model_name\n        if not azure_deployment:\n            raise ValueError(\n                \"Azure deployment name not provided. Set 'azure_deployment', \"\n                \"'AZURE_OPENAI_DEPLOYMENT', or pass `model_name`.\"\n            )\n\n    if api_version is None:\n        api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-04-14-preview\")\n\n    self.client = AzureOpenAI(\n        api_key=api_key,\n        azure_endpoint=azure_endpoint,\n        azure_deployment=azure_deployment,\n        api_version=api_version,\n    )\n    self.model_name = azure_deployment\n    self._tokenizer_name = tokenizer_name\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.azure_openai_embedding.AzureOpenAIEmbedding.get_client","title":"<code>get_client()</code>","text":"<p>Get the underlying Azure OpenAI client.</p> <p>Returns:</p> Name Type Description <code>AzureOpenAI</code> <code>AzureOpenAI</code> <p>The configured Azure OpenAI API client.</p> Source code in <code>src/splitter_mr/embedding/embeddings/azure_openai_embedding.py</code> <pre><code>def get_client(self) -&gt; AzureOpenAI:\n    \"\"\"\n    Get the underlying Azure OpenAI client.\n\n    Returns:\n        AzureOpenAI: The configured Azure OpenAI API client.\n    \"\"\"\n    return self.client\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.azure_openai_embedding.AzureOpenAIEmbedding.embed_text","title":"<code>embed_text(text, **parameters)</code>","text":"<p>Compute an embedding vector for a single text string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to embed. Must be non-empty and within the model's token limit.</p> required <code>**parameters</code> <code>Any</code> <p>Additional parameters to forward to the Azure OpenAI embeddings API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: The computed embedding vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>text</code> is empty or exceeds the token limit.</p> Source code in <code>src/splitter_mr/embedding/embeddings/azure_openai_embedding.py</code> <pre><code>def embed_text(self, text: str, **parameters: Any) -&gt; List[float]:\n    \"\"\"\n    Compute an embedding vector for a single text string.\n\n    Args:\n        text (str):\n            The text to embed. Must be non-empty and within the model's\n            token limit.\n        **parameters:\n            Additional parameters to forward to the Azure OpenAI embeddings API.\n\n    Returns:\n        List[float]: The computed embedding vector.\n\n    Raises:\n        ValueError: If `text` is empty or exceeds the token limit.\n    \"\"\"\n    if not text:\n        raise ValueError(\"`text` must be a non-empty string.\")\n    self._validate_token_length(text)\n    response = self.client.embeddings.create(\n        model=self.model_name,\n        input=text,\n        **parameters,\n    )\n    return response.data[0].embedding\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.azure_openai_embedding.AzureOpenAIEmbedding.embed_documents","title":"<code>embed_documents(texts, **parameters)</code>","text":"<p>Compute embeddings for multiple texts in a single API call.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of text strings to embed. All items must be non-empty strings within the token limit.</p> required <code>**parameters</code> <code>Any</code> <p>Additional parameters to forward to the Azure OpenAI embeddings API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>A list of embedding vectors, one per input text.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If <code>texts</code> is empty.</li> <li>If any text is empty or not a string.</li> <li>If any text exceeds the token limit.</li> </ul> Source code in <code>src/splitter_mr/embedding/embeddings/azure_openai_embedding.py</code> <pre><code>def embed_documents(self, texts: List[str], **parameters: Any) -&gt; List[List[float]]:\n    \"\"\"\n    Compute embeddings for multiple texts in a single API call.\n\n    Args:\n        texts (List[str]):\n            List of text strings to embed. All items must be non-empty strings\n            within the token limit.\n        **parameters:\n            Additional parameters to forward to the Azure OpenAI embeddings API.\n\n    Returns:\n        A list of embedding vectors, one per input text.\n\n    Raises:\n        ValueError:\n            - If `texts` is empty.\n            - If any text is empty or not a string.\n            - If any text exceeds the token limit.\n    \"\"\"\n    if not texts:\n        raise ValueError(\"`texts` must be a non-empty list of strings.\")\n    if any(not isinstance(t, str) or not t for t in texts):\n        raise ValueError(\"All items in `texts` must be non-empty strings.\")\n\n    encoder = self._get_encoder()\n    for t in texts:\n        if len(encoder.encode(t)) &gt; OPENAI_EMBEDDING_MAX_TOKENS:\n            raise ValueError(\n                f\"An input exceeds the maximum allowed length of {OPENAI_EMBEDDING_MAX_TOKENS} tokens.\"\n            )\n\n    response = self.client.embeddings.create(\n        model=self.model_name,\n        input=texts,\n        **parameters,\n    )\n    return [data.embedding for data in response.data]\n</code></pre>"},{"location":"api_reference/embedding/#geminiembedding","title":"GeminiEmbedding","text":""},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.gemini_embedding.GeminiEmbedding","title":"<code>GeminiEmbedding</code>","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Embedding provider using Google Gemini's embedding API.</p> <p>This class wraps the Gemini API for generating embeddings from text or documents. Requires the <code>google-genai</code> package and a valid Gemini API key. This class is available only if <code>splitter-mr[multimodal]</code> is installed.</p> Typical usage example <pre><code>from splitter_mr.embedding.models.gemini_embedding import GeminiEmbedding\nembedder = GeminiEmbedding(api_key=\"your-api-key\")\nvector = embedder.embed_text(\"Hello, world!\")\nprint(vector)\n</code></pre> Source code in <code>src/splitter_mr/embedding/embeddings/gemini_embedding.py</code> <pre><code>class GeminiEmbedding(BaseEmbedding):\n    \"\"\"\n    Embedding provider using Google Gemini's embedding API.\n\n    This class wraps the Gemini API for generating embeddings from text or documents.\n    Requires the `google-genai` package and a valid Gemini API key. This class\n    is available only if `splitter-mr[multimodal]` is installed.\n\n    Typical usage example:\n        ```python\n        from splitter_mr.embedding.models.gemini_embedding import GeminiEmbedding\n        embedder = GeminiEmbedding(api_key=\"your-api-key\")\n        vector = embedder.embed_text(\"Hello, world!\")\n        print(vector)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"models/embedding-001\",\n        api_key: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Gemini embedding provider.\n\n        Args:\n            model_name (str): The Gemini model identifier to use for embedding. Defaults to \"models/embedding-001\".\n            api_key (Optional[str]): The Gemini API key. If not provided, reads from the 'GEMINI_API_KEY' environment variable.\n\n        Raises:\n            ImportError: If the `google-genai` package is not installed.\n            ValueError: If no API key is provided or found in the environment.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"GEMINI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\n                \"Google Gemini API key not provided and 'GEMINI_API_KEY' environment variable not set.\"\n            )\n        self.model_name = model_name\n        self.client = genai.Client(api_key=api_key)\n        self.models = self.client.models\n\n    def get_client(self) -&gt; \"genai.Client\":\n        \"\"\"\n        Return the underlying Gemini API client.\n\n        Returns:\n            The loaded Gemini API module (`google.genai`).\n        \"\"\"\n        return self.client\n\n    def embed_text(self, text: str, **parameters: Any) -&gt; List[float]:\n        \"\"\"\n        Generate an embedding for a single text string using Gemini.\n\n        Args:\n            text (str): The input text to embed.\n            **parameters (Any): Additional parameters for the Gemini API.\n\n        Returns:\n            List[float]: The generated embedding vector.\n\n        Raises:\n            ValueError: If the input text is not a non-empty string.\n            RuntimeError: If the embedding call fails or returns an invalid response.\n        \"\"\"\n        if not isinstance(text, str) or not text.strip():\n            raise ValueError(\"`text` must be a non-empty string.\")\n\n        try:\n            result = self.models.embed_content(\n                model=self.model_name, contents=text, **parameters\n            )\n            embedding = getattr(result, \"embedding\", None)\n            if embedding is None:\n                raise RuntimeError(\n                    \"Gemini embedding call succeeded but no 'embedding' field was returned.\"\n                )\n            return embedding\n        except Exception as e:\n            raise RuntimeError(f\"Failed to get embedding from Gemini: {e}\") from e\n\n    def embed_documents(self, texts: List[str], **parameters: Any) -&gt; List[List[float]]:\n        \"\"\"\n        Generate embeddings for a list of text strings using Gemini.\n\n        Args:\n            texts (List[str]): A list of input text strings.\n            **parameters (Any): Additional parameters for the Gemini API.\n\n        Returns:\n            List[List[float]]: The generated embedding vectors, one per input.\n\n        Raises:\n            ValueError: If the input is not a non-empty list of non-empty strings.\n            RuntimeError: If the embedding call fails or returns an invalid response.\n        \"\"\"\n        if (\n            not isinstance(texts, list)\n            or not texts  # noqa: W503\n            or any(not isinstance(t, str) or not t.strip() for t in texts)  # noqa: W503\n        ):\n            raise ValueError(\"`texts` must be a non-empty list of non-empty strings.\")\n\n        try:\n            result = self.models.embed_content(\n                model=self.model_name, contents=texts, **parameters\n            )\n            # The Gemini API returns a list of embeddings under .embeddings\n            embeddings = getattr(result, \"embeddings\", None)\n            if embeddings is None:\n                raise RuntimeError(\n                    \"Gemini embedding call succeeded but no 'embeddings' field was returned.\"\n                )\n            return embeddings\n\n        except Exception as e:\n            raise RuntimeError(\n                f\"Failed to get document embeddings from Gemini: {e}\"\n            ) from e\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.gemini_embedding.GeminiEmbedding.__init__","title":"<code>__init__(model_name='models/embedding-001', api_key=None)</code>","text":"<p>Initialize the Gemini embedding provider.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The Gemini model identifier to use for embedding. Defaults to \"models/embedding-001\".</p> <code>'models/embedding-001'</code> <code>api_key</code> <code>Optional[str]</code> <p>The Gemini API key. If not provided, reads from the 'GEMINI_API_KEY' environment variable.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the <code>google-genai</code> package is not installed.</p> <code>ValueError</code> <p>If no API key is provided or found in the environment.</p> Source code in <code>src/splitter_mr/embedding/embeddings/gemini_embedding.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"models/embedding-001\",\n    api_key: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the Gemini embedding provider.\n\n    Args:\n        model_name (str): The Gemini model identifier to use for embedding. Defaults to \"models/embedding-001\".\n        api_key (Optional[str]): The Gemini API key. If not provided, reads from the 'GEMINI_API_KEY' environment variable.\n\n    Raises:\n        ImportError: If the `google-genai` package is not installed.\n        ValueError: If no API key is provided or found in the environment.\n    \"\"\"\n    self.api_key = api_key or os.getenv(\"GEMINI_API_KEY\")\n    if not self.api_key:\n        raise ValueError(\n            \"Google Gemini API key not provided and 'GEMINI_API_KEY' environment variable not set.\"\n        )\n    self.model_name = model_name\n    self.client = genai.Client(api_key=api_key)\n    self.models = self.client.models\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.gemini_embedding.GeminiEmbedding.get_client","title":"<code>get_client()</code>","text":"<p>Return the underlying Gemini API client.</p> <p>Returns:</p> Type Description <code>Client</code> <p>The loaded Gemini API module (<code>google.genai</code>).</p> Source code in <code>src/splitter_mr/embedding/embeddings/gemini_embedding.py</code> <pre><code>def get_client(self) -&gt; \"genai.Client\":\n    \"\"\"\n    Return the underlying Gemini API client.\n\n    Returns:\n        The loaded Gemini API module (`google.genai`).\n    \"\"\"\n    return self.client\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.gemini_embedding.GeminiEmbedding.embed_text","title":"<code>embed_text(text, **parameters)</code>","text":"<p>Generate an embedding for a single text string using Gemini.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to embed.</p> required <code>**parameters</code> <code>Any</code> <p>Additional parameters for the Gemini API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: The generated embedding vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input text is not a non-empty string.</p> <code>RuntimeError</code> <p>If the embedding call fails or returns an invalid response.</p> Source code in <code>src/splitter_mr/embedding/embeddings/gemini_embedding.py</code> <pre><code>def embed_text(self, text: str, **parameters: Any) -&gt; List[float]:\n    \"\"\"\n    Generate an embedding for a single text string using Gemini.\n\n    Args:\n        text (str): The input text to embed.\n        **parameters (Any): Additional parameters for the Gemini API.\n\n    Returns:\n        List[float]: The generated embedding vector.\n\n    Raises:\n        ValueError: If the input text is not a non-empty string.\n        RuntimeError: If the embedding call fails or returns an invalid response.\n    \"\"\"\n    if not isinstance(text, str) or not text.strip():\n        raise ValueError(\"`text` must be a non-empty string.\")\n\n    try:\n        result = self.models.embed_content(\n            model=self.model_name, contents=text, **parameters\n        )\n        embedding = getattr(result, \"embedding\", None)\n        if embedding is None:\n            raise RuntimeError(\n                \"Gemini embedding call succeeded but no 'embedding' field was returned.\"\n            )\n        return embedding\n    except Exception as e:\n        raise RuntimeError(f\"Failed to get embedding from Gemini: {e}\") from e\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.gemini_embedding.GeminiEmbedding.embed_documents","title":"<code>embed_documents(texts, **parameters)</code>","text":"<p>Generate embeddings for a list of text strings using Gemini.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>A list of input text strings.</p> required <code>**parameters</code> <code>Any</code> <p>Additional parameters for the Gemini API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>List[List[float]]: The generated embedding vectors, one per input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input is not a non-empty list of non-empty strings.</p> <code>RuntimeError</code> <p>If the embedding call fails or returns an invalid response.</p> Source code in <code>src/splitter_mr/embedding/embeddings/gemini_embedding.py</code> <pre><code>def embed_documents(self, texts: List[str], **parameters: Any) -&gt; List[List[float]]:\n    \"\"\"\n    Generate embeddings for a list of text strings using Gemini.\n\n    Args:\n        texts (List[str]): A list of input text strings.\n        **parameters (Any): Additional parameters for the Gemini API.\n\n    Returns:\n        List[List[float]]: The generated embedding vectors, one per input.\n\n    Raises:\n        ValueError: If the input is not a non-empty list of non-empty strings.\n        RuntimeError: If the embedding call fails or returns an invalid response.\n    \"\"\"\n    if (\n        not isinstance(texts, list)\n        or not texts  # noqa: W503\n        or any(not isinstance(t, str) or not t.strip() for t in texts)  # noqa: W503\n    ):\n        raise ValueError(\"`texts` must be a non-empty list of non-empty strings.\")\n\n    try:\n        result = self.models.embed_content(\n            model=self.model_name, contents=texts, **parameters\n        )\n        # The Gemini API returns a list of embeddings under .embeddings\n        embeddings = getattr(result, \"embeddings\", None)\n        if embeddings is None:\n            raise RuntimeError(\n                \"Gemini embedding call succeeded but no 'embeddings' field was returned.\"\n            )\n        return embeddings\n\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to get document embeddings from Gemini: {e}\"\n        ) from e\n</code></pre>"},{"location":"api_reference/embedding/#anthropicembedding","title":"AnthropicEmbedding","text":""},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.anthropic_embedding.AnthropicEmbedding","title":"<code>AnthropicEmbedding</code>","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Embedding provider aligned with Anthropic's guidance, implemented via Voyage AI.</p> <p>Anthropic does not offer a native embeddings API; their docs recommend using third-party providers such as Voyage AI for high-quality, domain-specific, and multimodal embeddings. This class wraps Voyage's Python SDK to provide a consistent interface that matches <code>BaseEmbedding</code>.</p> Example <pre><code>from splitter_mr.embedding import AnthropicEmbeddings\n\nembedder = AnthropicEmbeddings(model_name=\"voyage-3.5\")\nvec = embedder.embed_text(\"hello world\", input_type=\"document\")\nprint(len(vec))\n</code></pre> Source code in <code>src/splitter_mr/embedding/embeddings/anthropic_embedding.py</code> <pre><code>class AnthropicEmbedding(BaseEmbedding):\n    \"\"\"\n    Embedding provider aligned with Anthropic's guidance, implemented via Voyage AI.\n\n    Anthropic does not offer a native embeddings API; their docs recommend using\n    third-party providers such as **Voyage AI** for high-quality, domain-specific,\n    and multimodal embeddings. This class wraps Voyage's Python SDK to provide a\n    consistent interface that matches `BaseEmbedding`.\n\n    Example:\n        ```python\n        from splitter_mr.embedding import AnthropicEmbeddings\n\n        embedder = AnthropicEmbeddings(model_name=\"voyage-3.5\")\n        vec = embedder.embed_text(\"hello world\", input_type=\"document\")\n        print(len(vec))\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"voyage-3.5\",\n        api_key: Optional[str] = None,\n        default_input_type: Optional[str] = \"document\",\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Voyage embeddings provider.\n\n        Args:\n            model_name:\n                Voyage embedding model name (e.g., \"voyage-3.5\", \"voyage-3-large\",\n                \"voyage-code-3\", \"voyage-finance-2\", \"voyage-law-2\").\n            api_key:\n                Voyage API key. If not provided, reads from the `VOYAGE_API_KEY`\n                environment variable.\n            default_input_type:\n                Default for Voyage's `input_type` parameter (\"document\" | \"query\").\n\n        Raises:\n            ImportError: If the `multimodal` extra (with `voyageai`) is not installed.\n            ValueError: If no API key is provided or found in the environment.\n        \"\"\"\n\n        if api_key is None:\n            api_key = os.getenv(\"VOYAGE_API_KEY\")\n            if not api_key:\n                raise ValueError(\n                    \"Voyage API key not provided and 'VOYAGE_API_KEY' environment variable is not set.\"\n                )\n\n        self.client = voyageai.Client(api_key=api_key)\n        self.model_name = model_name\n        self.default_input_type = default_input_type\n\n    def get_client(self) -&gt; Any:\n        \"\"\"Return the underlying Voyage client.\"\"\"\n        return self.client\n\n    def _ensure_input_type(self, parameters: dict) -&gt; dict:\n        \"\"\"Default `input_type` to self.default_input_type if not set.\"\"\"\n        params = dict(parameters) if parameters else {}\n        if \"input_type\" not in params and self.default_input_type:\n            params[\"input_type\"] = self.default_input_type\n        return params\n\n    def embed_text(self, text: str, **parameters: Any) -&gt; List[float]:\n        \"\"\"Compute an embedding vector for a single text string.\"\"\"\n        if not isinstance(text, str) or not text.strip():\n            raise ValueError(\"`text` must be a non-empty string.\")\n\n        params = self._ensure_input_type(parameters)\n        result = self.client.embed([text], model=self.model_name, **params)\n\n        if not hasattr(result, \"embeddings\") or not result.embeddings:\n            raise RuntimeError(\n                \"Voyage returned an empty or malformed embeddings response.\"\n            )\n\n        embedding = result.embeddings[0]\n        if not isinstance(embedding, list) or not embedding:\n            raise RuntimeError(\"Voyage returned an invalid embedding vector.\")\n\n        return embedding\n\n    def embed_documents(self, texts: List[str], **parameters: Any) -&gt; List[List[float]]:\n        \"\"\"Compute embeddings for multiple texts in one API call.\"\"\"\n        if not texts:\n            raise ValueError(\"`texts` must be a non-empty list of strings.\")\n        if any(not isinstance(t, str) or not t.strip() for t in texts):\n            raise ValueError(\"All items in `texts` must be non-empty strings.\")\n\n        params = self._ensure_input_type(parameters)\n        result = self.client.embed(texts, model=self.model_name, **params)\n\n        if not hasattr(result, \"embeddings\") or not result.embeddings:\n            raise RuntimeError(\n                \"Voyage returned an empty or malformed embeddings response.\"\n            )\n\n        if len(result.embeddings) != len(texts):\n            raise RuntimeError(\n                f\"Voyage returned {len(result.embeddings)} embeddings for {len(texts)} inputs.\"\n            )\n\n        embeddings = result.embeddings\n\n        return embeddings\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.anthropic_embedding.AnthropicEmbedding.__init__","title":"<code>__init__(model_name='voyage-3.5', api_key=None, default_input_type='document')</code>","text":"<p>Initialize the Voyage embeddings provider.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Voyage embedding model name (e.g., \"voyage-3.5\", \"voyage-3-large\", \"voyage-code-3\", \"voyage-finance-2\", \"voyage-law-2\").</p> <code>'voyage-3.5'</code> <code>api_key</code> <code>Optional[str]</code> <p>Voyage API key. If not provided, reads from the <code>VOYAGE_API_KEY</code> environment variable.</p> <code>None</code> <code>default_input_type</code> <code>Optional[str]</code> <p>Default for Voyage's <code>input_type</code> parameter (\"document\" | \"query\").</p> <code>'document'</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the <code>multimodal</code> extra (with <code>voyageai</code>) is not installed.</p> <code>ValueError</code> <p>If no API key is provided or found in the environment.</p> Source code in <code>src/splitter_mr/embedding/embeddings/anthropic_embedding.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"voyage-3.5\",\n    api_key: Optional[str] = None,\n    default_input_type: Optional[str] = \"document\",\n) -&gt; None:\n    \"\"\"\n    Initialize the Voyage embeddings provider.\n\n    Args:\n        model_name:\n            Voyage embedding model name (e.g., \"voyage-3.5\", \"voyage-3-large\",\n            \"voyage-code-3\", \"voyage-finance-2\", \"voyage-law-2\").\n        api_key:\n            Voyage API key. If not provided, reads from the `VOYAGE_API_KEY`\n            environment variable.\n        default_input_type:\n            Default for Voyage's `input_type` parameter (\"document\" | \"query\").\n\n    Raises:\n        ImportError: If the `multimodal` extra (with `voyageai`) is not installed.\n        ValueError: If no API key is provided or found in the environment.\n    \"\"\"\n\n    if api_key is None:\n        api_key = os.getenv(\"VOYAGE_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"Voyage API key not provided and 'VOYAGE_API_KEY' environment variable is not set.\"\n            )\n\n    self.client = voyageai.Client(api_key=api_key)\n    self.model_name = model_name\n    self.default_input_type = default_input_type\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.anthropic_embedding.AnthropicEmbedding.get_client","title":"<code>get_client()</code>","text":"<p>Return the underlying Voyage client.</p> Source code in <code>src/splitter_mr/embedding/embeddings/anthropic_embedding.py</code> <pre><code>def get_client(self) -&gt; Any:\n    \"\"\"Return the underlying Voyage client.\"\"\"\n    return self.client\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.anthropic_embedding.AnthropicEmbedding.embed_text","title":"<code>embed_text(text, **parameters)</code>","text":"<p>Compute an embedding vector for a single text string.</p> Source code in <code>src/splitter_mr/embedding/embeddings/anthropic_embedding.py</code> <pre><code>def embed_text(self, text: str, **parameters: Any) -&gt; List[float]:\n    \"\"\"Compute an embedding vector for a single text string.\"\"\"\n    if not isinstance(text, str) or not text.strip():\n        raise ValueError(\"`text` must be a non-empty string.\")\n\n    params = self._ensure_input_type(parameters)\n    result = self.client.embed([text], model=self.model_name, **params)\n\n    if not hasattr(result, \"embeddings\") or not result.embeddings:\n        raise RuntimeError(\n            \"Voyage returned an empty or malformed embeddings response.\"\n        )\n\n    embedding = result.embeddings[0]\n    if not isinstance(embedding, list) or not embedding:\n        raise RuntimeError(\"Voyage returned an invalid embedding vector.\")\n\n    return embedding\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.anthropic_embedding.AnthropicEmbedding.embed_documents","title":"<code>embed_documents(texts, **parameters)</code>","text":"<p>Compute embeddings for multiple texts in one API call.</p> Source code in <code>src/splitter_mr/embedding/embeddings/anthropic_embedding.py</code> <pre><code>def embed_documents(self, texts: List[str], **parameters: Any) -&gt; List[List[float]]:\n    \"\"\"Compute embeddings for multiple texts in one API call.\"\"\"\n    if not texts:\n        raise ValueError(\"`texts` must be a non-empty list of strings.\")\n    if any(not isinstance(t, str) or not t.strip() for t in texts):\n        raise ValueError(\"All items in `texts` must be non-empty strings.\")\n\n    params = self._ensure_input_type(parameters)\n    result = self.client.embed(texts, model=self.model_name, **params)\n\n    if not hasattr(result, \"embeddings\") or not result.embeddings:\n        raise RuntimeError(\n            \"Voyage returned an empty or malformed embeddings response.\"\n        )\n\n    if len(result.embeddings) != len(texts):\n        raise RuntimeError(\n            f\"Voyage returned {len(result.embeddings)} embeddings for {len(texts)} inputs.\"\n        )\n\n    embeddings = result.embeddings\n\n    return embeddings\n</code></pre>"},{"location":"api_reference/embedding/#huggingfaceembedding","title":"HuggingFaceEmbedding","text":"<p>Warning</p> <p>Currently, only models compatible with <code>sentence-transformers</code> library are available. </p> <p> </p>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.huggingface_embedding.HuggingFaceEmbedding","title":"<code>HuggingFaceEmbedding</code>","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Encoder provider using Hugging Face <code>sentence-transformers</code> models.</p> <p>This class wraps a local (or HF Hub) SentenceTransformer model to produce dense embeddings for text. It provides a consistent interface with your <code>BaseEmbedding</code> and convenient options for device selection and optional input-length validation. This class is available only if <code>splitter-mr[multimodal]</code> is installed.</p> Example <pre><code>from splitter_mr.embedding.models.huggingface_embedding import HuggingFaceEmbedding\n\n# Any sentence-transformers checkpoint works (local path or HF Hub id)\nembedder = HuggingFaceEmbedding(\n    model_name=\"ibm-granite/granite-embedding-english-r2\",\n    device=\"cpu\",            # or \"cuda\", \"mps\", etc.\n    normalize=True,          # L2-normalize outputs\n    enforce_max_length=True  # raise if text exceeds model max seq length\n)\n\nvector = embedder.embed_text(\"hello world\")\nprint(vector)\n</code></pre> Source code in <code>src/splitter_mr/embedding/embeddings/huggingface_embedding.py</code> <pre><code>class HuggingFaceEmbedding(BaseEmbedding):\n    \"\"\"\n    Encoder provider using Hugging Face `sentence-transformers` models.\n\n    This class wraps a local (or HF Hub) SentenceTransformer model to produce\n    dense embeddings for text. It provides a consistent interface with your\n    `BaseEmbedding` and convenient options for device selection and optional\n    input-length validation. This class is available only if\n    `splitter-mr[multimodal]` is installed.\n\n    Example:\n        ```python\n        from splitter_mr.embedding.models.huggingface_embedding import HuggingFaceEmbedding\n\n        # Any sentence-transformers checkpoint works (local path or HF Hub id)\n        embedder = HuggingFaceEmbedding(\n            model_name=\"ibm-granite/granite-embedding-english-r2\",\n            device=\"cpu\",            # or \"cuda\", \"mps\", etc.\n            normalize=True,          # L2-normalize outputs\n            enforce_max_length=True  # raise if text exceeds model max seq length\n        )\n\n        vector = embedder.embed_text(\"hello world\")\n        print(vector)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n        device: Optional[str] = \"cpu\",\n        normalize: bool = True,\n        enforce_max_length: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the sentence-transformers embeddings provider.\n\n        Args:\n            model_name:\n                SentenceTransformer model id or local path. Examples:\n                - `\"ibm-granite/granite-embedding-english-r2\"`\n                - `\"sentence-transformers/all-MiniLM-L6-v2\"`\n                - `\"/path/to/local/model\"`\n            device:\n                Optional device spec (e.g., `\"cpu\"`, `\"cuda\"`, `\"mps\"` or a\n                `torch.device`). If omitted, sentence-transformers chooses.\n            normalize:\n                If True, return L2-normalized embeddings (sets\n                `normalize_embeddings=True` in `encode`).\n            enforce_max_length:\n                If True, attempt to count tokens and raise `ValueError` when\n                input exceeds the model's configured max sequence length.\n                (If the model/tokenizer does not expose this reliably, the\n                check is skipped gracefully.)\n\n        Raises:\n            ValueError: If the model cannot be loaded.\n        \"\"\"\n\n        from sentence_transformers import SentenceTransformer\n\n        st_device = str(device) if device is not None else None\n        try:\n            self.model = SentenceTransformer(model_name, device=st_device)\n        except Exception as e:\n            raise ValueError(\n                f\"Failed to load SentenceTransformer '{model_name}': {e}\"\n            ) from e\n\n        self.model_name = model_name\n        self.normalize = normalize\n        self.enforce_max_length = enforce_max_length\n\n    def get_client(self) -&gt; \"SentenceTransformer\":\n        \"\"\"Return the underlying `SentenceTransformer` instance.\"\"\"\n        return self.model\n\n    def _max_seq_length(self) -&gt; Optional[int]:\n        \"\"\"Best-effort retrieval of model's max sequence length.\"\"\"\n        try:\n            # sentence-transformers exposes this on the model\n            return int(self.model.get_max_seq_length())\n        except Exception:\n            try:\n                # Fallback: some versions have `max_seq_length` attribute\n                return int(getattr(self.model, \"max_seq_length\", None))\n            except Exception:\n                return None\n\n    def _count_tokens(self, text: str) -&gt; Optional[int]:\n        \"\"\"\n        Best-effort token counting via model.tokenize; returns None if unavailable.\n        \"\"\"\n        try:\n            features = self.model.tokenize([text])  # dict with \"input_ids\"\n            input_ids = features[\"input_ids\"]\n            # input_ids is usually a list/array/tensor of shape [batch, seq]\n            if isinstance(input_ids, list):\n                first = input_ids[0]\n                return len(first)\n            if torch is not None and torch.is_tensor(input_ids):\n                return int(input_ids.shape[1])\n            if isinstance(input_ids, np.ndarray):\n                return int(input_ids.shape[1])\n        except Exception:\n            pass\n        return None\n\n    def _validate_length_if_needed(self, text: str) -&gt; None:\n        \"\"\"Raise ValueError if enforce_max_length=True and text is too long.\"\"\"\n        if not self.enforce_max_length:\n            return\n        max_len = self._max_seq_length()\n        tok_count = self._count_tokens(text)\n        if max_len is not None and tok_count is not None and tok_count &gt; max_len:\n            raise ValueError(\n                f\"Input exceeds model max sequence length ({tok_count} &gt; {max_len} tokens).\"\n            )\n\n    def embed_text(self, text: str, **parameters: Any) -&gt; List[float]:\n        \"\"\"\n        Compute an embedding vector for a single text string.\n\n        Args:\n            text:\n                The text to embed. Must be non-empty. If `enforce_max_length`\n                is True, a ValueError is raised when it exceeds the model limit.\n            **parameters:\n                Extra keyword arguments forwarded to `SentenceTransformer.encode`.\n                Common options include:\n                  - `batch_size` (int)\n                  - `show_progress_bar` (bool)\n                  - `convert_to_tensor` (bool)  # will be forced False here\n                  - `device` (str)\n                  - `normalize_embeddings` (bool)\n\n        Returns:\n            List[float]: The computed embedding vector.\n\n        Raises:\n            ValueError: If `text` is empty or exceeds length constraints (when enforced).\n            RuntimeError: If the embedding call fails unexpectedly.\n        \"\"\"\n        if not isinstance(text, str) or not text:\n            raise ValueError(\"`text` must be a non-empty string.\")\n\n        self._validate_length_if_needed(text)\n\n        # Ensure Python list output\n        parameters = dict(parameters)  # shallow copy\n        parameters[\"convert_to_tensor\"] = False\n        parameters.setdefault(\"normalize_embeddings\", self.normalize)\n\n        try:\n            # `encode` accepts a single string and returns a 1D array-like\n            vec = self.model.encode(text, **parameters)\n        except Exception as e:\n            raise RuntimeError(f\"Embedding call failed: {e}\") from e\n\n        # Normalize output to List[float]\n        if isinstance(vec, np.ndarray):\n            return vec.astype(np.float32, copy=False).tolist()\n        if torch is not None and hasattr(vec, \"detach\"):\n            return vec.detach().cpu().float().tolist()\n        if isinstance(vec, (list, tuple)):\n            return [float(x) for x in vec]\n        # Anything else: try to coerce\n        try:\n            return list(map(float, vec))  # type: ignore[arg-type]\n        except Exception as e:\n            raise RuntimeError(f\"Unexpected embedding output type: {type(vec)}\") from e\n\n    def embed_documents(self, texts: List[str], **parameters: Any) -&gt; List[List[float]]:\n        \"\"\"\n        Compute embeddings for multiple texts efficiently using `encode`.\n\n        Args:\n            texts:\n                List of input strings to embed. Must be non-empty and contain\n                only non-empty strings. Length enforcement is applied per item\n                if `enforce_max_length=True`.\n            **parameters:\n                Extra keyword arguments forwarded to `SentenceTransformer.encode`.\n                Common options:\n                  - `batch_size` (int)\n                  - `show_progress_bar` (bool)\n                  - `convert_to_tensor` (bool)  # will be forced False here\n                  - `device` (str)\n                  - `normalize_embeddings` (bool)\n\n        Returns:\n            List[List[float]]: One embedding per input string.\n\n        Raises:\n            ValueError: If `texts` is empty or any element is empty/non-string.\n            RuntimeError: If the embedding call fails unexpectedly.\n        \"\"\"\n        if not texts:\n            raise ValueError(\"`texts` must be a non-empty list of strings.\")\n        if any((not isinstance(t, str) or not t) for t in texts):\n            raise ValueError(\"All items in `texts` must be non-empty strings.\")\n\n        if self.enforce_max_length:\n            for t in texts:\n                self._validate_length_if_needed(t)\n\n        parameters = dict(parameters)\n        parameters[\"convert_to_tensor\"] = False\n        parameters.setdefault(\"normalize_embeddings\", self.normalize)\n\n        try:\n            # Returns ndarray (n, d) or list-of-lists\n            mat = self.model.encode(texts, **parameters)\n        except Exception as e:\n            raise RuntimeError(f\"Batch embedding call failed: {e}\") from e\n\n        if isinstance(mat, np.ndarray):\n            return mat.astype(np.float32, copy=False).tolist()\n        if torch is not None and hasattr(mat, \"detach\"):\n            return mat.detach().cpu().float().tolist()\n        if (\n            isinstance(mat, list)\n            and mat  # noqa: W503\n            and isinstance(mat[0], (list, tuple, float, int))  # noqa: W503\n        ):\n            # Already python lists (ST often returns this when convert_to_tensor=False)\n            if mat and isinstance(mat[0], (float, int)):  # single vector in a flat list\n                return [list(map(float, mat))]\n            return [list(map(float, row)) for row in mat]  # type: ignore[arg-type]\n\n        raise RuntimeError(f\"Unexpected batch embedding output type: {type(mat)}\")\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.huggingface_embedding.HuggingFaceEmbedding.__init__","title":"<code>__init__(model_name='sentence-transformers/all-MiniLM-L6-v2', device='cpu', normalize=True, enforce_max_length=False)</code>","text":"<p>Initialize the sentence-transformers embeddings provider.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>SentenceTransformer model id or local path. Examples: - <code>\"ibm-granite/granite-embedding-english-r2\"</code> - <code>\"sentence-transformers/all-MiniLM-L6-v2\"</code> - <code>\"/path/to/local/model\"</code></p> <code>'sentence-transformers/all-MiniLM-L6-v2'</code> <code>device</code> <code>Optional[str]</code> <p>Optional device spec (e.g., <code>\"cpu\"</code>, <code>\"cuda\"</code>, <code>\"mps\"</code> or a <code>torch.device</code>). If omitted, sentence-transformers chooses.</p> <code>'cpu'</code> <code>normalize</code> <code>bool</code> <p>If True, return L2-normalized embeddings (sets <code>normalize_embeddings=True</code> in <code>encode</code>).</p> <code>True</code> <code>enforce_max_length</code> <code>bool</code> <p>If True, attempt to count tokens and raise <code>ValueError</code> when input exceeds the model's configured max sequence length. (If the model/tokenizer does not expose this reliably, the check is skipped gracefully.)</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model cannot be loaded.</p> Source code in <code>src/splitter_mr/embedding/embeddings/huggingface_embedding.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n    device: Optional[str] = \"cpu\",\n    normalize: bool = True,\n    enforce_max_length: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the sentence-transformers embeddings provider.\n\n    Args:\n        model_name:\n            SentenceTransformer model id or local path. Examples:\n            - `\"ibm-granite/granite-embedding-english-r2\"`\n            - `\"sentence-transformers/all-MiniLM-L6-v2\"`\n            - `\"/path/to/local/model\"`\n        device:\n            Optional device spec (e.g., `\"cpu\"`, `\"cuda\"`, `\"mps\"` or a\n            `torch.device`). If omitted, sentence-transformers chooses.\n        normalize:\n            If True, return L2-normalized embeddings (sets\n            `normalize_embeddings=True` in `encode`).\n        enforce_max_length:\n            If True, attempt to count tokens and raise `ValueError` when\n            input exceeds the model's configured max sequence length.\n            (If the model/tokenizer does not expose this reliably, the\n            check is skipped gracefully.)\n\n    Raises:\n        ValueError: If the model cannot be loaded.\n    \"\"\"\n\n    from sentence_transformers import SentenceTransformer\n\n    st_device = str(device) if device is not None else None\n    try:\n        self.model = SentenceTransformer(model_name, device=st_device)\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to load SentenceTransformer '{model_name}': {e}\"\n        ) from e\n\n    self.model_name = model_name\n    self.normalize = normalize\n    self.enforce_max_length = enforce_max_length\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.huggingface_embedding.HuggingFaceEmbedding.get_client","title":"<code>get_client()</code>","text":"<p>Return the underlying <code>SentenceTransformer</code> instance.</p> Source code in <code>src/splitter_mr/embedding/embeddings/huggingface_embedding.py</code> <pre><code>def get_client(self) -&gt; \"SentenceTransformer\":\n    \"\"\"Return the underlying `SentenceTransformer` instance.\"\"\"\n    return self.model\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.huggingface_embedding.HuggingFaceEmbedding.embed_text","title":"<code>embed_text(text, **parameters)</code>","text":"<p>Compute an embedding vector for a single text string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to embed. Must be non-empty. If <code>enforce_max_length</code> is True, a ValueError is raised when it exceeds the model limit.</p> required <code>**parameters</code> <code>Any</code> <p>Extra keyword arguments forwarded to <code>SentenceTransformer.encode</code>. Common options include:   - <code>batch_size</code> (int)   - <code>show_progress_bar</code> (bool)   - <code>convert_to_tensor</code> (bool)  # will be forced False here   - <code>device</code> (str)   - <code>normalize_embeddings</code> (bool)</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: The computed embedding vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>text</code> is empty or exceeds length constraints (when enforced).</p> <code>RuntimeError</code> <p>If the embedding call fails unexpectedly.</p> Source code in <code>src/splitter_mr/embedding/embeddings/huggingface_embedding.py</code> <pre><code>def embed_text(self, text: str, **parameters: Any) -&gt; List[float]:\n    \"\"\"\n    Compute an embedding vector for a single text string.\n\n    Args:\n        text:\n            The text to embed. Must be non-empty. If `enforce_max_length`\n            is True, a ValueError is raised when it exceeds the model limit.\n        **parameters:\n            Extra keyword arguments forwarded to `SentenceTransformer.encode`.\n            Common options include:\n              - `batch_size` (int)\n              - `show_progress_bar` (bool)\n              - `convert_to_tensor` (bool)  # will be forced False here\n              - `device` (str)\n              - `normalize_embeddings` (bool)\n\n    Returns:\n        List[float]: The computed embedding vector.\n\n    Raises:\n        ValueError: If `text` is empty or exceeds length constraints (when enforced).\n        RuntimeError: If the embedding call fails unexpectedly.\n    \"\"\"\n    if not isinstance(text, str) or not text:\n        raise ValueError(\"`text` must be a non-empty string.\")\n\n    self._validate_length_if_needed(text)\n\n    # Ensure Python list output\n    parameters = dict(parameters)  # shallow copy\n    parameters[\"convert_to_tensor\"] = False\n    parameters.setdefault(\"normalize_embeddings\", self.normalize)\n\n    try:\n        # `encode` accepts a single string and returns a 1D array-like\n        vec = self.model.encode(text, **parameters)\n    except Exception as e:\n        raise RuntimeError(f\"Embedding call failed: {e}\") from e\n\n    # Normalize output to List[float]\n    if isinstance(vec, np.ndarray):\n        return vec.astype(np.float32, copy=False).tolist()\n    if torch is not None and hasattr(vec, \"detach\"):\n        return vec.detach().cpu().float().tolist()\n    if isinstance(vec, (list, tuple)):\n        return [float(x) for x in vec]\n    # Anything else: try to coerce\n    try:\n        return list(map(float, vec))  # type: ignore[arg-type]\n    except Exception as e:\n        raise RuntimeError(f\"Unexpected embedding output type: {type(vec)}\") from e\n</code></pre>"},{"location":"api_reference/embedding/#src.splitter_mr.embedding.embeddings.huggingface_embedding.HuggingFaceEmbedding.embed_documents","title":"<code>embed_documents(texts, **parameters)</code>","text":"<p>Compute embeddings for multiple texts efficiently using <code>encode</code>.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of input strings to embed. Must be non-empty and contain only non-empty strings. Length enforcement is applied per item if <code>enforce_max_length=True</code>.</p> required <code>**parameters</code> <code>Any</code> <p>Extra keyword arguments forwarded to <code>SentenceTransformer.encode</code>. Common options:   - <code>batch_size</code> (int)   - <code>show_progress_bar</code> (bool)   - <code>convert_to_tensor</code> (bool)  # will be forced False here   - <code>device</code> (str)   - <code>normalize_embeddings</code> (bool)</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>List[List[float]]: One embedding per input string.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>texts</code> is empty or any element is empty/non-string.</p> <code>RuntimeError</code> <p>If the embedding call fails unexpectedly.</p> Source code in <code>src/splitter_mr/embedding/embeddings/huggingface_embedding.py</code> <pre><code>def embed_documents(self, texts: List[str], **parameters: Any) -&gt; List[List[float]]:\n    \"\"\"\n    Compute embeddings for multiple texts efficiently using `encode`.\n\n    Args:\n        texts:\n            List of input strings to embed. Must be non-empty and contain\n            only non-empty strings. Length enforcement is applied per item\n            if `enforce_max_length=True`.\n        **parameters:\n            Extra keyword arguments forwarded to `SentenceTransformer.encode`.\n            Common options:\n              - `batch_size` (int)\n              - `show_progress_bar` (bool)\n              - `convert_to_tensor` (bool)  # will be forced False here\n              - `device` (str)\n              - `normalize_embeddings` (bool)\n\n    Returns:\n        List[List[float]]: One embedding per input string.\n\n    Raises:\n        ValueError: If `texts` is empty or any element is empty/non-string.\n        RuntimeError: If the embedding call fails unexpectedly.\n    \"\"\"\n    if not texts:\n        raise ValueError(\"`texts` must be a non-empty list of strings.\")\n    if any((not isinstance(t, str) or not t) for t in texts):\n        raise ValueError(\"All items in `texts` must be non-empty strings.\")\n\n    if self.enforce_max_length:\n        for t in texts:\n            self._validate_length_if_needed(t)\n\n    parameters = dict(parameters)\n    parameters[\"convert_to_tensor\"] = False\n    parameters.setdefault(\"normalize_embeddings\", self.normalize)\n\n    try:\n        # Returns ndarray (n, d) or list-of-lists\n        mat = self.model.encode(texts, **parameters)\n    except Exception as e:\n        raise RuntimeError(f\"Batch embedding call failed: {e}\") from e\n\n    if isinstance(mat, np.ndarray):\n        return mat.astype(np.float32, copy=False).tolist()\n    if torch is not None and hasattr(mat, \"detach\"):\n        return mat.detach().cpu().float().tolist()\n    if (\n        isinstance(mat, list)\n        and mat  # noqa: W503\n        and isinstance(mat[0], (list, tuple, float, int))  # noqa: W503\n    ):\n        # Already python lists (ST often returns this when convert_to_tensor=False)\n        if mat and isinstance(mat[0], (float, int)):  # single vector in a flat list\n            return [list(map(float, mat))]\n        return [list(map(float, row)) for row in mat]  # type: ignore[arg-type]\n\n    raise RuntimeError(f\"Unexpected batch embedding output type: {type(mat)}\")\n</code></pre>"},{"location":"api_reference/model/","title":"Vision Models","text":"<p>Reading documents like Word, PDF, or PowerPoint can sometimes be complicated if they contain images. To avoid this problem, you can use visual language models (VLMs), which are capable of recognizing images and extracting descriptions from them. In this prospectus, a model module has been developed, the implementation of which is based on the BaseVisionModel class. It is presented below.</p>"},{"location":"api_reference/model/#which-model-should-i-use","title":"Which model should I use?","text":"<p>The choice of model depends on your cloud provider, available API keys, and desired level of integration. All models inherit from BaseVisionModel and provide the same interface for extracting text and descriptions from images.</p> Model When to use Requirements Features OpenAIVisionModel If you have an OpenAI API key and want OpenAI cloud <code>OPENAI_API_KEY</code> (optional: <code>OPENAI_MODEL</code>, defaults to <code>\"gpt-4o\"</code>) Simple setup; standard OpenAI chat API AzureOpenAIVisionModel For Azure OpenAI Services users <code>AZURE_OPENAI_API_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_DEPLOYMENT</code>, <code>AZURE_OPENAI_API_VERSION</code> Integrates with Azure; enterprise controls GrokVisionModel If you have access to xAI\u2019s Grok multimodal model <code>XAI_API_KEY</code> (optional: <code>XAI_MODEL</code>, defaults to <code>\"grok-4\"</code>) Supports data-URIs; optional image quality GeminiVisionModel If you want Google\u2019s Gemini Vision models <code>GEMINI_API_KEY</code> + Multimodal extra: <code>pip install 'splitter-mr[multimodal]'</code> Google Gemini API, multi-modal, high-quality extraction HuggingFaceVisionModel Local/open-source/offline inference Multimodal extra: <code>pip install 'splitter-mr[multimodal]'</code> (optional: <code>HF_ACCESS_TOKEN</code>, for required models) Runs locally, uses HF <code>AutoProcessor</code> + chat templates AnthropicVisionModel If you have an Anthropic key and want Claude Vision <code>ANTHROPIC_API_KEY</code> (optional: <code>ANTHROPIC_MODEL</code>, defaults to <code>\"claude-sonnet-4-20250514\"</code>) Uses OpenAI SDK with Anthropic base URL; data-URI (base64) image input; OpenAI-compatible <code>chat.completions</code> BaseVisionModel Abstract base, not used directly \u2013 Template to build your own adapters"},{"location":"api_reference/model/#models","title":"Models","text":""},{"location":"api_reference/model/#basevisionmodel","title":"BaseVisionModel","text":""},{"location":"api_reference/model/#src.splitter_mr.model.base_model.BaseVisionModel","title":"<code>BaseVisionModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for vision models that extract text from images.</p> <p>Subclasses encapsulate local or API-backed implementations (e.g., OpenAI, Azure OpenAI, or on-device models). Implementations should handle encoding, request construction, and response parsing while exposing a uniform interface for clients of the library.</p> Source code in <code>src/splitter_mr/model/base_model.py</code> <pre><code>class BaseVisionModel(ABC):\n    \"\"\"\n    Abstract base for vision models that extract text from images.\n\n    Subclasses encapsulate local or API-backed implementations (e.g., OpenAI,\n    Azure OpenAI, or on-device models). Implementations should handle encoding,\n    request construction, and response parsing while exposing a uniform\n    interface for clients of the library.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, model_name) -&gt; Any:\n        \"\"\"Initialize the model.\n\n        Args:\n            model_name (Any): Identifier of the underlying model. For hosted APIs\n                this could be a model name or deployment name; for local models,\n                it could be a path or configuration object.\n\n        Raises:\n            ValueError: If required configuration or credentials are missing.\n        \"\"\"\n\n    @abstractmethod\n    def get_client(self) -&gt; Any:\n        \"\"\"Return the underlying client or handle.\n\n        Returns:\n            Any: A client/handle that the implementation uses to perform\n                inference (e.g., an SDK client instance, session object, or\n                lightweight wrapper). May be ``None`` for pure-local implementations.\n        \"\"\"\n\n    @abstractmethod\n    def analyze_content(\n        self,\n        prompt: str,\n        file: Optional[bytes],\n        file_ext: Optional[str],\n        **parameters: Dict[str, Any],\n    ) -&gt; str:\n        \"\"\"Extract text from an image using the provided prompt.\n\n        Encodes the image (provided as base64 **without** the\n        ``data:&lt;mime&gt;;base64,`` prefix), sends it with an instruction prompt to\n        the underlying vision model, and returns the model's textual output.\n\n        Args:\n            prompt (str): Instruction or task description guiding the extraction\n                (e.g., *\"Read all visible text\"* or *\"Summarize the receipt\"*).\n            file (Optional[bytes]): Base64-encoded image bytes **without** the\n                header/prefix. Must not be ``None`` for remote/API calls that\n                require an image payload.\n            file_ext (Optional[str]): File extension (e.g., ``\"png\"``, ``\"jpg\"``)\n                used to infer the MIME type when required by the backend.\n            **parameters (Dict[str, Any]): Additional backend-specific options\n                forwarded to the implementation (e.g., timeouts, user tags,\n                temperature, etc.).\n\n        Returns:\n            str: The extracted text or the model's textual response.\n\n        Raises:\n            ValueError: If ``file`` is ``None`` when required, or if the file\n                type is unsupported by the implementation.\n            RuntimeError: If the inference call fails or returns an unexpected\n                response shape.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.base_model.BaseVisionModel.__init__","title":"<code>__init__(model_name)</code>  <code>abstractmethod</code>","text":"<p>Initialize the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>Any</code> <p>Identifier of the underlying model. For hosted APIs this could be a model name or deployment name; for local models, it could be a path or configuration object.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If required configuration or credentials are missing.</p> Source code in <code>src/splitter_mr/model/base_model.py</code> <pre><code>@abstractmethod\ndef __init__(self, model_name) -&gt; Any:\n    \"\"\"Initialize the model.\n\n    Args:\n        model_name (Any): Identifier of the underlying model. For hosted APIs\n            this could be a model name or deployment name; for local models,\n            it could be a path or configuration object.\n\n    Raises:\n        ValueError: If required configuration or credentials are missing.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.base_model.BaseVisionModel.analyze_content","title":"<code>analyze_content(prompt, file, file_ext, **parameters)</code>  <code>abstractmethod</code>","text":"<p>Extract text from an image using the provided prompt.</p> <p>Encodes the image (provided as base64 without the <code>data:&lt;mime&gt;;base64,</code> prefix), sends it with an instruction prompt to the underlying vision model, and returns the model's textual output.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Instruction or task description guiding the extraction (e.g., \"Read all visible text\" or \"Summarize the receipt\").</p> required <code>file</code> <code>Optional[bytes]</code> <p>Base64-encoded image bytes without the header/prefix. Must not be <code>None</code> for remote/API calls that require an image payload.</p> required <code>file_ext</code> <code>Optional[str]</code> <p>File extension (e.g., <code>\"png\"</code>, <code>\"jpg\"</code>) used to infer the MIME type when required by the backend.</p> required <code>**parameters</code> <code>Dict[str, Any]</code> <p>Additional backend-specific options forwarded to the implementation (e.g., timeouts, user tags, temperature, etc.).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The extracted text or the model's textual response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>file</code> is <code>None</code> when required, or if the file type is unsupported by the implementation.</p> <code>RuntimeError</code> <p>If the inference call fails or returns an unexpected response shape.</p> Source code in <code>src/splitter_mr/model/base_model.py</code> <pre><code>@abstractmethod\ndef analyze_content(\n    self,\n    prompt: str,\n    file: Optional[bytes],\n    file_ext: Optional[str],\n    **parameters: Dict[str, Any],\n) -&gt; str:\n    \"\"\"Extract text from an image using the provided prompt.\n\n    Encodes the image (provided as base64 **without** the\n    ``data:&lt;mime&gt;;base64,`` prefix), sends it with an instruction prompt to\n    the underlying vision model, and returns the model's textual output.\n\n    Args:\n        prompt (str): Instruction or task description guiding the extraction\n            (e.g., *\"Read all visible text\"* or *\"Summarize the receipt\"*).\n        file (Optional[bytes]): Base64-encoded image bytes **without** the\n            header/prefix. Must not be ``None`` for remote/API calls that\n            require an image payload.\n        file_ext (Optional[str]): File extension (e.g., ``\"png\"``, ``\"jpg\"``)\n            used to infer the MIME type when required by the backend.\n        **parameters (Dict[str, Any]): Additional backend-specific options\n            forwarded to the implementation (e.g., timeouts, user tags,\n            temperature, etc.).\n\n    Returns:\n        str: The extracted text or the model's textual response.\n\n    Raises:\n        ValueError: If ``file`` is ``None`` when required, or if the file\n            type is unsupported by the implementation.\n        RuntimeError: If the inference call fails or returns an unexpected\n            response shape.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.base_model.BaseVisionModel.get_client","title":"<code>get_client()</code>  <code>abstractmethod</code>","text":"<p>Return the underlying client or handle.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A client/handle that the implementation uses to perform inference (e.g., an SDK client instance, session object, or lightweight wrapper). May be <code>None</code> for pure-local implementations.</p> Source code in <code>src/splitter_mr/model/base_model.py</code> <pre><code>@abstractmethod\ndef get_client(self) -&gt; Any:\n    \"\"\"Return the underlying client or handle.\n\n    Returns:\n        Any: A client/handle that the implementation uses to perform\n            inference (e.g., an SDK client instance, session object, or\n            lightweight wrapper). May be ``None`` for pure-local implementations.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/model/#openaivisionmodel","title":"OpenAIVisionModel","text":""},{"location":"api_reference/model/#src.splitter_mr.model.models.openai_model.OpenAIVisionModel","title":"<code>OpenAIVisionModel</code>","text":"<p>               Bases: <code>BaseVisionModel</code></p> <p>Implementation of BaseModel leveraging OpenAI's Chat Completions API.</p> <p>Uses the <code>client.chat.completions.create()</code> method to send base64-encoded images along with text prompts in a single multimodal request.</p> Source code in <code>src/splitter_mr/model/models/openai_model.py</code> <pre><code>class OpenAIVisionModel(BaseVisionModel):\n    \"\"\"\n    Implementation of BaseModel leveraging OpenAI's Chat Completions API.\n\n    Uses the `client.chat.completions.create()` method to send base64-encoded\n    images along with text prompts in a single multimodal request.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        model_name: str = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\"),\n    ) -&gt; None:\n        \"\"\"\n        Initialize the OpenAIVisionModel.\n\n        Args:\n            api_key (str, optional): OpenAI API key. If not provided, uses the\n                ``OPENAI_API_KEY`` environment variable.\n            model_name (str): Vision-capable model name (e.g., ``\"gpt-4o\"``).\n\n        Raises:\n            ValueError: If no API key is provided or ``OPENAI_API_KEY`` is not set.\n        \"\"\"\n        if api_key is None:\n            api_key = os.getenv(\"OPENAI_API_KEY\")\n            if not api_key:\n                raise ValueError(\n                    \"OpenAI API key not provided or 'OPENAI_API_KEY' env var is not set.\"\n                )\n        self.client = OpenAI(api_key=api_key)\n        self.model_name = model_name\n\n    def get_client(self) -&gt; OpenAI:\n        \"\"\"\n        Get the underlying OpenAI client instance.\n\n        Returns:\n            OpenAI: The initialized API client.\n        \"\"\"\n        return self.client\n\n    def analyze_content(\n        self,\n        file: Optional[bytes],\n        prompt: str = DEFAULT_IMAGE_CAPTION_PROMPT,\n        *,\n        file_ext: Optional[str] = \"png\",\n        **parameters: Any,\n    ) -&gt; str:\n        \"\"\"\n        Extract text from an image using OpenAI's Chat Completions API.\n\n        Encodes the provided image bytes as a base64 data URI and sends it\n        along with a textual prompt to the specified vision-capable model.\n        The model processes the image and returns extracted text.\n\n        Args:\n            file (bytes, optional): Base64-encoded image content **without** the\n                ``data:image/...;base64,`` prefix. Must not be None.\n            prompt (str, optional): Instruction text guiding the extraction.\n                Defaults to ``DEFAULT_IMAGE_CAPTION_PROMPT``.\n            file_ext (str, optional): File extension (e.g., ``\"png\"``, ``\"jpg\"``,\n                ``\"jpeg\"``, ``\"webp\"``, ``\"gif\"``) used to determine the MIME type.\n                Defaults to ``\"png\"``.\n            **parameters (Any): Additional keyword arguments passed directly to\n                the OpenAI client ``chat.completions.create()`` method. Consult documentation\n                [here](https://platform.openai.com/docs/api-reference/chat/create).\n\n        Returns:\n            str: Extracted text returned by the model.\n\n        Raises:\n            ValueError: If ``file`` is None or the file extension is not compatible.\n            openai.OpenAIError: If the API request fails.\n\n        Example:\n            ```python\n            from splitter_mr.model import OpenAIVisionModel\n            import base64\n\n            model = OpenAIVisionModel(api_key=\"sk-...\")\n            with open(\"example.png\", \"rb\") as f:\n                img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n\n            text = model.analyze_content(img_b64, prompt=\"Describe the content of this image.\")\n            print(text)\n            ```\n        \"\"\"\n        if file is None:\n            raise ValueError(\"No file content provided for text extraction.\")\n\n        ext = (file_ext or \"png\").lower()\n        mime_type = (\n            OPENAI_MIME_BY_EXTENSION.get(ext)  # noqa: W503\n            or mimetypes.types_map.get(f\".{ext}\")  # noqa: W503\n            or \"image/png\"  # noqa: W503\n        )\n\n        if mime_type not in SUPPORTED_OPENAI_MIME_TYPES:\n            raise ValueError(f\"Unsupported image MIME type: {mime_type}\")\n\n        payload_obj = OpenAIClientPayload(\n            role=\"user\",\n            content=[\n                OpenAIClientTextContent(type=\"text\", text=prompt),\n                OpenAIClientImageContent(\n                    type=\"image_url\",\n                    image_url=OpenAIClientImageUrl(\n                        url=f\"data:{mime_type};base64,{file}\"\n                    ),\n                ),\n            ],\n        )\n        payload = payload_obj.model_dump(exclude_none=True)\n\n        response = self.client.chat.completions.create(\n            model=self.model_name,\n            messages=[payload],\n            **parameters,\n        )\n        return response.choices[0].message.content\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.openai_model.OpenAIVisionModel.__init__","title":"<code>__init__(api_key=None, model_name=os.getenv('OPENAI_MODEL', 'gpt-4o'))</code>","text":"<p>Initialize the OpenAIVisionModel.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key. If not provided, uses the <code>OPENAI_API_KEY</code> environment variable.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>Vision-capable model name (e.g., <code>\"gpt-4o\"</code>).</p> <code>getenv('OPENAI_MODEL', 'gpt-4o')</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no API key is provided or <code>OPENAI_API_KEY</code> is not set.</p> Source code in <code>src/splitter_mr/model/models/openai_model.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = None,\n    model_name: str = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\"),\n) -&gt; None:\n    \"\"\"\n    Initialize the OpenAIVisionModel.\n\n    Args:\n        api_key (str, optional): OpenAI API key. If not provided, uses the\n            ``OPENAI_API_KEY`` environment variable.\n        model_name (str): Vision-capable model name (e.g., ``\"gpt-4o\"``).\n\n    Raises:\n        ValueError: If no API key is provided or ``OPENAI_API_KEY`` is not set.\n    \"\"\"\n    if api_key is None:\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"OpenAI API key not provided or 'OPENAI_API_KEY' env var is not set.\"\n            )\n    self.client = OpenAI(api_key=api_key)\n    self.model_name = model_name\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.openai_model.OpenAIVisionModel.analyze_content","title":"<code>analyze_content(file, prompt=DEFAULT_IMAGE_CAPTION_PROMPT, *, file_ext='png', **parameters)</code>","text":"<p>Extract text from an image using OpenAI's Chat Completions API.</p> <p>Encodes the provided image bytes as a base64 data URI and sends it along with a textual prompt to the specified vision-capable model. The model processes the image and returns extracted text.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>bytes</code> <p>Base64-encoded image content without the <code>data:image/...;base64,</code> prefix. Must not be None.</p> required <code>prompt</code> <code>str</code> <p>Instruction text guiding the extraction. Defaults to <code>DEFAULT_IMAGE_CAPTION_PROMPT</code>.</p> <code>DEFAULT_IMAGE_CAPTION_PROMPT</code> <code>file_ext</code> <code>str</code> <p>File extension (e.g., <code>\"png\"</code>, <code>\"jpg\"</code>, <code>\"jpeg\"</code>, <code>\"webp\"</code>, <code>\"gif\"</code>) used to determine the MIME type. Defaults to <code>\"png\"</code>.</p> <code>'png'</code> <code>**parameters</code> <code>Any</code> <p>Additional keyword arguments passed directly to the OpenAI client <code>chat.completions.create()</code> method. Consult documentation here.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text returned by the model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>file</code> is None or the file extension is not compatible.</p> <code>OpenAIError</code> <p>If the API request fails.</p> Example <pre><code>from splitter_mr.model import OpenAIVisionModel\nimport base64\n\nmodel = OpenAIVisionModel(api_key=\"sk-...\")\nwith open(\"example.png\", \"rb\") as f:\n    img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n\ntext = model.analyze_content(img_b64, prompt=\"Describe the content of this image.\")\nprint(text)\n</code></pre> Source code in <code>src/splitter_mr/model/models/openai_model.py</code> <pre><code>def analyze_content(\n    self,\n    file: Optional[bytes],\n    prompt: str = DEFAULT_IMAGE_CAPTION_PROMPT,\n    *,\n    file_ext: Optional[str] = \"png\",\n    **parameters: Any,\n) -&gt; str:\n    \"\"\"\n    Extract text from an image using OpenAI's Chat Completions API.\n\n    Encodes the provided image bytes as a base64 data URI and sends it\n    along with a textual prompt to the specified vision-capable model.\n    The model processes the image and returns extracted text.\n\n    Args:\n        file (bytes, optional): Base64-encoded image content **without** the\n            ``data:image/...;base64,`` prefix. Must not be None.\n        prompt (str, optional): Instruction text guiding the extraction.\n            Defaults to ``DEFAULT_IMAGE_CAPTION_PROMPT``.\n        file_ext (str, optional): File extension (e.g., ``\"png\"``, ``\"jpg\"``,\n            ``\"jpeg\"``, ``\"webp\"``, ``\"gif\"``) used to determine the MIME type.\n            Defaults to ``\"png\"``.\n        **parameters (Any): Additional keyword arguments passed directly to\n            the OpenAI client ``chat.completions.create()`` method. Consult documentation\n            [here](https://platform.openai.com/docs/api-reference/chat/create).\n\n    Returns:\n        str: Extracted text returned by the model.\n\n    Raises:\n        ValueError: If ``file`` is None or the file extension is not compatible.\n        openai.OpenAIError: If the API request fails.\n\n    Example:\n        ```python\n        from splitter_mr.model import OpenAIVisionModel\n        import base64\n\n        model = OpenAIVisionModel(api_key=\"sk-...\")\n        with open(\"example.png\", \"rb\") as f:\n            img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n\n        text = model.analyze_content(img_b64, prompt=\"Describe the content of this image.\")\n        print(text)\n        ```\n    \"\"\"\n    if file is None:\n        raise ValueError(\"No file content provided for text extraction.\")\n\n    ext = (file_ext or \"png\").lower()\n    mime_type = (\n        OPENAI_MIME_BY_EXTENSION.get(ext)  # noqa: W503\n        or mimetypes.types_map.get(f\".{ext}\")  # noqa: W503\n        or \"image/png\"  # noqa: W503\n    )\n\n    if mime_type not in SUPPORTED_OPENAI_MIME_TYPES:\n        raise ValueError(f\"Unsupported image MIME type: {mime_type}\")\n\n    payload_obj = OpenAIClientPayload(\n        role=\"user\",\n        content=[\n            OpenAIClientTextContent(type=\"text\", text=prompt),\n            OpenAIClientImageContent(\n                type=\"image_url\",\n                image_url=OpenAIClientImageUrl(\n                    url=f\"data:{mime_type};base64,{file}\"\n                ),\n            ),\n        ],\n    )\n    payload = payload_obj.model_dump(exclude_none=True)\n\n    response = self.client.chat.completions.create(\n        model=self.model_name,\n        messages=[payload],\n        **parameters,\n    )\n    return response.choices[0].message.content\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.openai_model.OpenAIVisionModel.get_client","title":"<code>get_client()</code>","text":"<p>Get the underlying OpenAI client instance.</p> <p>Returns:</p> Name Type Description <code>OpenAI</code> <code>OpenAI</code> <p>The initialized API client.</p> Source code in <code>src/splitter_mr/model/models/openai_model.py</code> <pre><code>def get_client(self) -&gt; OpenAI:\n    \"\"\"\n    Get the underlying OpenAI client instance.\n\n    Returns:\n        OpenAI: The initialized API client.\n    \"\"\"\n    return self.client\n</code></pre>"},{"location":"api_reference/model/#azureopenaivisionmodel","title":"AzureOpenAIVisionModel","text":""},{"location":"api_reference/model/#src.splitter_mr.model.models.azure_openai_model.AzureOpenAIVisionModel","title":"<code>AzureOpenAIVisionModel</code>","text":"<p>               Bases: <code>BaseVisionModel</code></p> <p>Implementation of BaseModel for Azure OpenAI Vision using the Responses API.</p> <p>Utilizes Azure\u2019s preview <code>responses</code> API, which supports base64-encoded images and stateful multimodal calls.</p> Source code in <code>src/splitter_mr/model/models/azure_openai_model.py</code> <pre><code>class AzureOpenAIVisionModel(BaseVisionModel):\n    \"\"\"\n    Implementation of BaseModel for Azure OpenAI Vision using the Responses API.\n\n    Utilizes Azure\u2019s preview `responses` API, which supports\n    base64-encoded images and stateful multimodal calls.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str = None,\n        azure_endpoint: str = None,\n        azure_deployment: str = None,\n        api_version: str = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the AzureOpenAIVisionModel.\n\n        Args:\n            api_key (str, optional): Azure OpenAI API key.\n                If not provided, uses 'AZURE_OPENAI_API_KEY' env var.\n            azure_endpoint (str, optional): Azure endpoint.\n                If not provided, uses 'AZURE_OPENAI_ENDPOINT' env var.\n            azure_deployment (str, optional): Azure deployment name.\n                If not provided, uses 'AZURE_OPENAI_DEPLOYMENT' env var.\n            api_version (str, optional): API version string.\n                If not provided, uses 'AZURE_OPENAI_API_VERSION' env var or defaults to '2025-04-14-preview'.\n\n        Raises:\n            ValueError: If no connection details are provided or environment variables\n                are not set.\n        \"\"\"\n        if api_key is None:\n            api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n            if not api_key:\n                raise ValueError(\n                    \"Azure OpenAI API key not provided or 'AZURE_OPENAI_API_KEY' env var is not set.\"\n                )\n        if azure_endpoint is None:\n            azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n            if not azure_endpoint:\n                raise ValueError(\n                    \"Azure endpoint not provided or 'AZURE_OPENAI_ENDPOINT' env var is not set.\"\n                )\n        if azure_deployment is None:\n            azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n            if not azure_deployment:\n                raise ValueError(\n                    \"Azure deployment name not provided or 'AZURE_OPENAI_DEPLOYMENT' env var is not set.\"\n                )\n        if api_version is None:\n            api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-04-14-preview\")\n\n        self.client = AzureOpenAI(\n            api_key=api_key,\n            azure_endpoint=azure_endpoint,\n            azure_deployment=azure_deployment,\n            api_version=api_version,\n        )\n        self.model_name = azure_deployment\n\n    def get_client(self) -&gt; AzureOpenAI:\n        \"\"\"Returns the AzureOpenAI client instance.\"\"\"\n        return self.client\n\n    def analyze_content(\n        self,\n        file: Optional[bytes],\n        prompt: str = DEFAULT_IMAGE_CAPTION_PROMPT,\n        file_ext: Optional[str] = \"png\",\n        **parameters: Any,\n    ) -&gt; str:\n        \"\"\"\n        Extract text from an image using the Azure OpenAI Vision model.\n\n        Encodes the given image as a data URI with an appropriate MIME type based on\n        ``file_ext`` and sends it along with a prompt to the Azure OpenAI Vision API.\n        The API processes the image and returns extracted text in the response.\n\n        Args:\n            file (bytes, optional): Base64-encoded image content **without** the\n                ``data:image/...;base64,`` prefix. Must not be None.\n            prompt (str, optional): Instruction text guiding the extraction.\n                Defaults to ``DEFAULT_IMAGE_CAPTION_PROMPT``.\n            file_ext (str, optional): File extension (e.g., ``\"png\"``, ``\"jpg\"``)\n                used to determine the MIME type for the image. Defaults to ``\"png\"``.\n            **parameters (Any): Additional keyword arguments passed directly to\n                the Azure OpenAI client ``chat.completions.create()`` method. Consult\n                documentation [here](https://platform.openai.com/docs/api-reference/chat/create).\n\n        Returns:\n            str: The extracted text returned by the vision model.\n\n        Raises:\n            ValueError: If ``file`` is None or the file extension is not compatible.\n            openai.OpenAIError: If the API request fails.\n\n        Example:\n            ```python\n            model = AzureOpenAIVisionModel(...)\n            with open(\"image.jpg\", \"rb\") as f:\n                img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n            text = model.analyze_content(img_b64, prompt=\"Describe this image\", file_ext=\"jpg\")\n            print(text)\n            ```\n        \"\"\"\n        if file is None:\n            raise ValueError(\"No file content provided to be analyzed with the VLM.\")\n\n        ext = (file_ext or \"png\").lower()\n        mime_type = (\n            OPENAI_MIME_BY_EXTENSION.get(ext)  # noqa: W503\n            or mimetypes.types_map.get(f\".{ext}\")  # noqa: W503\n            or \"image/png\"  # noqa: W503\n        )\n\n        if mime_type not in SUPPORTED_OPENAI_MIME_TYPES:\n            raise ValueError(f\"Unsupported image MIME type: {mime_type}\")\n\n        payload_obj = OpenAIClientPayload(\n            role=\"user\",\n            content=[\n                OpenAIClientTextContent(type=\"text\", text=prompt),\n                OpenAIClientImageContent(\n                    type=\"image_url\",\n                    image_url=OpenAIClientImageUrl(\n                        url=f\"data:{mime_type};base64,{file}\"\n                    ),\n                ),\n            ],\n        )\n        payload = payload_obj.model_dump(exclude_none=True)\n\n        response = self.client.chat.completions.create(\n            model=self.get_client()._azure_deployment,\n            messages=[payload],\n            **parameters,\n        )\n        return response.choices[0].message.content\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.azure_openai_model.AzureOpenAIVisionModel.__init__","title":"<code>__init__(api_key=None, azure_endpoint=None, azure_deployment=None, api_version=None)</code>","text":"<p>Initializes the AzureOpenAIVisionModel.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Azure OpenAI API key. If not provided, uses 'AZURE_OPENAI_API_KEY' env var.</p> <code>None</code> <code>azure_endpoint</code> <code>str</code> <p>Azure endpoint. If not provided, uses 'AZURE_OPENAI_ENDPOINT' env var.</p> <code>None</code> <code>azure_deployment</code> <code>str</code> <p>Azure deployment name. If not provided, uses 'AZURE_OPENAI_DEPLOYMENT' env var.</p> <code>None</code> <code>api_version</code> <code>str</code> <p>API version string. If not provided, uses 'AZURE_OPENAI_API_VERSION' env var or defaults to '2025-04-14-preview'.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no connection details are provided or environment variables are not set.</p> Source code in <code>src/splitter_mr/model/models/azure_openai_model.py</code> <pre><code>def __init__(\n    self,\n    api_key: str = None,\n    azure_endpoint: str = None,\n    azure_deployment: str = None,\n    api_version: str = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the AzureOpenAIVisionModel.\n\n    Args:\n        api_key (str, optional): Azure OpenAI API key.\n            If not provided, uses 'AZURE_OPENAI_API_KEY' env var.\n        azure_endpoint (str, optional): Azure endpoint.\n            If not provided, uses 'AZURE_OPENAI_ENDPOINT' env var.\n        azure_deployment (str, optional): Azure deployment name.\n            If not provided, uses 'AZURE_OPENAI_DEPLOYMENT' env var.\n        api_version (str, optional): API version string.\n            If not provided, uses 'AZURE_OPENAI_API_VERSION' env var or defaults to '2025-04-14-preview'.\n\n    Raises:\n        ValueError: If no connection details are provided or environment variables\n            are not set.\n    \"\"\"\n    if api_key is None:\n        api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"Azure OpenAI API key not provided or 'AZURE_OPENAI_API_KEY' env var is not set.\"\n            )\n    if azure_endpoint is None:\n        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n        if not azure_endpoint:\n            raise ValueError(\n                \"Azure endpoint not provided or 'AZURE_OPENAI_ENDPOINT' env var is not set.\"\n            )\n    if azure_deployment is None:\n        azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n        if not azure_deployment:\n            raise ValueError(\n                \"Azure deployment name not provided or 'AZURE_OPENAI_DEPLOYMENT' env var is not set.\"\n            )\n    if api_version is None:\n        api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-04-14-preview\")\n\n    self.client = AzureOpenAI(\n        api_key=api_key,\n        azure_endpoint=azure_endpoint,\n        azure_deployment=azure_deployment,\n        api_version=api_version,\n    )\n    self.model_name = azure_deployment\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.azure_openai_model.AzureOpenAIVisionModel.analyze_content","title":"<code>analyze_content(file, prompt=DEFAULT_IMAGE_CAPTION_PROMPT, file_ext='png', **parameters)</code>","text":"<p>Extract text from an image using the Azure OpenAI Vision model.</p> <p>Encodes the given image as a data URI with an appropriate MIME type based on <code>file_ext</code> and sends it along with a prompt to the Azure OpenAI Vision API. The API processes the image and returns extracted text in the response.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>bytes</code> <p>Base64-encoded image content without the <code>data:image/...;base64,</code> prefix. Must not be None.</p> required <code>prompt</code> <code>str</code> <p>Instruction text guiding the extraction. Defaults to <code>DEFAULT_IMAGE_CAPTION_PROMPT</code>.</p> <code>DEFAULT_IMAGE_CAPTION_PROMPT</code> <code>file_ext</code> <code>str</code> <p>File extension (e.g., <code>\"png\"</code>, <code>\"jpg\"</code>) used to determine the MIME type for the image. Defaults to <code>\"png\"</code>.</p> <code>'png'</code> <code>**parameters</code> <code>Any</code> <p>Additional keyword arguments passed directly to the Azure OpenAI client <code>chat.completions.create()</code> method. Consult documentation here.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The extracted text returned by the vision model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>file</code> is None or the file extension is not compatible.</p> <code>OpenAIError</code> <p>If the API request fails.</p> Example <pre><code>model = AzureOpenAIVisionModel(...)\nwith open(\"image.jpg\", \"rb\") as f:\n    img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\ntext = model.analyze_content(img_b64, prompt=\"Describe this image\", file_ext=\"jpg\")\nprint(text)\n</code></pre> Source code in <code>src/splitter_mr/model/models/azure_openai_model.py</code> <pre><code>def analyze_content(\n    self,\n    file: Optional[bytes],\n    prompt: str = DEFAULT_IMAGE_CAPTION_PROMPT,\n    file_ext: Optional[str] = \"png\",\n    **parameters: Any,\n) -&gt; str:\n    \"\"\"\n    Extract text from an image using the Azure OpenAI Vision model.\n\n    Encodes the given image as a data URI with an appropriate MIME type based on\n    ``file_ext`` and sends it along with a prompt to the Azure OpenAI Vision API.\n    The API processes the image and returns extracted text in the response.\n\n    Args:\n        file (bytes, optional): Base64-encoded image content **without** the\n            ``data:image/...;base64,`` prefix. Must not be None.\n        prompt (str, optional): Instruction text guiding the extraction.\n            Defaults to ``DEFAULT_IMAGE_CAPTION_PROMPT``.\n        file_ext (str, optional): File extension (e.g., ``\"png\"``, ``\"jpg\"``)\n            used to determine the MIME type for the image. Defaults to ``\"png\"``.\n        **parameters (Any): Additional keyword arguments passed directly to\n            the Azure OpenAI client ``chat.completions.create()`` method. Consult\n            documentation [here](https://platform.openai.com/docs/api-reference/chat/create).\n\n    Returns:\n        str: The extracted text returned by the vision model.\n\n    Raises:\n        ValueError: If ``file`` is None or the file extension is not compatible.\n        openai.OpenAIError: If the API request fails.\n\n    Example:\n        ```python\n        model = AzureOpenAIVisionModel(...)\n        with open(\"image.jpg\", \"rb\") as f:\n            img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n        text = model.analyze_content(img_b64, prompt=\"Describe this image\", file_ext=\"jpg\")\n        print(text)\n        ```\n    \"\"\"\n    if file is None:\n        raise ValueError(\"No file content provided to be analyzed with the VLM.\")\n\n    ext = (file_ext or \"png\").lower()\n    mime_type = (\n        OPENAI_MIME_BY_EXTENSION.get(ext)  # noqa: W503\n        or mimetypes.types_map.get(f\".{ext}\")  # noqa: W503\n        or \"image/png\"  # noqa: W503\n    )\n\n    if mime_type not in SUPPORTED_OPENAI_MIME_TYPES:\n        raise ValueError(f\"Unsupported image MIME type: {mime_type}\")\n\n    payload_obj = OpenAIClientPayload(\n        role=\"user\",\n        content=[\n            OpenAIClientTextContent(type=\"text\", text=prompt),\n            OpenAIClientImageContent(\n                type=\"image_url\",\n                image_url=OpenAIClientImageUrl(\n                    url=f\"data:{mime_type};base64,{file}\"\n                ),\n            ),\n        ],\n    )\n    payload = payload_obj.model_dump(exclude_none=True)\n\n    response = self.client.chat.completions.create(\n        model=self.get_client()._azure_deployment,\n        messages=[payload],\n        **parameters,\n    )\n    return response.choices[0].message.content\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.azure_openai_model.AzureOpenAIVisionModel.get_client","title":"<code>get_client()</code>","text":"<p>Returns the AzureOpenAI client instance.</p> Source code in <code>src/splitter_mr/model/models/azure_openai_model.py</code> <pre><code>def get_client(self) -&gt; AzureOpenAI:\n    \"\"\"Returns the AzureOpenAI client instance.\"\"\"\n    return self.client\n</code></pre>"},{"location":"api_reference/model/#grokvisionmodel","title":"GrokVisionModel","text":""},{"location":"api_reference/model/#src.splitter_mr.model.models.grok_model.GrokVisionModel","title":"<code>GrokVisionModel</code>","text":"<p>               Bases: <code>BaseVisionModel</code></p> <p>Implementation of BaseModel for Grok Vision using the xAI API.</p> <p>Provides methods to interact with Grok\u2019s multimodal models that support base64-encoded images and natural language instructions. This class is designed to extract structured text descriptions or captions from images.</p> Source code in <code>src/splitter_mr/model/models/grok_model.py</code> <pre><code>class GrokVisionModel(BaseVisionModel):\n    \"\"\"\n    Implementation of BaseModel for Grok Vision using the xAI API.\n\n    Provides methods to interact with Grok\u2019s multimodal models that support\n    base64-encoded images and natural language instructions. This class is\n    designed to extract structured text descriptions or captions from images.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = os.getenv(\"XAI_API_KEY\"),\n        model_name: str = os.getenv(\"XAI_MODEL\", \"grok-4\"),\n    ) -&gt; None:\n        \"\"\"\n        Initializes the GrokVisionModel.\n\n        Args:\n            api_key (str, optional): Grok API key. If not provided, uses the\n                ``XAI_API_KEY`` environment variable.\n            model_name (str, optional): Model identifier to use. If not provided,\n                defaults to ``XAI_MODEL`` environment variable or ``\"grok-4\"``.\n\n        Raises:\n            ValueError: If ``api_key`` is not provided or cannot be resolved\n                from environment variables.\n        \"\"\"\n        api_key = api_key or os.getenv(\"XAI_API_KEY\")\n        model_name = model_name or os.getenv(\"XAI_MODEL\") or \"grok-4\"\n\n        if not api_key:\n            raise ValueError(\n                \"Grok API key not provided or 'XAI_API_KEY' env var is not set.\"\n            )\n\n        self.model_name = model_name\n        self.client = Client(\n            api_key=api_key,\n            base_url=\"https://api.x.ai/v1\",\n        )  # TODO: Change to xAI SDK\n\n    def get_client(self) -&gt; Client:\n        \"\"\"\n        Returns the underlying Grok API client.\n\n        Returns:\n            Client: The initialized Grok ``Client`` instance.\n        \"\"\"\n        return self.client\n\n    def analyze_content(\n        self,\n        file: Optional[bytes],\n        prompt: Optional[str] = None,\n        *,\n        file_ext: Optional[str] = \"png\",\n        detail: str = \"auto\",\n        **parameters: Any,\n    ) -&gt; str:\n        \"\"\"\n        Extract text from an image using the Grok Vision model.\n\n        Encodes the given image as a data URI with an appropriate MIME type based on\n        ``file_ext`` and sends it along with a prompt to the Grok API. The API\n        processes the image and returns extracted text in the response.\n\n        Args:\n            file (bytes, optional): Base64-encoded image content **without** the\n                ``data:image/...;base64,`` prefix. Must not be None.\n            prompt (str, optional): Instruction text guiding the extraction.\n                Defaults to ``DEFAULT_IMAGE_CAPTION_PROMPT``.\n            file_ext (str, optional): File extension (e.g., ``\"png\"``, ``\"jpg\"``)\n                used to determine the MIME type for the image. Defaults to ``\"png\"``.\n            detail (str, optional): Level of detail to request for the image\n                analysis. Options typically include ``\"low\"``, ``\"high\"`` or ``\"auto\"``.\n                Defaults to ``\"auto\"``.\n            **parameters (Any): Additional keyword arguments passed directly to\n                the Grok client ``chat.completions.create()`` method.\n\n        Returns:\n            str: The extracted text returned by the vision model.\n\n        Raises:\n            ValueError: If ``file`` is None or the file extension is not compatible.\n            openai.OpenAIError: If the API request fails.\n\n        Example:\n            ```python\n            from splitter_mr.model import GrokVisionModel\n\n            model = GrokVisionModel()\n            with open(\"image.jpg\", \"rb\") as f:\n                img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n\n            text = model.analyze_content(\n                img_b64, prompt=\"What's in this image?\", file_ext=\"jpg\", detail=\"high\"\n            )\n            print(text)\n            ```\n        \"\"\"\n        if file is None:\n            raise ValueError(\"No file content provided for text extraction.\")\n\n        ext = (file_ext or \"png\").lower()\n        mime_type = (\n            GROK_MIME_BY_EXTENSION.get(ext)  # noqa: W503\n            or mimetypes.types_map.get(f\".{ext}\")  # noqa: W503\n            or \"image/png\"  # noqa: W503\n        )\n\n        if mime_type not in SUPPORTED_GROK_MIME_TYPES:\n            raise ValueError(f\"Unsupported image MIME type: {mime_type}\")\n\n        prompt = prompt or DEFAULT_IMAGE_CAPTION_PROMPT\n\n        payload_obj = OpenAIClientPayload(\n            role=\"user\",\n            content=[\n                OpenAIClientTextContent(type=\"text\", text=prompt),\n                OpenAIClientImageContent(\n                    type=\"image_url\",\n                    image_url=OpenAIClientImageUrl(\n                        url=f\"data:{mime_type};base64,{file}\",\n                        detail=detail,\n                    ),\n                ),\n            ],\n        )\n\n        response = self.client.chat.completions.create(\n            model=self.model_name, messages=[payload_obj], **parameters\n        )\n\n        return response.choices[0].message.content\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.grok_model.GrokVisionModel.__init__","title":"<code>__init__(api_key=os.getenv('XAI_API_KEY'), model_name=os.getenv('XAI_MODEL', 'grok-4'))</code>","text":"<p>Initializes the GrokVisionModel.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Grok API key. If not provided, uses the <code>XAI_API_KEY</code> environment variable.</p> <code>getenv('XAI_API_KEY')</code> <code>model_name</code> <code>str</code> <p>Model identifier to use. If not provided, defaults to <code>XAI_MODEL</code> environment variable or <code>\"grok-4\"</code>.</p> <code>getenv('XAI_MODEL', 'grok-4')</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>api_key</code> is not provided or cannot be resolved from environment variables.</p> Source code in <code>src/splitter_mr/model/models/grok_model.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = os.getenv(\"XAI_API_KEY\"),\n    model_name: str = os.getenv(\"XAI_MODEL\", \"grok-4\"),\n) -&gt; None:\n    \"\"\"\n    Initializes the GrokVisionModel.\n\n    Args:\n        api_key (str, optional): Grok API key. If not provided, uses the\n            ``XAI_API_KEY`` environment variable.\n        model_name (str, optional): Model identifier to use. If not provided,\n            defaults to ``XAI_MODEL`` environment variable or ``\"grok-4\"``.\n\n    Raises:\n        ValueError: If ``api_key`` is not provided or cannot be resolved\n            from environment variables.\n    \"\"\"\n    api_key = api_key or os.getenv(\"XAI_API_KEY\")\n    model_name = model_name or os.getenv(\"XAI_MODEL\") or \"grok-4\"\n\n    if not api_key:\n        raise ValueError(\n            \"Grok API key not provided or 'XAI_API_KEY' env var is not set.\"\n        )\n\n    self.model_name = model_name\n    self.client = Client(\n        api_key=api_key,\n        base_url=\"https://api.x.ai/v1\",\n    )  # TODO: Change to xAI SDK\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.grok_model.GrokVisionModel.analyze_content","title":"<code>analyze_content(file, prompt=None, *, file_ext='png', detail='auto', **parameters)</code>","text":"<p>Extract text from an image using the Grok Vision model.</p> <p>Encodes the given image as a data URI with an appropriate MIME type based on <code>file_ext</code> and sends it along with a prompt to the Grok API. The API processes the image and returns extracted text in the response.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>bytes</code> <p>Base64-encoded image content without the <code>data:image/...;base64,</code> prefix. Must not be None.</p> required <code>prompt</code> <code>str</code> <p>Instruction text guiding the extraction. Defaults to <code>DEFAULT_IMAGE_CAPTION_PROMPT</code>.</p> <code>None</code> <code>file_ext</code> <code>str</code> <p>File extension (e.g., <code>\"png\"</code>, <code>\"jpg\"</code>) used to determine the MIME type for the image. Defaults to <code>\"png\"</code>.</p> <code>'png'</code> <code>detail</code> <code>str</code> <p>Level of detail to request for the image analysis. Options typically include <code>\"low\"</code>, <code>\"high\"</code> or <code>\"auto\"</code>. Defaults to <code>\"auto\"</code>.</p> <code>'auto'</code> <code>**parameters</code> <code>Any</code> <p>Additional keyword arguments passed directly to the Grok client <code>chat.completions.create()</code> method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The extracted text returned by the vision model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>file</code> is None or the file extension is not compatible.</p> <code>OpenAIError</code> <p>If the API request fails.</p> Example <pre><code>from splitter_mr.model import GrokVisionModel\n\nmodel = GrokVisionModel()\nwith open(\"image.jpg\", \"rb\") as f:\n    img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n\ntext = model.analyze_content(\n    img_b64, prompt=\"What's in this image?\", file_ext=\"jpg\", detail=\"high\"\n)\nprint(text)\n</code></pre> Source code in <code>src/splitter_mr/model/models/grok_model.py</code> <pre><code>def analyze_content(\n    self,\n    file: Optional[bytes],\n    prompt: Optional[str] = None,\n    *,\n    file_ext: Optional[str] = \"png\",\n    detail: str = \"auto\",\n    **parameters: Any,\n) -&gt; str:\n    \"\"\"\n    Extract text from an image using the Grok Vision model.\n\n    Encodes the given image as a data URI with an appropriate MIME type based on\n    ``file_ext`` and sends it along with a prompt to the Grok API. The API\n    processes the image and returns extracted text in the response.\n\n    Args:\n        file (bytes, optional): Base64-encoded image content **without** the\n            ``data:image/...;base64,`` prefix. Must not be None.\n        prompt (str, optional): Instruction text guiding the extraction.\n            Defaults to ``DEFAULT_IMAGE_CAPTION_PROMPT``.\n        file_ext (str, optional): File extension (e.g., ``\"png\"``, ``\"jpg\"``)\n            used to determine the MIME type for the image. Defaults to ``\"png\"``.\n        detail (str, optional): Level of detail to request for the image\n            analysis. Options typically include ``\"low\"``, ``\"high\"`` or ``\"auto\"``.\n            Defaults to ``\"auto\"``.\n        **parameters (Any): Additional keyword arguments passed directly to\n            the Grok client ``chat.completions.create()`` method.\n\n    Returns:\n        str: The extracted text returned by the vision model.\n\n    Raises:\n        ValueError: If ``file`` is None or the file extension is not compatible.\n        openai.OpenAIError: If the API request fails.\n\n    Example:\n        ```python\n        from splitter_mr.model import GrokVisionModel\n\n        model = GrokVisionModel()\n        with open(\"image.jpg\", \"rb\") as f:\n            img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n\n        text = model.analyze_content(\n            img_b64, prompt=\"What's in this image?\", file_ext=\"jpg\", detail=\"high\"\n        )\n        print(text)\n        ```\n    \"\"\"\n    if file is None:\n        raise ValueError(\"No file content provided for text extraction.\")\n\n    ext = (file_ext or \"png\").lower()\n    mime_type = (\n        GROK_MIME_BY_EXTENSION.get(ext)  # noqa: W503\n        or mimetypes.types_map.get(f\".{ext}\")  # noqa: W503\n        or \"image/png\"  # noqa: W503\n    )\n\n    if mime_type not in SUPPORTED_GROK_MIME_TYPES:\n        raise ValueError(f\"Unsupported image MIME type: {mime_type}\")\n\n    prompt = prompt or DEFAULT_IMAGE_CAPTION_PROMPT\n\n    payload_obj = OpenAIClientPayload(\n        role=\"user\",\n        content=[\n            OpenAIClientTextContent(type=\"text\", text=prompt),\n            OpenAIClientImageContent(\n                type=\"image_url\",\n                image_url=OpenAIClientImageUrl(\n                    url=f\"data:{mime_type};base64,{file}\",\n                    detail=detail,\n                ),\n            ),\n        ],\n    )\n\n    response = self.client.chat.completions.create(\n        model=self.model_name, messages=[payload_obj], **parameters\n    )\n\n    return response.choices[0].message.content\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.grok_model.GrokVisionModel.get_client","title":"<code>get_client()</code>","text":"<p>Returns the underlying Grok API client.</p> <p>Returns:</p> Name Type Description <code>Client</code> <code>Client</code> <p>The initialized Grok <code>Client</code> instance.</p> Source code in <code>src/splitter_mr/model/models/grok_model.py</code> <pre><code>def get_client(self) -&gt; Client:\n    \"\"\"\n    Returns the underlying Grok API client.\n\n    Returns:\n        Client: The initialized Grok ``Client`` instance.\n    \"\"\"\n    return self.client\n</code></pre>"},{"location":"api_reference/model/#geminivisionmodel","title":"GeminiVisionModel","text":""},{"location":"api_reference/model/#src.splitter_mr.model.models.gemini_model.GeminiVisionModel","title":"<code>GeminiVisionModel</code>","text":"<p>               Bases: <code>BaseVisionModel</code></p> <p>Implementation of <code>BaseVisionModel</code> using Google's Gemini Image Understanding API.</p> Source code in <code>src/splitter_mr/model/models/gemini_model.py</code> <pre><code>class GeminiVisionModel(BaseVisionModel):\n    \"\"\"Implementation of `BaseVisionModel` using Google's Gemini Image Understanding API.\"\"\"\n\n    def __init__(\n        self, api_key: Optional[str] = None, model_name: str = \"gemini-2.5-flash\"\n    ) -&gt; None:\n        \"\"\"\n        Initialize the GeminiVisionModel.\n\n        Args:\n            api_key: Gemini API key. If not provided, uses 'GEMINI_API_KEY' env var.\n            model_name: Vision-capable Gemini model name.\n\n        Raises:\n            ImportError: If `google-generativeai` is not installed.\n            ValueError: If no API key is provided or 'GEMINI_API_KEY' not set.\n        \"\"\"\n\n        if api_key is None:\n            api_key = os.getenv(\"GEMINI_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"Google Gemini API key not provided or 'GEMINI_API_KEY' not set.\"\n            )\n\n        self.api_key = api_key\n        self.model_name = model_name\n        self.client = genai.Client(api_key=self.api_key)\n        self.model = self.client.models\n        self._types = types  # keep handle for analyze_content\n\n    def get_client(self) -&gt; Any:\n        \"\"\"Return the underlying Gemini SDK client.\"\"\"\n        return self.client\n\n    def analyze_content(\n        self,\n        prompt: str,\n        file: Optional[bytes],\n        file_ext: Optional[str] = None,\n        **parameters: Any,\n    ) -&gt; str:\n        \"\"\"Extract text from an image using Gemini's image understanding API.\"\"\"\n        if file is None:\n            raise ValueError(\"No image file provided for extraction.\")\n\n        ext = (file_ext or \"jpg\").lower()\n        mime_type = mimetypes.types_map.get(f\".{ext}\", \"image/jpeg\")\n\n        img_b64 = file.decode(\"utf-8\") if isinstance(file, (bytes, bytearray)) else file\n        try:\n            img_bytes = base64.b64decode(img_b64)\n        except Exception as e:\n            raise ValueError(f\"Failed to decode base64 image data: {e}\")\n\n        # Build Gemini-compatible parts (using lazy-imported types)\n        image_part = self._types.Part.from_bytes(data=img_bytes, mime_type=mime_type)\n        text_part = prompt\n        contents = [image_part, text_part]\n\n        try:\n            response = self.model.generate_content(\n                model=self.model_name,\n                contents=contents,\n                **parameters,\n            )\n            return response.text\n        except Exception as e:\n            raise RuntimeError(f\"Gemini model inference failed: {e}\")\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.gemini_model.GeminiVisionModel.__init__","title":"<code>__init__(api_key=None, model_name='gemini-2.5-flash')</code>","text":"<p>Initialize the GeminiVisionModel.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>Gemini API key. If not provided, uses 'GEMINI_API_KEY' env var.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>Vision-capable Gemini model name.</p> <code>'gemini-2.5-flash'</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>google-generativeai</code> is not installed.</p> <code>ValueError</code> <p>If no API key is provided or 'GEMINI_API_KEY' not set.</p> Source code in <code>src/splitter_mr/model/models/gemini_model.py</code> <pre><code>def __init__(\n    self, api_key: Optional[str] = None, model_name: str = \"gemini-2.5-flash\"\n) -&gt; None:\n    \"\"\"\n    Initialize the GeminiVisionModel.\n\n    Args:\n        api_key: Gemini API key. If not provided, uses 'GEMINI_API_KEY' env var.\n        model_name: Vision-capable Gemini model name.\n\n    Raises:\n        ImportError: If `google-generativeai` is not installed.\n        ValueError: If no API key is provided or 'GEMINI_API_KEY' not set.\n    \"\"\"\n\n    if api_key is None:\n        api_key = os.getenv(\"GEMINI_API_KEY\")\n    if not api_key:\n        raise ValueError(\n            \"Google Gemini API key not provided or 'GEMINI_API_KEY' not set.\"\n        )\n\n    self.api_key = api_key\n    self.model_name = model_name\n    self.client = genai.Client(api_key=self.api_key)\n    self.model = self.client.models\n    self._types = types  # keep handle for analyze_content\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.gemini_model.GeminiVisionModel.analyze_content","title":"<code>analyze_content(prompt, file, file_ext=None, **parameters)</code>","text":"<p>Extract text from an image using Gemini's image understanding API.</p> Source code in <code>src/splitter_mr/model/models/gemini_model.py</code> <pre><code>def analyze_content(\n    self,\n    prompt: str,\n    file: Optional[bytes],\n    file_ext: Optional[str] = None,\n    **parameters: Any,\n) -&gt; str:\n    \"\"\"Extract text from an image using Gemini's image understanding API.\"\"\"\n    if file is None:\n        raise ValueError(\"No image file provided for extraction.\")\n\n    ext = (file_ext or \"jpg\").lower()\n    mime_type = mimetypes.types_map.get(f\".{ext}\", \"image/jpeg\")\n\n    img_b64 = file.decode(\"utf-8\") if isinstance(file, (bytes, bytearray)) else file\n    try:\n        img_bytes = base64.b64decode(img_b64)\n    except Exception as e:\n        raise ValueError(f\"Failed to decode base64 image data: {e}\")\n\n    # Build Gemini-compatible parts (using lazy-imported types)\n    image_part = self._types.Part.from_bytes(data=img_bytes, mime_type=mime_type)\n    text_part = prompt\n    contents = [image_part, text_part]\n\n    try:\n        response = self.model.generate_content(\n            model=self.model_name,\n            contents=contents,\n            **parameters,\n        )\n        return response.text\n    except Exception as e:\n        raise RuntimeError(f\"Gemini model inference failed: {e}\")\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.gemini_model.GeminiVisionModel.get_client","title":"<code>get_client()</code>","text":"<p>Return the underlying Gemini SDK client.</p> Source code in <code>src/splitter_mr/model/models/gemini_model.py</code> <pre><code>def get_client(self) -&gt; Any:\n    \"\"\"Return the underlying Gemini SDK client.\"\"\"\n    return self.client\n</code></pre>"},{"location":"api_reference/model/#anthropicvisionmodel","title":"AnthropicVisionModel","text":""},{"location":"api_reference/model/#src.splitter_mr.model.models.anthropic_model.AnthropicVisionModel","title":"<code>AnthropicVisionModel</code>","text":"<p>               Bases: <code>BaseVisionModel</code></p> <p>Implementation of BaseVisionModel using Anthropic's Claude Vision API via OpenAI SDK.</p> <p>Sends base64-encoded images + prompts to the Claude multimodal endpoint.</p> Source code in <code>src/splitter_mr/model/models/anthropic_model.py</code> <pre><code>class AnthropicVisionModel(BaseVisionModel):\n    \"\"\"\n    Implementation of BaseVisionModel using Anthropic's Claude Vision API via OpenAI SDK.\n\n    Sends base64-encoded images + prompts to the Claude multimodal endpoint.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        model_name: str = os.getenv(\"ANTHROPIC_MODEL\", \"claude-sonnet-4-20250514\"),\n    ) -&gt; None:\n        \"\"\"\n        Initialize the AnthropicVisionModel.\n\n        Args:\n            api_key (str, optional): Anthropic API key. Uses ANTHROPIC_API_KEY env var if not provided.\n            model_name (str): Vision-capable Claude model name.\n\n        Raises:\n            ValueError: If no API key provided or found in environment.\n        \"\"\"\n        if api_key is None:\n            api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n            if not api_key:\n                raise ValueError(\n                    \"Anthropic API key not provided and 'ANTHROPIC_API_KEY' env var not set.\"\n                )\n\n        base_url: str = (\"https://api.anthropic.com/v1/\",)\n        self.client = OpenAI(api_key=api_key, base_url=base_url)\n        self.model_name = model_name\n\n    def get_client(self) -&gt; OpenAI:\n        \"\"\"\n        Get the underlying Anthropic API client instance.\n\n        Returns:\n            OpenAI: The initialized API client.\n        \"\"\"\n        return self.client\n\n    def analyze_content(\n        self,\n        file: Optional[bytes],\n        prompt: str = DEFAULT_IMAGE_CAPTION_PROMPT,\n        *,\n        file_ext: Optional[str] = \"png\",\n        **parameters: Dict[str, Any],\n    ) -&gt; str:\n        \"\"\"\n        Extract text from an image using Anthropic's Claude Vision API.\n\n        Args:\n            prompt (str): Task or instruction (e.g. \"Describe the image contents\").\n            file (bytes): Base64-encoded image content, no prefix/header.\n            file_ext (str, optional): File extension (e.g. \"png\", \"jpg\").\n            **parameters: Extra arguments to client.chat.completions.create().\n\n        Returns:\n            str: Extracted text or model response.\n\n        Raises:\n            ValueError: If file is None or unsupported file type.\n            RuntimeError: For failed/invalid responses.\n        \"\"\"\n        if file is None:\n            raise ValueError(\"No file content provided for vision model.\")\n\n        ext = (file_ext or \"png\").lower()\n        mime_type = (\n            OPENAI_MIME_BY_EXTENSION.get(ext)\n            or mimetypes.types_map.get(f\".{ext}\")  # noqa: W503\n            or \"image/png\"  # noqa: W503\n        )\n        if mime_type not in SUPPORTED_OPENAI_MIME_TYPES:\n            raise ValueError(f\"Unsupported image MIME type for Anthropic: {mime_type}\")\n\n        # Build multimodal payload in OpenAI/Anthropic-compatible format\n        payload_obj = OpenAIClientPayload(\n            role=\"user\",\n            content=[\n                OpenAIClientTextContent(type=\"text\", text=prompt),\n                OpenAIClientImageContent(\n                    type=\"image_url\",\n                    image_url=OpenAIClientImageUrl(\n                        url=f\"data:{mime_type};base64,{file}\"\n                    ),\n                ),\n            ],\n        )\n        payload = payload_obj.model_dump(exclude_none=True)\n\n        response = self.client.chat.completions.create(\n            model=self.model_name,\n            messages=[payload],\n            **parameters,\n        )\n        try:\n            return response.choices[0].message.content\n        except Exception as e:\n            raise RuntimeError(f\"Failed to extract response: {e}\")\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.anthropic_model.AnthropicVisionModel.__init__","title":"<code>__init__(api_key=None, model_name=os.getenv('ANTHROPIC_MODEL', 'claude-sonnet-4-20250514'))</code>","text":"<p>Initialize the AnthropicVisionModel.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Anthropic API key. Uses ANTHROPIC_API_KEY env var if not provided.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>Vision-capable Claude model name.</p> <code>getenv('ANTHROPIC_MODEL', 'claude-sonnet-4-20250514')</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no API key provided or found in environment.</p> Source code in <code>src/splitter_mr/model/models/anthropic_model.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = None,\n    model_name: str = os.getenv(\"ANTHROPIC_MODEL\", \"claude-sonnet-4-20250514\"),\n) -&gt; None:\n    \"\"\"\n    Initialize the AnthropicVisionModel.\n\n    Args:\n        api_key (str, optional): Anthropic API key. Uses ANTHROPIC_API_KEY env var if not provided.\n        model_name (str): Vision-capable Claude model name.\n\n    Raises:\n        ValueError: If no API key provided or found in environment.\n    \"\"\"\n    if api_key is None:\n        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"Anthropic API key not provided and 'ANTHROPIC_API_KEY' env var not set.\"\n            )\n\n    base_url: str = (\"https://api.anthropic.com/v1/\",)\n    self.client = OpenAI(api_key=api_key, base_url=base_url)\n    self.model_name = model_name\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.anthropic_model.AnthropicVisionModel.analyze_content","title":"<code>analyze_content(file, prompt=DEFAULT_IMAGE_CAPTION_PROMPT, *, file_ext='png', **parameters)</code>","text":"<p>Extract text from an image using Anthropic's Claude Vision API.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Task or instruction (e.g. \"Describe the image contents\").</p> <code>DEFAULT_IMAGE_CAPTION_PROMPT</code> <code>file</code> <code>bytes</code> <p>Base64-encoded image content, no prefix/header.</p> required <code>file_ext</code> <code>str</code> <p>File extension (e.g. \"png\", \"jpg\").</p> <code>'png'</code> <code>**parameters</code> <code>Dict[str, Any]</code> <p>Extra arguments to client.chat.completions.create().</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text or model response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If file is None or unsupported file type.</p> <code>RuntimeError</code> <p>For failed/invalid responses.</p> Source code in <code>src/splitter_mr/model/models/anthropic_model.py</code> <pre><code>def analyze_content(\n    self,\n    file: Optional[bytes],\n    prompt: str = DEFAULT_IMAGE_CAPTION_PROMPT,\n    *,\n    file_ext: Optional[str] = \"png\",\n    **parameters: Dict[str, Any],\n) -&gt; str:\n    \"\"\"\n    Extract text from an image using Anthropic's Claude Vision API.\n\n    Args:\n        prompt (str): Task or instruction (e.g. \"Describe the image contents\").\n        file (bytes): Base64-encoded image content, no prefix/header.\n        file_ext (str, optional): File extension (e.g. \"png\", \"jpg\").\n        **parameters: Extra arguments to client.chat.completions.create().\n\n    Returns:\n        str: Extracted text or model response.\n\n    Raises:\n        ValueError: If file is None or unsupported file type.\n        RuntimeError: For failed/invalid responses.\n    \"\"\"\n    if file is None:\n        raise ValueError(\"No file content provided for vision model.\")\n\n    ext = (file_ext or \"png\").lower()\n    mime_type = (\n        OPENAI_MIME_BY_EXTENSION.get(ext)\n        or mimetypes.types_map.get(f\".{ext}\")  # noqa: W503\n        or \"image/png\"  # noqa: W503\n    )\n    if mime_type not in SUPPORTED_OPENAI_MIME_TYPES:\n        raise ValueError(f\"Unsupported image MIME type for Anthropic: {mime_type}\")\n\n    # Build multimodal payload in OpenAI/Anthropic-compatible format\n    payload_obj = OpenAIClientPayload(\n        role=\"user\",\n        content=[\n            OpenAIClientTextContent(type=\"text\", text=prompt),\n            OpenAIClientImageContent(\n                type=\"image_url\",\n                image_url=OpenAIClientImageUrl(\n                    url=f\"data:{mime_type};base64,{file}\"\n                ),\n            ),\n        ],\n    )\n    payload = payload_obj.model_dump(exclude_none=True)\n\n    response = self.client.chat.completions.create(\n        model=self.model_name,\n        messages=[payload],\n        **parameters,\n    )\n    try:\n        return response.choices[0].message.content\n    except Exception as e:\n        raise RuntimeError(f\"Failed to extract response: {e}\")\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.anthropic_model.AnthropicVisionModel.get_client","title":"<code>get_client()</code>","text":"<p>Get the underlying Anthropic API client instance.</p> <p>Returns:</p> Name Type Description <code>OpenAI</code> <code>OpenAI</code> <p>The initialized API client.</p> Source code in <code>src/splitter_mr/model/models/anthropic_model.py</code> <pre><code>def get_client(self) -&gt; OpenAI:\n    \"\"\"\n    Get the underlying Anthropic API client instance.\n\n    Returns:\n        OpenAI: The initialized API client.\n    \"\"\"\n    return self.client\n</code></pre>"},{"location":"api_reference/model/#huggingfacevisionmodel","title":"HuggingFaceVisionModel","text":"<p>Warning</p> <p><code>HuggingFaceVisionModel</code> can NOT currently support all the models available in HuggingFace. </p> <p>For example, closed models (e.g., Microsoft Florence 2 large) or models which uses uncommon architectures (NanoNets). We strongly recommend to use SmolDocling, since it has been exhaustively tested.</p> <p> </p>"},{"location":"api_reference/model/#src.splitter_mr.model.models.huggingface_model.HuggingFaceVisionModel","title":"<code>HuggingFaceVisionModel</code>","text":"<p>               Bases: <code>BaseVisionModel</code></p> <p>Vision-language model wrapper using Hugging Face Transformers.</p> <p>This implementation loads a local or Hugging Face Hub model that supports image-to-text or multimodal tasks. It accepts a prompt and an image as base64 (without the data URI header) and returns the model's generated text. Pydantic schema models are used for message validation.</p> Example <pre><code>import base64, requests\nfrom splitter_mr.model.models.huggingface_model import HuggingFaceVisionModel\n\n# Encode an image as base64\nimg_bytes = requests.get(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/\"\n    \"resolve/main/p-blog/candy.JPG\"\n).content\nimg_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n\nmodel = HuggingFaceVisionModel(\"ds4sd/SmolDocling-256M-preview\")\nresult = model.analyze_content(\"What animal is on the candy?\", file=img_b64)\nprint(result)  # e.g., \"A small green thing.\"\n</code></pre> Source code in <code>src/splitter_mr/model/models/huggingface_model.py</code> <pre><code>class HuggingFaceVisionModel(BaseVisionModel):\n    \"\"\"\n    Vision-language model wrapper using Hugging Face Transformers.\n\n    This implementation loads a local or Hugging Face Hub model that supports\n    image-to-text or multimodal tasks. It accepts a prompt and an image as\n    base64 (without the data URI header) and returns the model's generated text.\n    Pydantic schema models are used for message validation.\n\n    Example:\n        ```python\n        import base64, requests\n        from splitter_mr.model.models.huggingface_model import HuggingFaceVisionModel\n\n        # Encode an image as base64\n        img_bytes = requests.get(\n            \"https://huggingface.co/datasets/huggingface/documentation-images/\"\n            \"resolve/main/p-blog/candy.JPG\"\n        ).content\n        img_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n\n        model = HuggingFaceVisionModel(\"ds4sd/SmolDocling-256M-preview\")\n        result = model.analyze_content(\"What animal is on the candy?\", file=img_b64)\n        print(result)  # e.g., \"A small green thing.\"\n        ```\n    \"\"\"\n\n    DEFAULT_EXT: str = \"jpg\"\n    FALLBACKS: List[Tuple[str, Optional[Any]]] = [\n        (\"AutoModelForVision2Seq\", None),\n        (\"AutoModelForImageTextToText\", None),\n        (\"AutoModelForCausalLM\", None),\n        (\"AutoModelForPreTraining\", None),\n        (\"AutoModel\", None),\n    ]\n\n    def __init__(self, model_name: str = \"ds4sd/SmolDocling-256M-preview\") -&gt; None:\n        \"\"\"\n        Initialize a HuggingFaceVisionModel.\n\n        Args:\n            model_name (str, optional): Model repo ID or local path\n                (e.g., ``\"ds4sd/SmolDocling-256M-preview\"``).\n\n        Raises:\n            ImportError: If the 'multimodal' extra (transformers) is not installed.\n            RuntimeError: If processor or model loading fails after all attempts.\n        \"\"\"\n\n        transformers = importlib.import_module(\"transformers\")\n\n        AutoProcessor = transformers.AutoProcessor\n        AutoImageProcessor = transformers.AutoImageProcessor\n        AutoConfig = transformers.AutoConfig\n\n        self.model_id = model_name\n        self.model = None\n        self.processor = None\n\n        # Load processor\n        try:\n            self.processor = AutoProcessor.from_pretrained(\n                self.model_id, trust_remote_code=True\n            )\n        except Exception:\n            try:\n                self.processor = AutoImageProcessor.from_pretrained(\n                    self.model_id, trust_remote_code=True\n                )\n            except Exception as e:\n                raise RuntimeError(\"All processor loading attempts failed.\") from e\n\n        # Load model\n        config = AutoConfig.from_pretrained(self.model_id)\n        errors: List[str] = []\n\n        try:\n            arch_name = config.architectures[0]\n            ModelClass = getattr(transformers, arch_name)\n            self.model = ModelClass.from_pretrained(\n                self.model_id, trust_remote_code=True\n            )\n        except Exception as e:\n            errors.append(f\"[AutoModel by architecture] {e}\")\n\n        if self.model is None:\n            resolved: List[Tuple[str, Any]] = []\n            for name, cls in self.FALLBACKS:\n                resolved.append((name, cls or getattr(transformers, name)))\n            for name, cls in resolved:\n                try:\n                    self.model = cls.from_pretrained(\n                        self.model_id, trust_remote_code=True\n                    )\n                    break\n                except Exception as e:\n                    errors.append(f\"[{name}] {e}\")\n\n        if self.model is None:\n            raise RuntimeError(\n                \"All model loading attempts failed:\\n\" + \"\\n\".join(errors)\n            )\n\n    def get_client(self) -&gt; Any:\n        \"\"\"Return the underlying HuggingFace model instance.\n\n        Returns:\n            Any: The instantiated HuggingFace model object.\n        \"\"\"\n        return self.model\n\n    def analyze_content(\n        self,\n        prompt: str,\n        file: Optional[bytes],\n        file_ext: Optional[str] = None,\n        **parameters: Dict[str, Any],\n    ) -&gt; str:\n        \"\"\"\n        Extract text from an image using the vision-language model.\n\n        This method encodes an image as a data URI, builds a validated\n        message using schema models, prepares inputs, and calls the model\n        to generate a textual response.\n\n        Args:\n            prompt (str): Instruction or question for the model\n                (e.g., ``\"Describe this image.\"``).\n            file (Optional[bytes]): Image as a base64-encoded string (without prefix).\n            file_ext (Optional[str], optional): File extension (e.g., ``\"jpg\"`` or ``\"png\"``).\n                Defaults to ``\"jpg\"`` if not provided.\n            **parameters (Dict[str, Any]): Extra keyword arguments passed directly\n                to the model's ``generate()`` method (e.g., ``max_new_tokens``,\n                ``temperature``).\n\n        Returns:\n            str: The extracted or generated text.\n\n        Raises:\n            ValueError: If ``file`` is None.\n            RuntimeError: If input preparation or inference fails.\n        \"\"\"\n        if file is None:\n            raise ValueError(\"No image file provided for extraction.\")\n\n        ext = (file_ext or self.DEFAULT_EXT).lower()\n        mime_type = mimetypes.types_map.get(f\".{ext}\", \"image/jpeg\")\n        img_b64 = file if isinstance(file, str) else file.decode(\"utf-8\")\n        img_data_uri = f\"data:{mime_type};base64,{img_b64}\"\n\n        text_content = HFChatTextContent(type=\"text\", text=prompt)\n        image_content = HFChatImageContent(type=\"image\", image=img_data_uri)\n        chat_msg = HFChatMessage(role=\"user\", content=[image_content, text_content])\n        messages = [chat_msg.model_dump(exclude_none=True)]\n\n        try:\n            inputs = self.processor.apply_chat_template(\n                messages,\n                add_generation_prompt=True,\n                tokenize=True,\n                return_dict=True,\n                return_tensors=\"pt\",\n                truncation=True,\n            ).to(self.model.device)\n        except Exception as e:\n            raise RuntimeError(f\"Failed to prepare input: {e}\")\n\n        try:\n            max_new_tokens = parameters.pop(\"max_new_tokens\", 40)\n            outputs = self.model.generate(\n                **inputs, max_new_tokens=max_new_tokens, **parameters\n            )\n            output_text = self.processor.decode(\n                outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n            )\n            return output_text\n        except Exception as e:\n            raise RuntimeError(f\"Model inference failed: {e}\")\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.huggingface_model.HuggingFaceVisionModel.__init__","title":"<code>__init__(model_name='ds4sd/SmolDocling-256M-preview')</code>","text":"<p>Initialize a HuggingFaceVisionModel.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model repo ID or local path (e.g., <code>\"ds4sd/SmolDocling-256M-preview\"</code>).</p> <code>'ds4sd/SmolDocling-256M-preview'</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the 'multimodal' extra (transformers) is not installed.</p> <code>RuntimeError</code> <p>If processor or model loading fails after all attempts.</p> Source code in <code>src/splitter_mr/model/models/huggingface_model.py</code> <pre><code>def __init__(self, model_name: str = \"ds4sd/SmolDocling-256M-preview\") -&gt; None:\n    \"\"\"\n    Initialize a HuggingFaceVisionModel.\n\n    Args:\n        model_name (str, optional): Model repo ID or local path\n            (e.g., ``\"ds4sd/SmolDocling-256M-preview\"``).\n\n    Raises:\n        ImportError: If the 'multimodal' extra (transformers) is not installed.\n        RuntimeError: If processor or model loading fails after all attempts.\n    \"\"\"\n\n    transformers = importlib.import_module(\"transformers\")\n\n    AutoProcessor = transformers.AutoProcessor\n    AutoImageProcessor = transformers.AutoImageProcessor\n    AutoConfig = transformers.AutoConfig\n\n    self.model_id = model_name\n    self.model = None\n    self.processor = None\n\n    # Load processor\n    try:\n        self.processor = AutoProcessor.from_pretrained(\n            self.model_id, trust_remote_code=True\n        )\n    except Exception:\n        try:\n            self.processor = AutoImageProcessor.from_pretrained(\n                self.model_id, trust_remote_code=True\n            )\n        except Exception as e:\n            raise RuntimeError(\"All processor loading attempts failed.\") from e\n\n    # Load model\n    config = AutoConfig.from_pretrained(self.model_id)\n    errors: List[str] = []\n\n    try:\n        arch_name = config.architectures[0]\n        ModelClass = getattr(transformers, arch_name)\n        self.model = ModelClass.from_pretrained(\n            self.model_id, trust_remote_code=True\n        )\n    except Exception as e:\n        errors.append(f\"[AutoModel by architecture] {e}\")\n\n    if self.model is None:\n        resolved: List[Tuple[str, Any]] = []\n        for name, cls in self.FALLBACKS:\n            resolved.append((name, cls or getattr(transformers, name)))\n        for name, cls in resolved:\n            try:\n                self.model = cls.from_pretrained(\n                    self.model_id, trust_remote_code=True\n                )\n                break\n            except Exception as e:\n                errors.append(f\"[{name}] {e}\")\n\n    if self.model is None:\n        raise RuntimeError(\n            \"All model loading attempts failed:\\n\" + \"\\n\".join(errors)\n        )\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.huggingface_model.HuggingFaceVisionModel.analyze_content","title":"<code>analyze_content(prompt, file, file_ext=None, **parameters)</code>","text":"<p>Extract text from an image using the vision-language model.</p> <p>This method encodes an image as a data URI, builds a validated message using schema models, prepares inputs, and calls the model to generate a textual response.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Instruction or question for the model (e.g., <code>\"Describe this image.\"</code>).</p> required <code>file</code> <code>Optional[bytes]</code> <p>Image as a base64-encoded string (without prefix).</p> required <code>file_ext</code> <code>Optional[str]</code> <p>File extension (e.g., <code>\"jpg\"</code> or <code>\"png\"</code>). Defaults to <code>\"jpg\"</code> if not provided.</p> <code>None</code> <code>**parameters</code> <code>Dict[str, Any]</code> <p>Extra keyword arguments passed directly to the model's <code>generate()</code> method (e.g., <code>max_new_tokens</code>, <code>temperature</code>).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The extracted or generated text.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>file</code> is None.</p> <code>RuntimeError</code> <p>If input preparation or inference fails.</p> Source code in <code>src/splitter_mr/model/models/huggingface_model.py</code> <pre><code>def analyze_content(\n    self,\n    prompt: str,\n    file: Optional[bytes],\n    file_ext: Optional[str] = None,\n    **parameters: Dict[str, Any],\n) -&gt; str:\n    \"\"\"\n    Extract text from an image using the vision-language model.\n\n    This method encodes an image as a data URI, builds a validated\n    message using schema models, prepares inputs, and calls the model\n    to generate a textual response.\n\n    Args:\n        prompt (str): Instruction or question for the model\n            (e.g., ``\"Describe this image.\"``).\n        file (Optional[bytes]): Image as a base64-encoded string (without prefix).\n        file_ext (Optional[str], optional): File extension (e.g., ``\"jpg\"`` or ``\"png\"``).\n            Defaults to ``\"jpg\"`` if not provided.\n        **parameters (Dict[str, Any]): Extra keyword arguments passed directly\n            to the model's ``generate()`` method (e.g., ``max_new_tokens``,\n            ``temperature``).\n\n    Returns:\n        str: The extracted or generated text.\n\n    Raises:\n        ValueError: If ``file`` is None.\n        RuntimeError: If input preparation or inference fails.\n    \"\"\"\n    if file is None:\n        raise ValueError(\"No image file provided for extraction.\")\n\n    ext = (file_ext or self.DEFAULT_EXT).lower()\n    mime_type = mimetypes.types_map.get(f\".{ext}\", \"image/jpeg\")\n    img_b64 = file if isinstance(file, str) else file.decode(\"utf-8\")\n    img_data_uri = f\"data:{mime_type};base64,{img_b64}\"\n\n    text_content = HFChatTextContent(type=\"text\", text=prompt)\n    image_content = HFChatImageContent(type=\"image\", image=img_data_uri)\n    chat_msg = HFChatMessage(role=\"user\", content=[image_content, text_content])\n    messages = [chat_msg.model_dump(exclude_none=True)]\n\n    try:\n        inputs = self.processor.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=True,\n            return_dict=True,\n            return_tensors=\"pt\",\n            truncation=True,\n        ).to(self.model.device)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to prepare input: {e}\")\n\n    try:\n        max_new_tokens = parameters.pop(\"max_new_tokens\", 40)\n        outputs = self.model.generate(\n            **inputs, max_new_tokens=max_new_tokens, **parameters\n        )\n        output_text = self.processor.decode(\n            outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n        )\n        return output_text\n    except Exception as e:\n        raise RuntimeError(f\"Model inference failed: {e}\")\n</code></pre>"},{"location":"api_reference/model/#src.splitter_mr.model.models.huggingface_model.HuggingFaceVisionModel.get_client","title":"<code>get_client()</code>","text":"<p>Return the underlying HuggingFace model instance.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The instantiated HuggingFace model object.</p> Source code in <code>src/splitter_mr/model/models/huggingface_model.py</code> <pre><code>def get_client(self) -&gt; Any:\n    \"\"\"Return the underlying HuggingFace model instance.\n\n    Returns:\n        Any: The instantiated HuggingFace model object.\n    \"\"\"\n    return self.model\n</code></pre>"},{"location":"api_reference/reader/","title":"Reader","text":""},{"location":"api_reference/reader/#introduction","title":"Introduction","text":"<p>The Reader component is designed to read files homogeneously which come from many different formats and extensions. All of these readers are implemented sharing the same parent class, <code>BaseReader</code>.</p>"},{"location":"api_reference/reader/#which-reader-should-i-use-for-my-project","title":"Which Reader should I use for my project?","text":"<p>Each Reader component extracts document text in different ways. Therefore, choosing the most suitable Reader component depends on your use case.</p> <ul> <li>If you want to preserve the original structure as much as possible, without any kind of markdown parsing, you can use the <code>VanillaReader</code> class.</li> <li>In case that you have documents which have presented many tables in its structure or with many visual components (such as images), we strongly recommend to use <code>DoclingReader</code>. </li> <li>If you are looking to maximize efficiency or make conversions to markdown simpler, we recommend using the <code>MarkItDownReader</code> component.</li> </ul> <p>Note</p> <p>Remember to visit the official repository and guides for these two last reader classes: </p> <ul> <li>Docling Developer guide </li> <li>MarkItDown GitHub repository.</li> </ul> <p>Additionally, the file compatibility depending on the Reader class is given by the following table:</p> Reader Unstructured files &amp; PDFs MS Office suite files Tabular data Files with hierarchical schema Image files Markdown conversion Vanilla Reader <code>txt</code>, <code>md</code>, <code>pdf</code> <code>xlsx</code>, <code>docx</code>, <code>pptx</code> <code>csv</code>, <code>tsv</code>, <code>parquet</code> <code>json</code>, <code>yaml</code>, <code>html</code>, <code>xml</code> <code>jpg</code>, <code>png</code>, <code>webp</code>, <code>gif</code> Yes MarkItDown Reader <code>txt</code>, <code>md</code>, <code>pdf</code> <code>docx</code>, <code>xlsx</code>, <code>pptx</code> <code>csv</code>, <code>tsv</code> <code>json</code>, <code>html</code>, <code>xml</code> <code>jpg</code>, <code>png</code>, <code>pneg</code> Yes Docling Reader <code>txt</code>, <code>md</code>, <code>pdf</code> <code>docx</code>, <code>xlsx</code>, <code>pptx</code> \u2013 <code>html</code>, <code>xhtml</code> <code>png</code>, <code>jpeg</code>, <code>tiff</code>, <code>bmp</code>, <code>webp</code> Yes"},{"location":"api_reference/reader/#installing-docling-markitdown","title":"Installing Docling &amp; MarkItDown","text":"<p>By default, <code>pip install splitter-mr</code> installs core features only. To use <code>DoclingReader</code> and/or <code>MarkItDownReader</code>, install the corresponding extras:</p> <p>Python \u2265 3.11 is required.</p> <p>MarkItDown:</p> <pre><code>pip install \"splitter-mr[markitdown]\"\n</code></pre> <p>Docling:</p> <pre><code>pip install \"splitter-mr[docling]\"\n</code></pre> <p>Both:</p> <pre><code>pip install \"splitter-mr[markitdown,docling]\"\n</code></pre> <p>Note</p> <p>For the full matrix of extras and alternative package managers, see the global How to install section in the project README: Splitter_MR \u2014 How to install</p>"},{"location":"api_reference/reader/#output-format","title":"Output format","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model defining the output structure for all readers.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>Optional[str]</code> <p>The textual content extracted by the reader.</p> <code>document_name</code> <code>Optional[str]</code> <p>The name of the document.</p> <code>document_path</code> <code>str</code> <p>The path to the document.</p> <code>document_id</code> <code>Optional[str]</code> <p>A unique identifier for the document.</p> <code>conversion_method</code> <code>Optional[str]</code> <p>The method used for document conversion.</p> <code>reader_method</code> <code>Optional[str]</code> <p>The method used for reading the document.</p> <code>ocr_method</code> <code>Optional[str]</code> <p>The OCR method used, if any.</p> <code>page_placeholder</code> <code>Optional[str]</code> <p>The placeholder use to identify each page, if used.</p> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata associated with the document.</p> Source code in <code>src/splitter_mr/schema/models.py</code> <pre><code>class ReaderOutput(BaseModel):\n    \"\"\"Pydantic model defining the output structure for all readers.\n\n    Attributes:\n        text: The textual content extracted by the reader.\n        document_name: The name of the document.\n        document_path: The path to the document.\n        document_id: A unique identifier for the document.\n        conversion_method: The method used for document conversion.\n        reader_method: The method used for reading the document.\n        ocr_method: The OCR method used, if any.\n        page_placeholder: The placeholder use to identify each page, if used.\n        metadata: Additional metadata associated with the document.\n    \"\"\"\n\n    text: Optional[str] = \"\"\n    document_name: Optional[str] = None\n    document_path: str = \"\"\n    document_id: Optional[str] = None\n    conversion_method: Optional[str] = None\n    reader_method: Optional[str] = None\n    ocr_method: Optional[str] = None\n    page_placeholder: Optional[str] = None\n    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n\n    @field_validator(\"document_id\", mode=\"before\")\n    def default_document_id(cls, v: str):\n        \"\"\"Generate a default UUID for document_id if not provided.\n\n        Args:\n            v (str): The provided document_id value.\n\n        Returns:\n            document_id (str): The provided document_id or a newly generated UUID string.\n        \"\"\"\n        document_id = v or str(uuid.uuid4())\n        return document_id\n\n    def from_variable(\n        self, variable: Union[str, Dict[str, Any]], variable_name: str\n    ) -&gt; \"ReaderOutput\":\n        \"\"\"\n        Generate a new ReaderOutput object from a variable (str or dict).\n\n        Args:\n            variable (Union[str, Dict[str, Any]]): The variable to use as text.\n            variable_name (str): The name for document_name.\n\n        Returns:\n            ReaderOutput: The new ReaderOutput object.\n        \"\"\"\n        if isinstance(variable, dict):\n            text = json.dumps(variable, ensure_ascii=False, indent=2)\n            conversion_method = \"json\"\n            metadata = {\"details\": \"Generated from a json variable\"}\n        elif isinstance(variable, str):\n            text = variable\n            conversion_method = \"txt\"\n            metadata = {\"details\": \"Generated from a str variable\"}\n        else:\n            raise ValueError(\"Variable must be either a string or a dictionary.\")\n\n        return ReaderOutput(\n            text=text,\n            document_name=variable_name,\n            document_path=\"\",\n            conversion_method=conversion_method,\n            reader_method=\"vanilla\",\n            ocr_method=None,\n            page_placeholder=None,\n            metadata=metadata,\n        )\n\n    def append_metadata(self, metadata: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Append (update) the metadata dictionary with new key-value pairs.\n\n        Args:\n            metadata (Dict[str, Any]): The metadata to add or update.\n        \"\"\"\n        if self.metadata is None:\n            self.metadata = {}\n        self.metadata.update(metadata)\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.schema.models.ReaderOutput.append_metadata","title":"<code>append_metadata(metadata)</code>","text":"<p>Append (update) the metadata dictionary with new key-value pairs.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Dict[str, Any]</code> <p>The metadata to add or update.</p> required Source code in <code>src/splitter_mr/schema/models.py</code> <pre><code>def append_metadata(self, metadata: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Append (update) the metadata dictionary with new key-value pairs.\n\n    Args:\n        metadata (Dict[str, Any]): The metadata to add or update.\n    \"\"\"\n    if self.metadata is None:\n        self.metadata = {}\n    self.metadata.update(metadata)\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.schema.models.ReaderOutput.default_document_id","title":"<code>default_document_id(v)</code>","text":"<p>Generate a default UUID for document_id if not provided.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>str</code> <p>The provided document_id value.</p> required <p>Returns:</p> Name Type Description <code>document_id</code> <code>str</code> <p>The provided document_id or a newly generated UUID string.</p> Source code in <code>src/splitter_mr/schema/models.py</code> <pre><code>@field_validator(\"document_id\", mode=\"before\")\ndef default_document_id(cls, v: str):\n    \"\"\"Generate a default UUID for document_id if not provided.\n\n    Args:\n        v (str): The provided document_id value.\n\n    Returns:\n        document_id (str): The provided document_id or a newly generated UUID string.\n    \"\"\"\n    document_id = v or str(uuid.uuid4())\n    return document_id\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.schema.models.ReaderOutput.from_variable","title":"<code>from_variable(variable, variable_name)</code>","text":"<p>Generate a new ReaderOutput object from a variable (str or dict).</p> <p>Parameters:</p> Name Type Description Default <code>variable</code> <code>Union[str, Dict[str, Any]]</code> <p>The variable to use as text.</p> required <code>variable_name</code> <code>str</code> <p>The name for document_name.</p> required <p>Returns:</p> Name Type Description <code>ReaderOutput</code> <code>ReaderOutput</code> <p>The new ReaderOutput object.</p> Source code in <code>src/splitter_mr/schema/models.py</code> <pre><code>def from_variable(\n    self, variable: Union[str, Dict[str, Any]], variable_name: str\n) -&gt; \"ReaderOutput\":\n    \"\"\"\n    Generate a new ReaderOutput object from a variable (str or dict).\n\n    Args:\n        variable (Union[str, Dict[str, Any]]): The variable to use as text.\n        variable_name (str): The name for document_name.\n\n    Returns:\n        ReaderOutput: The new ReaderOutput object.\n    \"\"\"\n    if isinstance(variable, dict):\n        text = json.dumps(variable, ensure_ascii=False, indent=2)\n        conversion_method = \"json\"\n        metadata = {\"details\": \"Generated from a json variable\"}\n    elif isinstance(variable, str):\n        text = variable\n        conversion_method = \"txt\"\n        metadata = {\"details\": \"Generated from a str variable\"}\n    else:\n        raise ValueError(\"Variable must be either a string or a dictionary.\")\n\n    return ReaderOutput(\n        text=text,\n        document_name=variable_name,\n        document_path=\"\",\n        conversion_method=conversion_method,\n        reader_method=\"vanilla\",\n        ocr_method=None,\n        page_placeholder=None,\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api_reference/reader/#readers","title":"Readers","text":"<p>To see a comparison between reading methods, refer to the following example.</p>"},{"location":"api_reference/reader/#basereader","title":"BaseReader","text":"<p>\ud83d\udcda Note: file examples are extracted from  the<code>data</code> folder in the GitHub repository: link.</p>"},{"location":"api_reference/reader/#splitter_mr.reader.base_reader.BaseReader","title":"<code>BaseReader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all document readers.</p> <p>This interface defines the contract for file readers that process documents and return a standardized dictionary containing the extracted text and document-level metadata. Subclasses must implement the <code>read</code> method to handle specific file formats or reading strategies.</p> <p>Methods:</p> Name Description <code>read</code> <p>Reads the input file and returns a dictionary with text and metadata.</p> <code>is_valid_file_path</code> <p>Check if a path is valid.</p> <code>is_url</code> <p>Check if the string provided is an URL.</p> <code>parse_json</code> <p>Try to parse a JSON object when a dictionary or string is provided.</p> Source code in <code>src/splitter_mr/reader/base_reader.py</code> <pre><code>class BaseReader(ABC):\n    \"\"\"\n    Abstract base class for all document readers.\n\n    This interface defines the contract for file readers that process documents and return\n    a standardized dictionary containing the extracted text and document-level metadata.\n    Subclasses must implement the `read` method to handle specific file formats or reading\n    strategies.\n\n    Methods:\n        read: Reads the input file and returns a dictionary with text and metadata.\n        is_valid_file_path: Check if a path is valid.\n        is_url: Check if the string provided is an URL.\n        parse_json: Try to parse a JSON object when a dictionary or string is provided.\n    \"\"\"\n\n    @staticmethod\n    def is_valid_file_path(path: str) -&gt; bool:\n        \"\"\"\n        Checks if the provided string is a valid file path.\n\n        Args:\n            path (str): The string to check.\n\n        Returns:\n            bool: True if the string is a valid file path to an existing file, False otherwise.\n\n        Example:\n            ```python\n            BaseReader.is_valid_file_path(\"/tmp/myfile.txt\")\n            ```\n            ```bash\n            True\n            ```\n        \"\"\"\n        return os.path.isfile(path)\n\n    @staticmethod\n    def is_url(string: str) -&gt; bool:\n        \"\"\"\n        Determines whether the given string is a valid HTTP or HTTPS URL.\n\n        Args:\n            string (str): The string to check.\n\n        Returns:\n            bool: True if the string is a valid URL with HTTP or HTTPS scheme, False otherwise.\n\n        Example:\n            ```python\n            BaseReader.is_url(\"https://example.com\")\n            ```\n            ```bash\n            True\n            ```\n            ```python\n            BaseReader.is_url(\"not_a_url\")\n            ```\n            ```bash\n            False\n            ```\n        \"\"\"\n        try:\n            result = urlparse(string)\n            return all([result.scheme in (\"http\", \"https\"), result.netloc])\n        except Exception:\n            return False\n\n    @staticmethod\n    def parse_json(obj: Union[dict, str]) -&gt; dict:\n        \"\"\"\n        Attempts to parse the provided object as JSON.\n\n        Args:\n            obj (Union[dict, str]): The object to parse. If a dict, returns it as-is.\n                If a string, attempts to parse it as a JSON string.\n\n        Returns:\n            dict: The parsed JSON object.\n\n        Raises:\n            ValueError: If a string is provided that cannot be parsed as valid JSON.\n            TypeError: If the provided object is neither a dict nor a string.\n\n        Example:\n            ```python\n            BaseReader.try_parse_json('{\"a\": 1}')\n            ```\n            ```python\n            {'a': 1}\n            ```\n            ```python\n            BaseReader.try_parse_json({'b': 2})\n            ```\n            ```python\n            {'b': 2}\n            ```\n            ```python\n            BaseReader.try_parse_json('[not valid json]')\n            ```\n            ```python\n            ValueError: String could not be parsed as JSON: ...\n            ```\n        \"\"\"\n        if isinstance(obj, dict):\n            return obj\n        if isinstance(obj, str):\n            try:\n                return json.loads(obj)\n            except Exception as e:\n                raise ValueError(f\"String could not be parsed as JSON: {e}\")\n        raise TypeError(\"Provided object is not a string or dictionary\")\n\n    @abstractmethod\n    def read(\n        self, file_path: str, model: Optional[BaseVisionModel] = None, **kwargs: Any\n    ) -&gt; ReaderOutput:\n        \"\"\"\n        Reads input and returns a ReaderOutput with text content and standardized metadata.\n\n        Args:\n            file_path (str): Path to the input file, a URL, raw string, or dictionary.\n            model (Optional[BaseVisionModel]): Optional model instance to assist or customize the reading or extraction process. Used for cases where VLMs or specialized parsers are required for processing the file content.\n            **kwargs: Additional keyword arguments for implementation-specific options.\n\n        Returns:\n            ReaderOutput: Dataclass defining the output structure for all readers.\n\n        Raises:\n            ValueError: If the provided string is not valid file path, URL, or parsable content.\n            TypeError: If input type is unsupported.\n\n        Example:\n            ```python\n            class MyReader(BaseReader):\n                def read(self, file_path: str, **kwargs) -&gt; ReaderOutput:\n                    return ReaderOutput(\n                        text=\"example\",\n                        document_name=\"example.txt\",\n                        document_path=file_path,\n                        document_id=kwargs.get(\"document_id\"),\n                        conversion_method=\"custom\",\n                        ocr_method=None,\n                        metadata={}\n                    )\n            ```\n        \"\"\"\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.reader.base_reader.BaseReader.is_url","title":"<code>is_url(string)</code>  <code>staticmethod</code>","text":"<p>Determines whether the given string is a valid HTTP or HTTPS URL.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>The string to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the string is a valid URL with HTTP or HTTPS scheme, False otherwise.</p> Example <p><pre><code>BaseReader.is_url(\"https://example.com\")\n</code></pre> <pre><code>True\n</code></pre> <pre><code>BaseReader.is_url(\"not_a_url\")\n</code></pre> <pre><code>False\n</code></pre></p> Source code in <code>src/splitter_mr/reader/base_reader.py</code> <pre><code>@staticmethod\ndef is_url(string: str) -&gt; bool:\n    \"\"\"\n    Determines whether the given string is a valid HTTP or HTTPS URL.\n\n    Args:\n        string (str): The string to check.\n\n    Returns:\n        bool: True if the string is a valid URL with HTTP or HTTPS scheme, False otherwise.\n\n    Example:\n        ```python\n        BaseReader.is_url(\"https://example.com\")\n        ```\n        ```bash\n        True\n        ```\n        ```python\n        BaseReader.is_url(\"not_a_url\")\n        ```\n        ```bash\n        False\n        ```\n    \"\"\"\n    try:\n        result = urlparse(string)\n        return all([result.scheme in (\"http\", \"https\"), result.netloc])\n    except Exception:\n        return False\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.reader.base_reader.BaseReader.is_valid_file_path","title":"<code>is_valid_file_path(path)</code>  <code>staticmethod</code>","text":"<p>Checks if the provided string is a valid file path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The string to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the string is a valid file path to an existing file, False otherwise.</p> Example <p><pre><code>BaseReader.is_valid_file_path(\"/tmp/myfile.txt\")\n</code></pre> <pre><code>True\n</code></pre></p> Source code in <code>src/splitter_mr/reader/base_reader.py</code> <pre><code>@staticmethod\ndef is_valid_file_path(path: str) -&gt; bool:\n    \"\"\"\n    Checks if the provided string is a valid file path.\n\n    Args:\n        path (str): The string to check.\n\n    Returns:\n        bool: True if the string is a valid file path to an existing file, False otherwise.\n\n    Example:\n        ```python\n        BaseReader.is_valid_file_path(\"/tmp/myfile.txt\")\n        ```\n        ```bash\n        True\n        ```\n    \"\"\"\n    return os.path.isfile(path)\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.reader.base_reader.BaseReader.parse_json","title":"<code>parse_json(obj)</code>  <code>staticmethod</code>","text":"<p>Attempts to parse the provided object as JSON.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[dict, str]</code> <p>The object to parse. If a dict, returns it as-is. If a string, attempts to parse it as a JSON string.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The parsed JSON object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a string is provided that cannot be parsed as valid JSON.</p> <code>TypeError</code> <p>If the provided object is neither a dict nor a string.</p> Example <p><pre><code>BaseReader.try_parse_json('{\"a\": 1}')\n</code></pre> <pre><code>{'a': 1}\n</code></pre> <pre><code>BaseReader.try_parse_json({'b': 2})\n</code></pre> <pre><code>{'b': 2}\n</code></pre> <pre><code>BaseReader.try_parse_json('[not valid json]')\n</code></pre> <pre><code>ValueError: String could not be parsed as JSON: ...\n</code></pre></p> Source code in <code>src/splitter_mr/reader/base_reader.py</code> <pre><code>@staticmethod\ndef parse_json(obj: Union[dict, str]) -&gt; dict:\n    \"\"\"\n    Attempts to parse the provided object as JSON.\n\n    Args:\n        obj (Union[dict, str]): The object to parse. If a dict, returns it as-is.\n            If a string, attempts to parse it as a JSON string.\n\n    Returns:\n        dict: The parsed JSON object.\n\n    Raises:\n        ValueError: If a string is provided that cannot be parsed as valid JSON.\n        TypeError: If the provided object is neither a dict nor a string.\n\n    Example:\n        ```python\n        BaseReader.try_parse_json('{\"a\": 1}')\n        ```\n        ```python\n        {'a': 1}\n        ```\n        ```python\n        BaseReader.try_parse_json({'b': 2})\n        ```\n        ```python\n        {'b': 2}\n        ```\n        ```python\n        BaseReader.try_parse_json('[not valid json]')\n        ```\n        ```python\n        ValueError: String could not be parsed as JSON: ...\n        ```\n    \"\"\"\n    if isinstance(obj, dict):\n        return obj\n    if isinstance(obj, str):\n        try:\n            return json.loads(obj)\n        except Exception as e:\n            raise ValueError(f\"String could not be parsed as JSON: {e}\")\n    raise TypeError(\"Provided object is not a string or dictionary\")\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.reader.base_reader.BaseReader.read","title":"<code>read(file_path, model=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Reads input and returns a ReaderOutput with text content and standardized metadata.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the input file, a URL, raw string, or dictionary.</p> required <code>model</code> <code>Optional[BaseVisionModel]</code> <p>Optional model instance to assist or customize the reading or extraction process. Used for cases where VLMs or specialized parsers are required for processing the file content.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for implementation-specific options.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ReaderOutput</code> <code>ReaderOutput</code> <p>Dataclass defining the output structure for all readers.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided string is not valid file path, URL, or parsable content.</p> <code>TypeError</code> <p>If input type is unsupported.</p> Example <pre><code>class MyReader(BaseReader):\n    def read(self, file_path: str, **kwargs) -&gt; ReaderOutput:\n        return ReaderOutput(\n            text=\"example\",\n            document_name=\"example.txt\",\n            document_path=file_path,\n            document_id=kwargs.get(\"document_id\"),\n            conversion_method=\"custom\",\n            ocr_method=None,\n            metadata={}\n        )\n</code></pre> Source code in <code>src/splitter_mr/reader/base_reader.py</code> <pre><code>@abstractmethod\ndef read(\n    self, file_path: str, model: Optional[BaseVisionModel] = None, **kwargs: Any\n) -&gt; ReaderOutput:\n    \"\"\"\n    Reads input and returns a ReaderOutput with text content and standardized metadata.\n\n    Args:\n        file_path (str): Path to the input file, a URL, raw string, or dictionary.\n        model (Optional[BaseVisionModel]): Optional model instance to assist or customize the reading or extraction process. Used for cases where VLMs or specialized parsers are required for processing the file content.\n        **kwargs: Additional keyword arguments for implementation-specific options.\n\n    Returns:\n        ReaderOutput: Dataclass defining the output structure for all readers.\n\n    Raises:\n        ValueError: If the provided string is not valid file path, URL, or parsable content.\n        TypeError: If input type is unsupported.\n\n    Example:\n        ```python\n        class MyReader(BaseReader):\n            def read(self, file_path: str, **kwargs) -&gt; ReaderOutput:\n                return ReaderOutput(\n                    text=\"example\",\n                    document_name=\"example.txt\",\n                    document_path=file_path,\n                    document_id=kwargs.get(\"document_id\"),\n                    conversion_method=\"custom\",\n                    ocr_method=None,\n                    metadata={}\n                )\n        ```\n    \"\"\"\n</code></pre>"},{"location":"api_reference/reader/#vanillareader","title":"VanillaReader","text":"<p><code>VanillaReader</code> uses a helper class to read PDF and use Visual Language Models. This class is <code>PDFPlumberReader</code>.</p>"},{"location":"api_reference/reader/#splitter_mr.reader.readers.vanilla_reader.SimpleHTMLTextExtractor","title":"<code>SimpleHTMLTextExtractor</code>","text":"<p>               Bases: <code>HTMLParser</code></p> <p>Extract text from HTML by concatenating text nodes (legacy helper).</p> Source code in <code>src/splitter_mr/reader/readers/vanilla_reader.py</code> <pre><code>class SimpleHTMLTextExtractor(HTMLParser):\n    \"\"\"Extract text from HTML by concatenating text nodes (legacy helper).\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.text_parts = []\n\n    def handle_data(self, data):\n        self.text_parts.append(data)\n\n    def get_text(self):\n        return \" \".join(self.text_parts).strip()\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.reader.readers.vanilla_reader.VanillaReader","title":"<code>VanillaReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Read multiple file types using Python's built-in and standard libraries. Supported: .json, .html/.htm, .txt, .xml, .yaml/.yml, .csv, .tsv, .parquet, .pdf</p> <p>NEW: HTML handling (local files and URLs):   - If <code>html_to_markdown=True</code> (kw arg), HTML is converted to Markdown using the     project's HtmlToMarkdown utility, and the conversion method is reported as <code>\"md\"</code>.   - If <code>html_to_markdown=False</code> (default), raw HTML is returned without transformation,     and the conversion method is <code>\"html\"</code>.</p> <p>For PDFs, this reader uses PDFPlumberReader to extract text, tables, and images, with options to show or omit images, and to annotate images using a vision model.</p> Source code in <code>src/splitter_mr/reader/readers/vanilla_reader.py</code> <pre><code>class VanillaReader(BaseReader):\n    \"\"\"\n    Read multiple file types using Python's built-in and standard libraries.\n    Supported: .json, .html/.htm, .txt, .xml, .yaml/.yml, .csv, .tsv, .parquet, .pdf\n\n    **NEW**: HTML handling (local files and URLs):\n      - If ``html_to_markdown=True`` (kw arg), HTML is converted to Markdown using the\n        project's HtmlToMarkdown utility, and the conversion method is reported as ``\"md\"``.\n      - If ``html_to_markdown=False`` (default), raw HTML is returned without transformation,\n        and the conversion method is ``\"html\"``.\n\n    For PDFs, this reader uses PDFPlumberReader to extract text, tables, and images,\n    with options to show or omit images, and to annotate images using a vision model.\n    \"\"\"\n\n    def __init__(self, model: Optional[BaseVisionModel] = None):\n        super().__init__()\n        self.model = model\n        self.pdf_reader = PDFPlumberReader()\n\n    def read(\n        self,\n        file_path: str | Path = None,\n        **kwargs: Any,\n    ) -&gt; ReaderOutput:\n        \"\"\"\n        Read a document from various sources and return standardized output.\n\n        This method supports:\n        - Local file paths (``file_path`` or positional arg)\n        - URLs (``file_url``)\n        - JSON/dict objects (``json_document``)\n        - Raw text strings (``text_document``)\n\n        If multiple sources are provided, the priority is:\n        ``file_path`` &gt; ``file_url`` &gt; ``json_document`` &gt; ``text_document``.\n        If only ``file_path`` is provided, auto-detects whether it is a path, URL,\n        JSON, YAML, or plain text.\n\n        Args:\n            file_path (str | Path): Path to the input file (overridden by\n                ``kwargs['file_path']`` if present).\n\n            **kwargs: Optional arguments that adjust behavior:\n\n                Source selection:\n                    file_path (str): Path to the input file (overrides positional arg).\n                    file_url (str): HTTPS/HTTP URL to read from.\n                    json_document (dict | str): JSON-like document (dict or JSON string).\n                    text_document (str): Raw text content.\n\n                Identification/metadata:\n                    document_id (str): Explicit document id. Defaults to a new UUID.\n                    metadata (dict): Additional metadata to attach to the output.\n\n                HTML handling:\n                    html_to_markdown (bool): If True, convert HTML to Markdown before\n                        returning. If False (default), return raw HTML as-is.\n\n                PDF extraction:\n                    scan_pdf_pages (bool): If True, rasterize and describe pages using a\n                        vision model (VLM). If False (default), use element-wise extraction.\n                    model (BaseVisionModel): Vision-capable model used for scanned PDFs and/or\n                        image captioning (also used for image files).\n                    prompt (str): Prompt for image captioning / page description. Defaults to\n                        ``DEFAULT_IMAGE_CAPTION_PROMPT`` for element-wise PDFs and\n                        ``DEFAULT_IMAGE_EXTRACTION_PROMPT`` for scanned PDFs/images.\n                    resolution (int): DPI when rasterizing pages for VLM. Default: 300.\n                    show_base64_images (bool): Include base64-embedded images in PDF output.\n                        Default: False.\n                    image_placeholder (str): Placeholder for omitted images in PDFs.\n                        Default: ``\"&lt;!-- image --&gt;\"``.\n                    page_placeholder (str): Placeholder inserted between PDF pages (only\n                        surfaced when scanning or when the placeholder occurs in text).\n                        Default: ``\"&lt;!-- page --&gt;\"``.\n                    vlm_parameters (dict): Extra keyword args forwarded to\n                        ``model.analyze_content(...)``.\n\n                Excel / Parquet reading:\n                    as_table (bool): For Excel (``.xlsx``/``.xls``), if True read as a table\n                        using pandas and return CSV text. If False (default), convert to PDF\n                        and run the PDF pipeline.\n                    excel_engine (str): pandas Excel engine. Default: ``\"openpyxl\"``.\n                    parquet_engine (str): pandas Parquet engine (e.g. ``\"pyarrow\"``,\n                        ``\"fastparquet\"``). Default: pandas auto-selection.\n\n        Returns:\n            ReaderOutput: Unified result containing text, metadata, and extraction info.\n\n        Raises:\n            ValueError: If the source is invalid/unsupported, or if a VLM is required\n                but not provided.\n            TypeError: If provided arguments are of unsupported types.\n\n        Notes:\n            - HTML control via ``html_to_markdown`` applies to both local files and URLs.\n            - For `.parquet` files, content is loaded via pandas and returned as CSV-formatted text.\n\n        Example:\n            ```python\n            # Convert HTML to Markdown\n            reader = VanillaReader()\n            md_output = reader.read(file_path=\"page.html\", html_to_markdown=True)\n\n            # Keep raw HTML as-is\n            html_output = reader.read(file_path=\"page.html\", html_to_markdown=False)\n            ```\n        \"\"\"\n\n        source_type, source_val = _guess_source(kwargs, file_path)\n        name, path, text, conv, ocr = self._dispatch_source(\n            source_type, source_val, kwargs\n        )\n\n        page_ph: str = kwargs.get(\"page_placeholder\", \"&lt;!-- page --&gt;\")\n        page_ph_out = self._surface_page_placeholder(\n            scan=bool(kwargs.get(\"scan_pdf_pages\")),\n            placeholder=page_ph,\n            text=text,\n        )\n\n        return ReaderOutput(\n            text=_ensure_str(text),\n            document_name=name,\n            document_path=path or \"\",\n            document_id=kwargs.get(\"document_id\", str(uuid.uuid4())),\n            conversion_method=conv,\n            reader_method=\"vanilla\",\n            ocr_method=ocr,\n            page_placeholder=page_ph_out,\n            metadata=kwargs.get(\"metadata\", {}),\n        )\n\n    def _dispatch_source(  # noqa: WPS231\n        self,\n        src_type: str,\n        src_val: Any,\n        kw: Dict[str, Any],\n    ) -&gt; Tuple[str, Optional[str], Any, str, Optional[str]]:\n        \"\"\"\n        Route the request to a specialised handler and return\n        (document_name, document_path, text/content, conversion_method, ocr_method)\n        \"\"\"\n        handlers = {\n            \"file_path\": self._handle_local_path,\n            \"file_url\": self._handle_url,\n            \"json_document\": self._handle_explicit_json,\n            \"text_document\": self._handle_explicit_text,\n        }\n        if src_type not in handlers:\n            raise ValueError(f\"Unrecognized document source: {src_type}\")\n        return handlers[src_type](src_val, kw)\n\n    # ---- individual strategies below \u2013 each ~20 lines or fewer ---------- #\n\n    # 1) Local / drive paths\n    def _handle_local_path(\n        self,\n        path_like: str | Path,\n        kw: Dict[str, Any],\n    ) -&gt; Tuple[str, str, Any, str, Optional[str]]:\n        \"\"\"Load from the filesystem (or, if it \u2018looks like\u2019 one, via HTTP).\"\"\"\n        path_str = os.fspath(path_like) if isinstance(path_like, Path) else path_like\n        if not isinstance(path_str, str):\n            raise ValueError(\"file_path must be a string or Path object.\")\n\n        if not self.is_valid_file_path(path_str):\n            if self.is_url(path_str):\n                return self._handle_url(path_str, kw)\n            return self._handle_fallback(path_str, kw)\n\n        ext = os.path.splitext(path_str)[1].lower().lstrip(\".\")\n        doc_name = os.path.basename(path_str)\n        rel_path = os.path.relpath(path_str)\n\n        # ---- type-specific branches ---- #\n        # TODO: Refactor to sort the code and make it more readable\n        if ext == \"pdf\":\n            return (\n                doc_name,\n                rel_path,\n                *self._process_pdf(path_str, kw),\n            )\n        if ext == \"html\" or ext == \"htm\":\n            content, conv = _read_html_file(\n                path_str, html_to_markdown=bool(kw.get(\"html_to_markdown\", False))\n            )\n            return doc_name, rel_path, content, conv, None\n        if ext in (\"json\", \"txt\", \"xml\", \"csv\", \"tsv\", \"md\", \"markdown\"):\n            return doc_name, rel_path, _read_text_file(path_str, ext), ext, None\n        if ext == \"parquet\":\n            parquet_engine = kw.get(\n                \"parquet_engine\"\n            )  # e.g., \"pyarrow\" or \"fastparquet\"\n            return (\n                doc_name,\n                rel_path,\n                _read_parquet(path_str, engine=parquet_engine),\n                \"csv\",\n                None,\n            )\n        if ext in (\"yaml\", \"yml\"):\n            return doc_name, rel_path, _read_text_file(path_str, ext), \"json\", None\n        if ext in (\"xlsx\", \"xls\"):\n            # When as_table=True, pass excel_engine\n            if kw.get(\"as_table\", False):\n                excel_engine = kw.get(\"excel_engine\", \"openpyxl\")\n                return (\n                    doc_name,\n                    rel_path,\n                    _read_excel(path_str, engine=excel_engine),\n                    ext,\n                    None,\n                )\n            # Otherwise convert workbook to PDF and reuse the PDF extractor\n            pdf_path = self._convert_office_to_pdf(path_str)\n            return (\n                os.path.basename(pdf_path),\n                os.path.relpath(pdf_path),\n                *self._process_pdf(pdf_path, kw),\n            )\n        if ext in (\"docx\", \"pptx\"):\n            pdf_path = self._convert_office_to_pdf(path_str)\n            return (\n                os.path.basename(pdf_path),\n                os.path.relpath(pdf_path),\n                *self._process_pdf(pdf_path, kw),\n            )\n        if ext in (\"xlsx\", \"xls\"):\n            if kw.get(\"as_table\", False):\n                # direct spreadsheet \u2192 pandas \u2192 CSV\n                return doc_name, rel_path, _read_excel(path_str), ext, None\n            # otherwise convert workbook to PDF and reuse the PDF extractor\n            pdf_path = self._convert_office_to_pdf(path_str)\n            return (\n                os.path.basename(pdf_path),\n                os.path.relpath(pdf_path),\n                *self._process_pdf(pdf_path, kw),\n            )\n        if ext in SUPPORTED_VANILLA_IMAGE_EXTENSIONS:\n            model = kw.get(\"model\", self.model)\n            prompt = kw.get(\"prompt\", DEFAULT_IMAGE_EXTRACTION_PROMPT)\n            vlm_parameters = kw.get(\"vlm_parameters\", {})\n            return self._handle_image_to_llm(\n                model, path_str, prompt=prompt, vlm_parameters=vlm_parameters\n            )\n        if ext in SUPPORTED_PROGRAMMING_LANGUAGES:\n            return doc_name, rel_path, _read_text_file(path_str, ext), \"txt\", None\n\n        raise ValueError(f\"Unsupported file extension: {ext}. Use another Reader.\")\n\n    # 2) Remote URL\n    def _handle_url(\n        self,\n        url: str,\n        kw: Dict[str, Any],\n    ) -&gt; Tuple[str, str, Any, str, Optional[str]]:  # noqa: D401\n        \"\"\"Fetch via HTTP(S).\"\"\"\n        if not isinstance(url, str) or not self.is_url(url):\n            raise ValueError(\"file_url must be a valid URL string.\")\n        content, conv = _load_via_requests(\n            url, html_to_markdown=bool(kw.get(\"html_to_markdown\", False))\n        )\n        name = url.split(\"/\")[-1] or \"downloaded_file\"\n        return name, url, content, conv, None\n\n    # 3) Explicit JSON (dict or str)\n    def _handle_explicit_json(\n        self,\n        json_doc: Any,\n        _kw: Dict[str, Any],\n    ) -&gt; Tuple[str, None, Any, str, None]:\n        \"\"\"JSON passed straight in.\"\"\"\n        return (\n            _kw.get(\"document_name\", None),\n            None,\n            self.parse_json(json_doc),\n            \"json\",\n            None,\n        )\n\n    # 4) Explicit raw text\n    def _handle_explicit_text(\n        self,\n        txt: str,\n        _kw: Dict[str, Any],\n    ) -&gt; Tuple[str, None, Any, str, None]:  # noqa: D401\n        \"\"\"Text (maybe JSON / YAML) passed straight in.\"\"\"\n        for parser, conv in ((self.parse_json, \"json\"), (yaml.safe_load, \"json\")):\n            try:\n                parsed = parser(txt)\n                if isinstance(parsed, (dict, list)):\n                    return _kw.get(\"document_name\", None), None, parsed, conv, None\n            except Exception:  # pragma: no cover\n                pass\n        return _kw.get(\"document_name\", None), None, txt, \"txt\", None\n\n    # ----- shared utilities ------------------------------------------------ #\n\n    def _process_pdf(\n        self,\n        path: str,\n        kw: Dict[str, Any],\n    ) -&gt; Tuple[Any, str, Optional[str]]:\n        \"\"\"\n        Process a PDF file and extract content.\n\n        This method supports two modes:\n        - Scanned PDF pages using a vision-capable model (image-based extraction).\n        - Element-wise text and image extraction using PDFPlumber.\n\n        Args:\n            path (str): The path to the PDF file.\n            kw (dict): Keyword arguments controlling extraction behavior. Recognized keys include:\n                scan_pdf_pages (bool): If True, process the PDF as scanned images.\n                model (BaseVisionModel, optional): Vision-capable model for scanned PDFs or image captioning.\n                prompt (str, optional): Prompt for image captioning.\n                show_base64_images (bool): Whether to include base64 images in the output.\n                image_placeholder (str): Placeholder for omitted images.\n                page_placeholder (str): Placeholder for page breaks.\n\n        Returns:\n            tuple: A tuple of:\n                - content (Any): Extracted text/content from the PDF.\n                - conv (str): Conversion method used (e.g., \"pdf\", \"png\").\n                - ocr_method (str or None): OCR model name if applicable.\n\n        Raises:\n            ValueError: If `scan_pdf_pages` is True but no vision-capable model is provided.\n        \"\"\"\n        if kw.get(\"scan_pdf_pages\"):\n            model = kw.get(\"model\", self.model)\n            if model is None:\n                raise ValueError(\"scan_pdf_pages=True requires a vision-capable model.\")\n            joined = self._scan_pdf_pages(path, model=model, **kw)\n            return joined, \"png\", model.model_name\n        # element-wise extraction\n        content = self.pdf_reader.read(\n            path,\n            model=kw.get(\"model\", self.model),\n            prompt=kw.get(\"prompt\") or DEFAULT_IMAGE_CAPTION_PROMPT,\n            show_base64_images=kw.get(\"show_base64_images\", False),\n            image_placeholder=kw.get(\"image_placeholder\", \"&lt;!-- image --&gt;\"),\n            page_placeholder=kw.get(\"page_placeholder\", \"&lt;!-- page --&gt;\"),\n        )\n        ocr_name = (\n            (kw.get(\"model\") or self.model).model_name\n            if kw.get(\"model\") or self.model\n            else None\n        )\n        return content, \"pdf\", ocr_name\n\n    def _scan_pdf_pages(self, file_path: str, model: BaseVisionModel, **kw) -&gt; str:\n        \"\"\"\n        Describe each page of a PDF using a vision model.\n\n        Args:\n            file_path (str): The path to the PDF file.\n            model (BaseVisionModel): Vision-capable model used for page description.\n            **kw: Additional keyword arguments. Recognized keys include:\n                prompt (str, optional): Prompt for describing PDF pages.\n                resolution (int): DPI resolution for rasterizing pages (default: 300).\n                vlm_parameters (dict): Extra parameters for the vision model.\n\n        Returns:\n            str: A string containing page descriptions separated by page placeholders.\n        \"\"\"\n        page_ph = kw.get(\"page_placeholder\", \"&lt;!-- page --&gt;\")\n        pages = self.pdf_reader.describe_pages(\n            file_path=file_path,\n            model=model,\n            prompt=kw.get(\"prompt\") or DEFAULT_IMAGE_EXTRACTION_PROMPT,\n            resolution=kw.get(\"resolution\", 300),\n            **kw.get(\"vlm_parameters\", {}),\n        )\n        return \"\\n\\n---\\n\\n\".join(f\"{page_ph}\\n\\n{md}\" for md in pages)\n\n    def _handle_fallback(self, raw: str, kw: Dict[str, Any]):\n        \"\"\"\n        Handle unsupported or unknown sources.\n\n        Attempts to parse the input as JSON, then as text.\n        Falls back to returning the raw content as plain text.\n\n        Args:\n            raw (str): Raw string content to be processed.\n            kw (dict): Additional keyword arguments, may include:\n                document_name (str): Optional name of the document.\n\n        Returns:\n            tuple: A tuple of:\n                - document_name (str or None)\n                - document_path (None)\n                - content (Any): Parsed or raw content\n                - conversion_method (str)\n                - ocr_method (None)\n        \"\"\"\n        try:\n            return self._handle_explicit_json(raw, kw)\n        except Exception:\n            try:\n                return self._handle_explicit_text(raw, kw)\n            except Exception:  # pragma: no cover\n                return kw.get(\"document_name\", None), None, raw, \"txt\", None\n\n    def _handle_image_to_llm(\n        self,\n        model: BaseVisionModel,\n        file_path: str,\n        prompt: Optional[str] = None,\n        vlm_parameters: Optional[dict] = None,\n    ) -&gt; Tuple[str, str, Any, str, str]:\n        \"\"\"\n        Extract content from an image file using a vision model.\n\n        Reads the image, encodes it in base64, and sends it to the given vision model\n        with the provided prompt.\n\n        Args:\n            model (BaseVisionModel): Vision-capable model to process the image.\n            file_path (str): Path to the image file.\n            prompt (str, optional): Prompt for guiding the vision model.\n            vlm_parameters (dict, optional): Additional parameters for the vision model.\n\n        Returns:\n            tuple: A tuple of:\n                - document_name (str)\n                - document_path (str)\n                - extracted (Any): Extracted content from the image.\n                - conversion_method (str): Always \"image\".\n                - ocr_method (str): Model name.\n\n        Raises:\n            ValueError: If no vision model is provided.\n        \"\"\"\n        if model is None:\n            raise ValueError(\"No vision model provided for image extraction.\")\n        # Read image as bytes and encode as base64\n        with open(file_path, \"rb\") as f:\n            img_bytes = f.read()\n        ext = os.path.splitext(file_path)[1].lstrip(\".\").lower()\n        img_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n        prompt = prompt or DEFAULT_IMAGE_EXTRACTION_PROMPT\n        vlm_parameters = vlm_parameters or {}\n        extracted = model.analyze_content(\n            img_b64, prompt=prompt, file_ext=ext, **vlm_parameters\n        )\n        doc_name = os.path.basename(file_path)\n        rel_path = os.path.relpath(file_path)\n        return doc_name, rel_path, extracted, \"image\", model.model_name\n\n    @staticmethod\n    def _surface_page_placeholder(\n        scan: bool, placeholder: str, text: Any\n    ) -&gt; Optional[str]:\n        \"\"\"\n        Decide whether to expose the page placeholder in output.\n\n        Never exposes placeholders containing '%'. Returns the placeholder if\n        scanning mode is enabled or if the placeholder is found in the text.\n\n        Args:\n            scan (bool): Whether the document was scanned.\n            placeholder (str): Page placeholder string.\n            text (Any): Extracted text or content.\n\n        Returns:\n            str or None: The placeholder string if it should be exposed, else None.\n        \"\"\"\n        if \"%\" in placeholder:\n            return None\n        txt = _ensure_str(text)\n        return placeholder if (scan or placeholder in txt) else None\n\n    def _convert_office_to_pdf(self, file_path: str) -&gt; str:\n        \"\"\"\n        Convert a DOCX/XLSX/PPTX file to PDF using LibreOffice.\n\n        Args:\n            file_path: Absolute path to the Office document.\n\n        Returns:\n            Path to the generated PDF in a temporary directory.\n\n        Raises:\n            RuntimeError: If LibreOffice (``soffice``) is not in *PATH* or the\n            conversion fails for any reason.\n        \"\"\"\n        if not shutil.which(\"soffice\"):\n            raise RuntimeError(\n                \"LibreOffice/soffice is required for Office-to-PDF conversion \"\n                \"but was not found in PATH.  Install LibreOffice or use a \"\n                \"different reader.\"\n            )\n\n        outdir = tempfile.mkdtemp(prefix=\"vanilla_office2pdf_\")\n        cmd = [\n            \"soffice\",\n            \"--headless\",\n            \"--convert-to\",\n            \"pdf\",\n            \"--outdir\",\n            outdir,\n            file_path,\n        ]\n        proc = subprocess.run(cmd, capture_output=True)\n        if proc.returncode != 0:\n            raise RuntimeError(\n                f\"LibreOffice failed converting {file_path} \u2192 PDF:\\n{proc.stderr.decode()}\"\n            )\n\n        pdf_name = os.path.splitext(os.path.basename(file_path))[0] + \".pdf\"\n        pdf_path = os.path.join(outdir, pdf_name)\n        if not os.path.exists(pdf_path):\n            raise RuntimeError(f\"Expected PDF not found: {pdf_path}\")\n\n        return pdf_path\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.reader.readers.vanilla_reader.VanillaReader.read","title":"<code>read(file_path=None, **kwargs)</code>","text":"<p>Read a document from various sources and return standardized output.</p> <p>This method supports: - Local file paths (<code>file_path</code> or positional arg) - URLs (<code>file_url</code>) - JSON/dict objects (<code>json_document</code>) - Raw text strings (<code>text_document</code>)</p> <p>If multiple sources are provided, the priority is: <code>file_path</code> &gt; <code>file_url</code> &gt; <code>json_document</code> &gt; <code>text_document</code>. If only <code>file_path</code> is provided, auto-detects whether it is a path, URL, JSON, YAML, or plain text.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to the input file (overridden by <code>kwargs['file_path']</code> if present).</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Optional arguments that adjust behavior:</p> <p>Source selection:     file_path (str): Path to the input file (overrides positional arg).     file_url (str): HTTPS/HTTP URL to read from.     json_document (dict | str): JSON-like document (dict or JSON string).     text_document (str): Raw text content.</p> <p>Identification/metadata:     document_id (str): Explicit document id. Defaults to a new UUID.     metadata (dict): Additional metadata to attach to the output.</p> <p>HTML handling:     html_to_markdown (bool): If True, convert HTML to Markdown before         returning. If False (default), return raw HTML as-is.</p> <p>PDF extraction:     scan_pdf_pages (bool): If True, rasterize and describe pages using a         vision model (VLM). If False (default), use element-wise extraction.     model (BaseVisionModel): Vision-capable model used for scanned PDFs and/or         image captioning (also used for image files).     prompt (str): Prompt for image captioning / page description. Defaults to         <code>DEFAULT_IMAGE_CAPTION_PROMPT</code> for element-wise PDFs and         <code>DEFAULT_IMAGE_EXTRACTION_PROMPT</code> for scanned PDFs/images.     resolution (int): DPI when rasterizing pages for VLM. Default: 300.     show_base64_images (bool): Include base64-embedded images in PDF output.         Default: False.     image_placeholder (str): Placeholder for omitted images in PDFs.         Default: <code>\"&lt;!-- image --&gt;\"</code>.     page_placeholder (str): Placeholder inserted between PDF pages (only         surfaced when scanning or when the placeholder occurs in text).         Default: <code>\"&lt;!-- page --&gt;\"</code>.     vlm_parameters (dict): Extra keyword args forwarded to         <code>model.analyze_content(...)</code>.</p> <p>Excel / Parquet reading:     as_table (bool): For Excel (<code>.xlsx</code>/<code>.xls</code>), if True read as a table         using pandas and return CSV text. If False (default), convert to PDF         and run the PDF pipeline.     excel_engine (str): pandas Excel engine. Default: <code>\"openpyxl\"</code>.     parquet_engine (str): pandas Parquet engine (e.g. <code>\"pyarrow\"</code>,         <code>\"fastparquet\"</code>). Default: pandas auto-selection.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ReaderOutput</code> <code>ReaderOutput</code> <p>Unified result containing text, metadata, and extraction info.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source is invalid/unsupported, or if a VLM is required but not provided.</p> <code>TypeError</code> <p>If provided arguments are of unsupported types.</p> Notes <ul> <li>HTML control via <code>html_to_markdown</code> applies to both local files and URLs.</li> <li>For <code>.parquet</code> files, content is loaded via pandas and returned as CSV-formatted text.</li> </ul> Example <pre><code># Convert HTML to Markdown\nreader = VanillaReader()\nmd_output = reader.read(file_path=\"page.html\", html_to_markdown=True)\n\n# Keep raw HTML as-is\nhtml_output = reader.read(file_path=\"page.html\", html_to_markdown=False)\n</code></pre> Source code in <code>src/splitter_mr/reader/readers/vanilla_reader.py</code> <pre><code>def read(\n    self,\n    file_path: str | Path = None,\n    **kwargs: Any,\n) -&gt; ReaderOutput:\n    \"\"\"\n    Read a document from various sources and return standardized output.\n\n    This method supports:\n    - Local file paths (``file_path`` or positional arg)\n    - URLs (``file_url``)\n    - JSON/dict objects (``json_document``)\n    - Raw text strings (``text_document``)\n\n    If multiple sources are provided, the priority is:\n    ``file_path`` &gt; ``file_url`` &gt; ``json_document`` &gt; ``text_document``.\n    If only ``file_path`` is provided, auto-detects whether it is a path, URL,\n    JSON, YAML, or plain text.\n\n    Args:\n        file_path (str | Path): Path to the input file (overridden by\n            ``kwargs['file_path']`` if present).\n\n        **kwargs: Optional arguments that adjust behavior:\n\n            Source selection:\n                file_path (str): Path to the input file (overrides positional arg).\n                file_url (str): HTTPS/HTTP URL to read from.\n                json_document (dict | str): JSON-like document (dict or JSON string).\n                text_document (str): Raw text content.\n\n            Identification/metadata:\n                document_id (str): Explicit document id. Defaults to a new UUID.\n                metadata (dict): Additional metadata to attach to the output.\n\n            HTML handling:\n                html_to_markdown (bool): If True, convert HTML to Markdown before\n                    returning. If False (default), return raw HTML as-is.\n\n            PDF extraction:\n                scan_pdf_pages (bool): If True, rasterize and describe pages using a\n                    vision model (VLM). If False (default), use element-wise extraction.\n                model (BaseVisionModel): Vision-capable model used for scanned PDFs and/or\n                    image captioning (also used for image files).\n                prompt (str): Prompt for image captioning / page description. Defaults to\n                    ``DEFAULT_IMAGE_CAPTION_PROMPT`` for element-wise PDFs and\n                    ``DEFAULT_IMAGE_EXTRACTION_PROMPT`` for scanned PDFs/images.\n                resolution (int): DPI when rasterizing pages for VLM. Default: 300.\n                show_base64_images (bool): Include base64-embedded images in PDF output.\n                    Default: False.\n                image_placeholder (str): Placeholder for omitted images in PDFs.\n                    Default: ``\"&lt;!-- image --&gt;\"``.\n                page_placeholder (str): Placeholder inserted between PDF pages (only\n                    surfaced when scanning or when the placeholder occurs in text).\n                    Default: ``\"&lt;!-- page --&gt;\"``.\n                vlm_parameters (dict): Extra keyword args forwarded to\n                    ``model.analyze_content(...)``.\n\n            Excel / Parquet reading:\n                as_table (bool): For Excel (``.xlsx``/``.xls``), if True read as a table\n                    using pandas and return CSV text. If False (default), convert to PDF\n                    and run the PDF pipeline.\n                excel_engine (str): pandas Excel engine. Default: ``\"openpyxl\"``.\n                parquet_engine (str): pandas Parquet engine (e.g. ``\"pyarrow\"``,\n                    ``\"fastparquet\"``). Default: pandas auto-selection.\n\n    Returns:\n        ReaderOutput: Unified result containing text, metadata, and extraction info.\n\n    Raises:\n        ValueError: If the source is invalid/unsupported, or if a VLM is required\n            but not provided.\n        TypeError: If provided arguments are of unsupported types.\n\n    Notes:\n        - HTML control via ``html_to_markdown`` applies to both local files and URLs.\n        - For `.parquet` files, content is loaded via pandas and returned as CSV-formatted text.\n\n    Example:\n        ```python\n        # Convert HTML to Markdown\n        reader = VanillaReader()\n        md_output = reader.read(file_path=\"page.html\", html_to_markdown=True)\n\n        # Keep raw HTML as-is\n        html_output = reader.read(file_path=\"page.html\", html_to_markdown=False)\n        ```\n    \"\"\"\n\n    source_type, source_val = _guess_source(kwargs, file_path)\n    name, path, text, conv, ocr = self._dispatch_source(\n        source_type, source_val, kwargs\n    )\n\n    page_ph: str = kwargs.get(\"page_placeholder\", \"&lt;!-- page --&gt;\")\n    page_ph_out = self._surface_page_placeholder(\n        scan=bool(kwargs.get(\"scan_pdf_pages\")),\n        placeholder=page_ph,\n        text=text,\n    )\n\n    return ReaderOutput(\n        text=_ensure_str(text),\n        document_name=name,\n        document_path=path or \"\",\n        document_id=kwargs.get(\"document_id\", str(uuid.uuid4())),\n        conversion_method=conv,\n        reader_method=\"vanilla\",\n        ocr_method=ocr,\n        page_placeholder=page_ph_out,\n        metadata=kwargs.get(\"metadata\", {}),\n    )\n</code></pre>"},{"location":"api_reference/reader/#doclingreader","title":"DoclingReader","text":"<p>To execute pipelines, DoclingReader has a utils class, <code>DoclingUtils</code>.</p>"},{"location":"api_reference/reader/#splitter_mr.reader.readers.docling_reader.DoclingReader","title":"<code>DoclingReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>High-level document reader leveraging IBM Docling for flexible document-to-Markdown conversion, with optional image captioning or VLM-based PDF processing. Supports automatic pipeline selection, seamless integration with custom vision-language models, and configurable output for both PDF and non-PDF files.</p> Source code in <code>src/splitter_mr/reader/readers/docling_reader.py</code> <pre><code>class DoclingReader(BaseReader):\n    \"\"\"\n    High-level document reader leveraging IBM Docling for flexible document-to-Markdown conversion,\n    with optional image captioning or VLM-based PDF processing. Supports automatic pipeline selection,\n    seamless integration with custom vision-language models, and configurable output for both PDF\n    and non-PDF files.\n    \"\"\"\n\n    SUPPORTED_EXTENSIONS = SUPPORTED_DOCLING_FILE_EXTENSIONS\n\n    _IMAGE_PATTERN = re.compile(\n        r\"!\\[(?P&lt;alt&gt;[^\\]]*?)\\]\"\n        r\"\\((?P&lt;uri&gt;data:image/[a-zA-Z0-9.+-]+;base64,(?P&lt;b64&gt;[A-Za-z0-9+/=]+))\\)\"\n    )\n\n    def __init__(self, model: Optional[BaseVisionModel] = None) -&gt; None:\n        \"\"\"\n        Initialize a DoclingReader instance.\n\n        Args:\n            model (Optional[BaseVisionModel], optional): An optional vision-language\n                model instance used for PDF pipelines that require image captioning\n                or per-page analysis. If provided, the model\u2019s client and metadata\n                (e.g., Azure deployment settings) are stored for use in downstream\n                processing. Defaults to None.\n        \"\"\"\n        self.model = model\n        self.client = None\n        self.model_name: Optional[str] = None\n        if model:\n            self.client = model.get_client()\n            self.model_name = model.model_name\n\n    def read(\n        self,\n        file_path: str | Path,\n        **kwargs: Any,\n    ) -&gt; ReaderOutput:\n        \"\"\"\n        Reads a document, automatically selecting the appropriate Docling pipeline for extraction.\n        Supports PDFs (per-page VLM or standard extraction), as well as other file types.\n\n        Args:\n            file_path (str | Path): Path or URL to the document file.\n            **kwargs: Keyword arguments to control extraction, including:\n                - prompt (str): Prompt for image captioning or VLM-based PDF extraction.\n                - scan_pdf_pages (bool): If True (and model provided), analyze each PDF page via VLM.\n                - show_base64_images (bool): If True, embed base64 images in Markdown; if False, use\n                    image placeholders.\n                - page_placeholder (str): Placeholder for page breaks in output Markdown.\n                - image_placeholder (str): Placeholder for image locations in output Markdown.\n                - image_resolution (float): Resolution scaling factor for image extraction.\n                - document_id (Optional[str]): Optional document ID for metadata.\n                - metadata (Optional[dict]): Optional metadata dictionary.\n\n        Returns:\n            ReaderOutput: Extracted document in Markdown format and associated metadata.\n\n        Raises:\n            Warning: If a file extension is unsupported, falls back to VanillaReader and emits a warning.\n            ValueError: If PDF pipeline requirements are not satisfied (e.g., neither model nor\n                show_base64_images provided).\n        \"\"\"\n\n        ext: str = os.path.splitext(file_path)[1].lower().lstrip(\".\")\n        if ext not in self.SUPPORTED_EXTENSIONS:\n            msg = f\"Unsupported extension '{ext}'. Using VanillaReader.\"\n            warnings.warn(msg)\n            return VanillaReader().read(file_path=file_path, **kwargs)\n\n        # Pipeline selection and execution\n        pipeline_name, pipeline_args = self._select_pipeline(file_path, ext, **kwargs)\n        md = DoclingPipelineFactory.run(pipeline_name, file_path, **pipeline_args)\n\n        page_placeholder: str = pipeline_args.get(\"page_placeholder\", \"&lt;!-- page --&gt;\")\n        page_placeholder_value = (\n            page_placeholder if page_placeholder and page_placeholder in md else None\n        )\n\n        text = md\n\n        return ReaderOutput(\n            text=text,\n            document_name=os.path.basename(file_path),\n            document_path=file_path,\n            document_id=kwargs.get(\"document_id\", str(uuid.uuid4())),\n            conversion_method=\"markdown\",\n            reader_method=\"docling\",\n            ocr_method=self.model_name,\n            page_placeholder=page_placeholder_value,\n            metadata=kwargs.get(\"metadata\", {}),\n        )\n\n    def _select_pipeline(self, file_path: str, ext: str, **kwargs) -&gt; tuple[str, dict]:\n        \"\"\"\n        Decides which pipeline to use and prepares arguments for it.\n\n        Args:\n            file_path (str): Path to the input document.\n            ext (str): File extension.\n            **kwargs: Extraction and pipeline control options, including:\n                - prompt (str)\n                - scan_pdf_pages (bool)\n                - show_base64_images (bool)\n                - page_placeholder (str)\n                - image_placeholder (str)\n                - image_resolution (float)\n\n        Returns:\n            tuple[str, dict]: Name of the selected pipeline and the dictionary of arguments for that pipeline.\n\n        Pipeline selection logic:\n            - For PDFs:\n                - If scan_pdf_pages is True: uses per-page VLM/image pipeline.\n                - Else if model is provided: uses VLM pipeline.\n                - Else: uses default Markdown pipeline.\n            - For other extensions: always uses Markdown pipeline.\n        \"\"\"\n        # Defaults\n        show_base64_images: bool = kwargs.get(\"show_base64_images\", False)\n        page_placeholder: str = kwargs.get(\"page_placeholder\", \"&lt;!-- page --&gt;\")\n        image_placeholder: str = kwargs.get(\"image_placeholder\", \"&lt;!-- image --&gt;\")\n        image_resolution: float = kwargs.get(\"image_resolution\", 1.0)\n        scan_pdf_pages: bool = kwargs.get(\"scan_pdf_pages\", False)\n\n        # --- PDF logic ---\n        if ext == \"pdf\":\n            if scan_pdf_pages:\n                # Scan pages as images and extract their content\n                pipeline_args = {\n                    \"model\": self.model,\n                    \"prompt\": kwargs.get(\"prompt\", DEFAULT_IMAGE_EXTRACTION_PROMPT),\n                    \"image_resolution\": image_resolution,\n                    \"page_placeholder\": page_placeholder,\n                    \"show_base64_images\": show_base64_images,\n                }\n                pipeline_name = \"page_image\"\n            else:\n                if self.model:\n                    if show_base64_images:\n                        warnings.warn(\n                            \"When using a model, base64 images are not rendered. So, deactivate the `show_base64_images` option or don't provide the model in the class constructor.\"\n                        )\n                    # Use VLM pipeline for the whole PDF\n                    pipeline_args = {\n                        \"model\": self.model,\n                        \"prompt\": kwargs.get(\"prompt\", DEFAULT_IMAGE_CAPTION_PROMPT),\n                        \"page_placeholder\": page_placeholder,\n                        \"image_placeholder\": image_placeholder,\n                    }\n                    pipeline_name = \"vlm\"\n                else:\n                    # No model: use markdown pipeline (default docling, base64 or placeholders)\n                    pipeline_args = {\n                        \"show_base64_images\": show_base64_images,\n                        \"page_placeholder\": page_placeholder,\n                        \"image_placeholder\": image_placeholder,\n                        \"image_resolution\": image_resolution,\n                        \"ext\": ext,\n                    }\n                    pipeline_name = \"markdown\"\n        else:\n            # For non-PDF: use markdown pipeline\n            pipeline_args = {\n                \"show_base64_images\": show_base64_images,\n                \"page_placeholder\": page_placeholder,\n                \"image_placeholder\": image_placeholder,\n                \"ext\": ext,\n            }\n            pipeline_name = \"markdown\"\n\n        return pipeline_name, pipeline_args\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.reader.readers.docling_reader.DoclingReader.__init__","title":"<code>__init__(model=None)</code>","text":"<p>Initialize a DoclingReader instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[BaseVisionModel]</code> <p>An optional vision-language model instance used for PDF pipelines that require image captioning or per-page analysis. If provided, the model\u2019s client and metadata (e.g., Azure deployment settings) are stored for use in downstream processing. Defaults to None.</p> <code>None</code> Source code in <code>src/splitter_mr/reader/readers/docling_reader.py</code> <pre><code>def __init__(self, model: Optional[BaseVisionModel] = None) -&gt; None:\n    \"\"\"\n    Initialize a DoclingReader instance.\n\n    Args:\n        model (Optional[BaseVisionModel], optional): An optional vision-language\n            model instance used for PDF pipelines that require image captioning\n            or per-page analysis. If provided, the model\u2019s client and metadata\n            (e.g., Azure deployment settings) are stored for use in downstream\n            processing. Defaults to None.\n    \"\"\"\n    self.model = model\n    self.client = None\n    self.model_name: Optional[str] = None\n    if model:\n        self.client = model.get_client()\n        self.model_name = model.model_name\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.reader.readers.docling_reader.DoclingReader.read","title":"<code>read(file_path, **kwargs)</code>","text":"<p>Reads a document, automatically selecting the appropriate Docling pipeline for extraction. Supports PDFs (per-page VLM or standard extraction), as well as other file types.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path or URL to the document file.</p> required <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to control extraction, including: - prompt (str): Prompt for image captioning or VLM-based PDF extraction. - scan_pdf_pages (bool): If True (and model provided), analyze each PDF page via VLM. - show_base64_images (bool): If True, embed base64 images in Markdown; if False, use     image placeholders. - page_placeholder (str): Placeholder for page breaks in output Markdown. - image_placeholder (str): Placeholder for image locations in output Markdown. - image_resolution (float): Resolution scaling factor for image extraction. - document_id (Optional[str]): Optional document ID for metadata. - metadata (Optional[dict]): Optional metadata dictionary.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ReaderOutput</code> <code>ReaderOutput</code> <p>Extracted document in Markdown format and associated metadata.</p> <p>Raises:</p> Type Description <code>Warning</code> <p>If a file extension is unsupported, falls back to VanillaReader and emits a warning.</p> <code>ValueError</code> <p>If PDF pipeline requirements are not satisfied (e.g., neither model nor show_base64_images provided).</p> Source code in <code>src/splitter_mr/reader/readers/docling_reader.py</code> <pre><code>def read(\n    self,\n    file_path: str | Path,\n    **kwargs: Any,\n) -&gt; ReaderOutput:\n    \"\"\"\n    Reads a document, automatically selecting the appropriate Docling pipeline for extraction.\n    Supports PDFs (per-page VLM or standard extraction), as well as other file types.\n\n    Args:\n        file_path (str | Path): Path or URL to the document file.\n        **kwargs: Keyword arguments to control extraction, including:\n            - prompt (str): Prompt for image captioning or VLM-based PDF extraction.\n            - scan_pdf_pages (bool): If True (and model provided), analyze each PDF page via VLM.\n            - show_base64_images (bool): If True, embed base64 images in Markdown; if False, use\n                image placeholders.\n            - page_placeholder (str): Placeholder for page breaks in output Markdown.\n            - image_placeholder (str): Placeholder for image locations in output Markdown.\n            - image_resolution (float): Resolution scaling factor for image extraction.\n            - document_id (Optional[str]): Optional document ID for metadata.\n            - metadata (Optional[dict]): Optional metadata dictionary.\n\n    Returns:\n        ReaderOutput: Extracted document in Markdown format and associated metadata.\n\n    Raises:\n        Warning: If a file extension is unsupported, falls back to VanillaReader and emits a warning.\n        ValueError: If PDF pipeline requirements are not satisfied (e.g., neither model nor\n            show_base64_images provided).\n    \"\"\"\n\n    ext: str = os.path.splitext(file_path)[1].lower().lstrip(\".\")\n    if ext not in self.SUPPORTED_EXTENSIONS:\n        msg = f\"Unsupported extension '{ext}'. Using VanillaReader.\"\n        warnings.warn(msg)\n        return VanillaReader().read(file_path=file_path, **kwargs)\n\n    # Pipeline selection and execution\n    pipeline_name, pipeline_args = self._select_pipeline(file_path, ext, **kwargs)\n    md = DoclingPipelineFactory.run(pipeline_name, file_path, **pipeline_args)\n\n    page_placeholder: str = pipeline_args.get(\"page_placeholder\", \"&lt;!-- page --&gt;\")\n    page_placeholder_value = (\n        page_placeholder if page_placeholder and page_placeholder in md else None\n    )\n\n    text = md\n\n    return ReaderOutput(\n        text=text,\n        document_name=os.path.basename(file_path),\n        document_path=file_path,\n        document_id=kwargs.get(\"document_id\", str(uuid.uuid4())),\n        conversion_method=\"markdown\",\n        reader_method=\"docling\",\n        ocr_method=self.model_name,\n        page_placeholder=page_placeholder_value,\n        metadata=kwargs.get(\"metadata\", {}),\n    )\n</code></pre>"},{"location":"api_reference/reader/#markitdownreader","title":"MarkItDownReader","text":""},{"location":"api_reference/reader/#splitter_mr.reader.readers.markitdown_reader.MarkItDownReader","title":"<code>MarkItDownReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Read multiple file types using Microsoft's MarkItDown library, and convert the documents using markdown format.</p> <p>This reader supports both standard MarkItDown conversion and the use of Vision Language Models (VLMs) for LLM-based OCR when extracting text from images or scanned documents.</p> Source code in <code>src/splitter_mr/reader/readers/markitdown_reader.py</code> <pre><code>class MarkItDownReader(BaseReader):\n    \"\"\"\n    Read multiple file types using Microsoft's MarkItDown library, and convert\n    the documents using markdown format.\n\n    This reader supports both standard MarkItDown conversion and the use of Vision Language Models (VLMs)\n    for LLM-based OCR when extracting text from images or scanned documents.\n    \"\"\"\n\n    def __init__(self, model: BaseVisionModel = None) -&gt; None:\n        \"\"\"\n        Initializer method for MarkItDownReader\n\n        Args:\n            model (Optional[BaseVisionModel], optional): An optional vision-language\n                model instance used for PDF pipelines that require image captioning\n                or per-page analysis. If provided, the model\u2019s client and metadata\n                (e.g., Azure deployment settings) are stored for use in downstream\n                processing. Defaults to None.\n        \"\"\"\n        self.model = model\n        self.model_name = model.model_name if self.model else None\n\n    def _convert_to_pdf(self, file_path: str) -&gt; str:\n        \"\"\"\n        Converts DOCX, PPTX, or XLSX to PDF using LibreOffice (headless mode).\n\n        Args:\n            file_path (str): Path to the Office file.\n            ext (str): File extension (lowercase, no dot).\n\n        Returns:\n            str: Path to the converted PDF.\n\n        Raises:\n            RuntimeError: If conversion fails or LibreOffice is not installed.\n        \"\"\"\n        if not shutil.which(\"soffice\"):\n            raise RuntimeError(\n                \"LibreOffice (soffice) is required for Office to PDF conversion but was not found in PATH. \"\n                \"Please install LibreOffice or set split_by_pages=False. \"\n                \"How to install: https://www.libreoffice.org/get-help/install-howto/\"\n            )\n\n        outdir = tempfile.mkdtemp()\n        # Use soffice (LibreOffice) in headless mode\n        cmd = [\n            \"soffice\",\n            \"--headless\",\n            \"--convert-to\",\n            \"pdf\",\n            \"--outdir\",\n            outdir,\n            file_path,\n        ]\n        result = subprocess.run(cmd, capture_output=True)\n        if result.returncode != 0:\n            raise RuntimeError(\n                f\"Failed to convert {file_path} to PDF: {result.stderr.decode()}\"\n            )\n        pdf_name = os.path.splitext(os.path.basename(file_path))[0] + \".pdf\"\n        pdf_path = os.path.join(outdir, pdf_name)\n        if not os.path.exists(pdf_path):\n            raise RuntimeError(f\"PDF was not created: {pdf_path}\")\n        return pdf_path\n\n    def _pdf_pages_to_streams(self, pdf_path: str) -&gt; List[io.BytesIO]:\n        \"\"\"\n        Convert each PDF page to a PNG and wrap in a BytesIO stream.\n\n        Args:\n            pdf_path (str): Path to the PDF file.\n\n        Returns:\n            List[io.BytesIO]: List of PNG image streams for each page.\n        \"\"\"\n        doc = fitz.open(pdf_path)\n        streams = []\n        for idx in range(len(doc)):\n            pix = doc.load_page(idx).get_pixmap()\n            buf = io.BytesIO(pix.tobytes(\"png\"))\n            buf.name = f\"page_{idx + 1}.png\"\n            buf.seek(0)\n            streams.append(buf)\n        return streams\n\n    def _split_pdf_to_temp_pdfs(self, pdf_path: str) -&gt; List[str]:\n        \"\"\"\n        Split a PDF file into single-page temporary PDF files.\n\n        Args:\n            pdf_path (str): Path to the PDF file to split.\n\n        Returns:\n            List[str]: List of file paths for the temporary single-page PDFs.\n\n        Example:\n            temp_files = self._split_pdf_to_temp_pdfs(\"document.pdf\")\n            # temp_files = [\"/tmp/tmpa1b2c3.pdf\", \"/tmp/tmpd4e5f6.pdf\", ...]\n        \"\"\"\n        temp_files = []\n        reader = PdfReader(pdf_path)\n        for i, page in enumerate(reader.pages):\n            writer = PdfWriter()\n            writer.add_page(page)\n            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n                writer.write(tmp)\n                temp_files.append(tmp.name)\n        return temp_files\n\n    def _pdf_pages_to_markdown(\n        self, file_path: str, md: MarkItDown, prompt: str, page_placeholder: str\n    ) -&gt; str:\n        \"\"\"\n        Convert each scanned PDF page to markdown using the provided MarkItDown instance.\n\n        Args:\n            file_path (str): Path to PDF.\n            md (MarkItDown): The MarkItDown converter instance.\n            prompt (str): The LLM prompt for OCR.\n            page_placeholder (str): Page break placeholder for markdown.\n\n        Returns:\n            str: Markdown of the entire PDF (one page per placeholder).\n        \"\"\"\n        page_md = []\n        for idx, page_stream in enumerate(\n            self._pdf_pages_to_streams(file_path), start=1\n        ):\n            page_md.append(page_placeholder.replace(\"{page}\", str(idx)))\n            result = md.convert(page_stream, llm_prompt=prompt)\n            page_md.append(result.text_content)\n        return \"\\n\".join(page_md)\n\n    def _pdf_file_per_page_to_markdown(\n        self, file_path: str, md: \"MarkItDown\", prompt: str, page_placeholder: str\n    ) -&gt; str:\n        \"\"\"\n        Convert each page of a PDF to markdown by splitting the PDF into temporary single-page files,\n        extracting text from each page using MarkItDown, and joining the results with a page placeholder.\n\n        Args:\n            file_path (str): Path to the PDF file.\n            md (MarkItDown): The MarkItDown converter instance.\n            prompt (str): The LLM prompt for extraction.\n            page_placeholder (str): Markdown placeholder for page breaks; supports '{page}' for numbering.\n\n        Returns:\n            str: Concatenated markdown content for the entire PDF, separated by page placeholders.\n\n        Raises:\n            Any exception raised by MarkItDown or file I/O will propagate.\n\n        Example:\n            markdown = self._pdf_file_per_page_to_markdown(\"doc.pdf\", md, prompt, \"&lt;!-- page {page} --&gt;\")\n        \"\"\"\n        temp_files = self._split_pdf_to_temp_pdfs(pdf_path=file_path)\n        page_md = []\n        try:\n            for idx, temp_pdf in enumerate(temp_files, start=1):\n                page_md.append(page_placeholder.replace(\"{page}\", str(idx)))\n                result = md.convert(temp_pdf, llm_prompt=prompt)\n                page_md.append(result.text_content)\n            return \"\\n\".join(page_md)\n        finally:\n            # Clean up temp files\n            for temp_pdf in temp_files:\n                os.remove(temp_pdf)\n\n    def _get_markitdown(self) -&gt; tuple:\n        \"\"\"\n        Returns a MarkItDown instance and OCR method name depending on model presence.\n\n        Returns:\n            tuple[MarkItDown, Optional[str]]: MarkItDown instance, OCR method or None.\n\n        Raises:\n            ValueError: If provided model is not supported.\n        \"\"\"\n        if self.model:\n            self.client = self.model.get_client()\n            if not isinstance(self.client, OpenAI):\n                raise ValueError(\n                    \"Incompatible client. Only models that use the OpenAI client are supported.\"\n                )\n            return (\n                MarkItDown(llm_client=self.client, llm_model=self.model.model_name),\n                self.model.model_name,\n            )\n        else:\n            return MarkItDown(), None\n\n    def read(self, file_path: Path | str = None, **kwargs: Any) -&gt; ReaderOutput:\n        \"\"\"\n        Reads a file and converts its contents to Markdown using MarkItDown.\n\n        Features:\n            - Standard file-to-Markdown conversion for most formats.\n            - LLM-based OCR (if a Vision model is provided) for images and scanned PDFs.\n            - Optional PDF page-wise OCR with fine-grained control and custom LLM prompt.\n\n        Args:\n            file_path (str): Path to the input file to be read and converted.\n            **kwargs:\n                - `document_id (Optional[str])`: Unique document identifier.\n                    If not provided, a UUID will be generated.\n                - `metadata (Dict[str, Any], optional)`: Additional metadata, given in dictionary format.\n                    If not provided, no metadata is returned.\n                - `prompt (Optional[str])`: Prompt for image captioning or VLM extraction.\n                - `page_placeholder (str)`: Markdown placeholder string for pages (default: \"&lt;!-- page --&gt;\").\n                - split_by_pages (bool): If True and the input is a PDF, split the PDF by pages and process\n                    each page separately. Default is False.\n\n        Returns:\n            ReaderOutput: Dataclass defining the output structure for all readers.\n\n        Example:\n            ```python\n            from splitter_mr.model import OpenAIVisionModel\n            from splitter_mr.reader import MarkItDownReader\n\n            model = AzureOpenAIVisionModel()\n            reader = MarkItDownReader(model=model)\n            output = reader.read(file_path=\"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/lorem_ipsum.pdf\")\n            print(output.text)\n            ```\n            ```python\n            Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eget purus non est porta\n            rutrum. Suspendisse euismod lectus laoreet sem pellentesque egestas et et sem.\n            Pellentesque ex felis, cursus ege...\n            ```\n        \"\"\"\n        # Initialize MarkItDown reader\n        file_path: str | Path = os.fspath(file_path)\n        ext: str = os.path.splitext(file_path)[1].lower().lstrip(\".\")\n        prompt: str = kwargs.get(\"prompt\", DEFAULT_IMAGE_EXTRACTION_PROMPT)\n        page_placeholder: str = kwargs.get(\"page_placeholder\", \"&lt;!-- page --&gt;\")\n        split_by_pages: bool = kwargs.get(\"split_by_pages\", False)\n        conversion_method: str = None\n        md, ocr_method = self._get_markitdown()\n\n        PDF_CONVERTIBLE_EXT: Set[str] = {\"docx\", \"pptx\", \"xlsx\"}\n\n        if split_by_pages and ext != \"pdf\":\n            if ext in PDF_CONVERTIBLE_EXT:\n                file_path = self._convert_to_pdf(file_path)\n\n        md, ocr_method = self._get_markitdown()\n\n        # Process text\n        if split_by_pages:\n            markdown_text = self._pdf_file_per_page_to_markdown(\n                file_path=file_path,\n                md=md,\n                prompt=prompt,\n                page_placeholder=page_placeholder,\n            )\n            conversion_method = \"markdown\"\n        elif self.model is not None:\n            markdown_text = self._pdf_pages_to_markdown(\n                file_path=file_path,\n                md=md,\n                prompt=prompt,\n                page_placeholder=page_placeholder,\n            )\n            conversion_method = \"markdown\"\n        else:\n            markdown_text = md.convert(file_path, llm_prompt=prompt).text_content\n            conversion_method = \"json\" if ext == \"json\" else \"markdown\"\n\n        page_placeholder_value = (\n            page_placeholder\n            if page_placeholder and page_placeholder in markdown_text\n            else None\n        )\n\n        # Return output\n        return ReaderOutput(\n            text=markdown_text,\n            document_name=os.path.basename(file_path),\n            document_path=file_path,\n            document_id=kwargs.get(\"document_id\", str(uuid.uuid4())),\n            conversion_method=conversion_method,\n            reader_method=\"markitdown\",\n            ocr_method=ocr_method,\n            page_placeholder=page_placeholder_value,\n            metadata=kwargs.get(\"metadata\", {}),\n        )\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.reader.readers.markitdown_reader.MarkItDownReader.__init__","title":"<code>__init__(model=None)</code>","text":"<p>Initializer method for MarkItDownReader</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[BaseVisionModel]</code> <p>An optional vision-language model instance used for PDF pipelines that require image captioning or per-page analysis. If provided, the model\u2019s client and metadata (e.g., Azure deployment settings) are stored for use in downstream processing. Defaults to None.</p> <code>None</code> Source code in <code>src/splitter_mr/reader/readers/markitdown_reader.py</code> <pre><code>def __init__(self, model: BaseVisionModel = None) -&gt; None:\n    \"\"\"\n    Initializer method for MarkItDownReader\n\n    Args:\n        model (Optional[BaseVisionModel], optional): An optional vision-language\n            model instance used for PDF pipelines that require image captioning\n            or per-page analysis. If provided, the model\u2019s client and metadata\n            (e.g., Azure deployment settings) are stored for use in downstream\n            processing. Defaults to None.\n    \"\"\"\n    self.model = model\n    self.model_name = model.model_name if self.model else None\n</code></pre>"},{"location":"api_reference/reader/#splitter_mr.reader.readers.markitdown_reader.MarkItDownReader.read","title":"<code>read(file_path=None, **kwargs)</code>","text":"<p>Reads a file and converts its contents to Markdown using MarkItDown.</p> Features <ul> <li>Standard file-to-Markdown conversion for most formats.</li> <li>LLM-based OCR (if a Vision model is provided) for images and scanned PDFs.</li> <li>Optional PDF page-wise OCR with fine-grained control and custom LLM prompt.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the input file to be read and converted.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <ul> <li><code>document_id (Optional[str])</code>: Unique document identifier.     If not provided, a UUID will be generated.</li> <li><code>metadata (Dict[str, Any], optional)</code>: Additional metadata, given in dictionary format.     If not provided, no metadata is returned.</li> <li><code>prompt (Optional[str])</code>: Prompt for image captioning or VLM extraction.</li> <li><code>page_placeholder (str)</code>: Markdown placeholder string for pages (default: \"\").</li> <li>split_by_pages (bool): If True and the input is a PDF, split the PDF by pages and process     each page separately. Default is False.</li> </ul> <code>{}</code> <p>Returns:</p> Name Type Description <code>ReaderOutput</code> <code>ReaderOutput</code> <p>Dataclass defining the output structure for all readers.</p> Example <p><pre><code>from splitter_mr.model import OpenAIVisionModel\nfrom splitter_mr.reader import MarkItDownReader\n\nmodel = AzureOpenAIVisionModel()\nreader = MarkItDownReader(model=model)\noutput = reader.read(file_path=\"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/lorem_ipsum.pdf\")\nprint(output.text)\n</code></pre> <pre><code>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eget purus non est porta\nrutrum. Suspendisse euismod lectus laoreet sem pellentesque egestas et et sem.\nPellentesque ex felis, cursus ege...\n</code></pre></p> Source code in <code>src/splitter_mr/reader/readers/markitdown_reader.py</code> <pre><code>def read(self, file_path: Path | str = None, **kwargs: Any) -&gt; ReaderOutput:\n    \"\"\"\n    Reads a file and converts its contents to Markdown using MarkItDown.\n\n    Features:\n        - Standard file-to-Markdown conversion for most formats.\n        - LLM-based OCR (if a Vision model is provided) for images and scanned PDFs.\n        - Optional PDF page-wise OCR with fine-grained control and custom LLM prompt.\n\n    Args:\n        file_path (str): Path to the input file to be read and converted.\n        **kwargs:\n            - `document_id (Optional[str])`: Unique document identifier.\n                If not provided, a UUID will be generated.\n            - `metadata (Dict[str, Any], optional)`: Additional metadata, given in dictionary format.\n                If not provided, no metadata is returned.\n            - `prompt (Optional[str])`: Prompt for image captioning or VLM extraction.\n            - `page_placeholder (str)`: Markdown placeholder string for pages (default: \"&lt;!-- page --&gt;\").\n            - split_by_pages (bool): If True and the input is a PDF, split the PDF by pages and process\n                each page separately. Default is False.\n\n    Returns:\n        ReaderOutput: Dataclass defining the output structure for all readers.\n\n    Example:\n        ```python\n        from splitter_mr.model import OpenAIVisionModel\n        from splitter_mr.reader import MarkItDownReader\n\n        model = AzureOpenAIVisionModel()\n        reader = MarkItDownReader(model=model)\n        output = reader.read(file_path=\"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/lorem_ipsum.pdf\")\n        print(output.text)\n        ```\n        ```python\n        Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eget purus non est porta\n        rutrum. Suspendisse euismod lectus laoreet sem pellentesque egestas et et sem.\n        Pellentesque ex felis, cursus ege...\n        ```\n    \"\"\"\n    # Initialize MarkItDown reader\n    file_path: str | Path = os.fspath(file_path)\n    ext: str = os.path.splitext(file_path)[1].lower().lstrip(\".\")\n    prompt: str = kwargs.get(\"prompt\", DEFAULT_IMAGE_EXTRACTION_PROMPT)\n    page_placeholder: str = kwargs.get(\"page_placeholder\", \"&lt;!-- page --&gt;\")\n    split_by_pages: bool = kwargs.get(\"split_by_pages\", False)\n    conversion_method: str = None\n    md, ocr_method = self._get_markitdown()\n\n    PDF_CONVERTIBLE_EXT: Set[str] = {\"docx\", \"pptx\", \"xlsx\"}\n\n    if split_by_pages and ext != \"pdf\":\n        if ext in PDF_CONVERTIBLE_EXT:\n            file_path = self._convert_to_pdf(file_path)\n\n    md, ocr_method = self._get_markitdown()\n\n    # Process text\n    if split_by_pages:\n        markdown_text = self._pdf_file_per_page_to_markdown(\n            file_path=file_path,\n            md=md,\n            prompt=prompt,\n            page_placeholder=page_placeholder,\n        )\n        conversion_method = \"markdown\"\n    elif self.model is not None:\n        markdown_text = self._pdf_pages_to_markdown(\n            file_path=file_path,\n            md=md,\n            prompt=prompt,\n            page_placeholder=page_placeholder,\n        )\n        conversion_method = \"markdown\"\n    else:\n        markdown_text = md.convert(file_path, llm_prompt=prompt).text_content\n        conversion_method = \"json\" if ext == \"json\" else \"markdown\"\n\n    page_placeholder_value = (\n        page_placeholder\n        if page_placeholder and page_placeholder in markdown_text\n        else None\n    )\n\n    # Return output\n    return ReaderOutput(\n        text=markdown_text,\n        document_name=os.path.basename(file_path),\n        document_path=file_path,\n        document_id=kwargs.get(\"document_id\", str(uuid.uuid4())),\n        conversion_method=conversion_method,\n        reader_method=\"markitdown\",\n        ocr_method=ocr_method,\n        page_placeholder=page_placeholder_value,\n        metadata=kwargs.get(\"metadata\", {}),\n    )\n</code></pre>"},{"location":"api_reference/splitter/","title":"Splitter","text":""},{"location":"api_reference/splitter/#introduction","title":"Introduction","text":"<p>The Splitter component implements the main functionality of this library. This component is designed to deliver classes (inherited from <code>BaseSplitter</code>) which supports to split a markdown text or a string following many different strategies. </p>"},{"location":"api_reference/splitter/#splitter-strategies-description","title":"Splitter strategies description","text":"Splitting Technique Description Character Splitter Splits text into chunks based on a specified number of characters. Supports overlapping by character count or percentage.  Parameters: <code>chunk_size</code> (max chars per chunk), <code>chunk_overlap</code> (overlapping chars: int or %).  Compatible with: Text. Word Splitter Splits text into chunks based on a specified number of words. Supports overlapping by word count or percentage.  Parameters: <code>chunk_size</code> (max words per chunk), <code>chunk_overlap</code> (overlapping words: int or %).  Compatible with: Text. Sentence Splitter Splits text into chunks by a specified number of sentences. Allows overlap defined by a number or percentage of words from the end of the previous chunk. Customizable sentence separators (e.g., <code>.</code>, <code>!</code>, <code>?</code>).  Parameters: <code>chunk_size</code> (max sentences per chunk), <code>chunk_overlap</code> (overlapping words: int or %), <code>sentence_separators</code> (list of characters).  Compatible with: Text. Paragraph Splitter Splits text into chunks based on a specified number of paragraphs. Allows overlapping by word count or percentage, and customizable line breaks.  Parameters: <code>chunk_size</code> (max paragraphs per chunk), <code>chunk_overlap</code> (overlapping words: int or %), <code>line_break</code> (delimiter(s) for paragraphs).  Compatible with: Text. Recursive Splitter Recursively splits text based on a hierarchy of separators (e.g., paragraph, sentence, word, character) until chunks reach a target size. Tries to preserve semantic units as long as possible.  Parameters: <code>chunk_size</code> (max chars per chunk), <code>chunk_overlap</code> (overlapping chars), <code>separators</code> (list of characters to split on, e.g., <code>[\"\\n\\n\", \"\\n\", \" \", \"\"]</code>).  Compatible with: Text. Keyword Splitter Splits text into chunks around matches of specified keywords, using one or more regex patterns. Supports precise boundary control\u2014matched keywords can be included <code>before</code>, <code>after</code>, <code>both</code> sides, or omitted from the split. Each keyword can have a custom name (via <code>dict</code>) for metadata counting. Secondary soft-wrapping by <code>chunk_size</code> is supported.  Parameters: <code>patterns</code> (list of regex patterns, or <code>dict</code> mapping names to patterns), <code>include_delimiters</code> (<code>\"before\"</code>, <code>\"after\"</code>, <code>\"both\"</code>, or <code>\"none\"</code>), <code>flags</code> (regex flags, e.g. <code>re.MULTILINE</code>), <code>chunk_size</code> (max chars per chunk, soft-wrapped).  Compatible with: Text. Token Splitter Splits text into chunks based on the number of tokens, using various tokenization models (e.g., tiktoken, spaCy, NLTK). Useful for ensuring chunks are compatible with LLM context limits.  Parameters: <code>chunk_size</code> (max tokens per chunk), <code>model_name</code> (tokenizer/model, e.g., <code>\"tiktoken/cl100k_base\"</code>, <code>\"spacy/en_core_web_sm\"</code>, <code>\"nltk/punkt\"</code>), <code>language</code> (for NLTK).  Compatible with: Text. Paged Splitter Splits text by pages for documents that have page structure. Each chunk contains a specified number of pages, with optional word overlap.  Parameters: <code>num_pages</code> (pages per chunk), <code>chunk_overlap</code> (overlapping words).  Compatible with: Word, PDF, Excel, PowerPoint. Row/Column Splitter For tabular formats, splits data by a set number of rows or columns per chunk, with possible overlap. Row-based and column-based splitting are mutually exclusive.  Parameters: <code>num_rows</code>, <code>num_cols</code> (rows/columns per chunk), <code>overlap</code> (overlapping rows or columns).  Compatible with: Tabular formats (csv, tsv, parquet, flat json). JSON Splitter Recursively splits JSON documents into smaller sub-structures that preserve the original JSON schema.  Parameters: <code>max_chunk_size</code> (max chars per chunk), <code>min_chunk_size</code> (min chars per chunk).  Compatible with: JSON. Semantic Splitter Splits text into chunks based on semantic similarity, using an embedding model and a max tokens parameter. Useful for meaningful semantic groupings.  Parameters: <code>embedding_model</code> (model for embeddings), <code>max_tokens</code> (max tokens per chunk).  Compatible with: Text. HTML Tag Splitter Splits HTML content based on a specified tag, or automatically detects the most frequent and shallowest tag if not specified. Each chunk is a complete HTML fragment for that tag.  Parameters: <code>chunk_size</code> (max chars per chunk), <code>tag</code> (HTML tag to split on, optional).  Compatible with: HTML. Header Splitter Splits Markdown or HTML documents into chunks using header levels (e.g., <code>#</code>, <code>##</code>, or <code>&lt;h1&gt;</code>, <code>&lt;h2&gt;</code>). Uses configurable headers for chunking.  Parameters: <code>headers_to_split_on</code> (list of headers and semantic names), <code>chunk_size</code> (unused, for compatibility).  Compatible with: Markdown, HTML. Code Splitter Splits source code files into programmatically meaningful chunks (functions, classes, methods, etc.), aware of the syntax of the specified programming language (e.g., Python, Java, Kotlin). Uses language-aware logic to avoid splitting inside code blocks.  Parameters: <code>chunk_size</code> (max chars per chunk), <code>language</code> (programming language as string, e.g., <code>\"python\"</code>, <code>\"java\"</code>).  Compatible with: Source code files (Python, Java, Kotlin, C++, JavaScript, Go, etc.)."},{"location":"api_reference/splitter/#output-format","title":"Output format","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model defining the output structure for all splitters.</p> <p>Attributes:</p> Name Type Description <code>chunks</code> <code>List[str]</code> <p>List of text chunks produced by splitting.</p> <code>chunk_id</code> <code>List[str]</code> <p>List of unique IDs corresponding to each chunk.</p> <code>document_name</code> <code>Optional[str]</code> <p>The name of the document.</p> <code>document_path</code> <code>str</code> <p>The path to the document.</p> <code>document_id</code> <code>Optional[str]</code> <p>A unique identifier for the document.</p> <code>conversion_method</code> <code>Optional[str]</code> <p>The method used for document conversion.</p> <code>reader_method</code> <code>Optional[str]</code> <p>The method used for reading the document.</p> <code>ocr_method</code> <code>Optional[str]</code> <p>The OCR method used, if any.</p> <code>split_method</code> <code>str</code> <p>The method used to split the document.</p> <code>split_params</code> <code>Optional[Dict[str, Any]]</code> <p>Parameters used during the splitting process.</p> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata associated with the splitting.</p> Source code in <code>src/splitter_mr/schema/models.py</code> <pre><code>class SplitterOutput(BaseModel):\n    \"\"\"Pydantic model defining the output structure for all splitters.\n\n    Attributes:\n        chunks: List of text chunks produced by splitting.\n        chunk_id: List of unique IDs corresponding to each chunk.\n        document_name: The name of the document.\n        document_path: The path to the document.\n        document_id: A unique identifier for the document.\n        conversion_method: The method used for document conversion.\n        reader_method: The method used for reading the document.\n        ocr_method: The OCR method used, if any.\n        split_method: The method used to split the document.\n        split_params: Parameters used during the splitting process.\n        metadata: Additional metadata associated with the splitting.\n    \"\"\"\n\n    chunks: List[str] = Field(default_factory=list)\n    chunk_id: List[str] = Field(default_factory=list)\n    document_name: Optional[str] = None\n    document_path: str = \"\"\n    document_id: Optional[str] = None\n    conversion_method: Optional[str] = None\n    reader_method: Optional[str] = None\n    ocr_method: Optional[str] = None\n    split_method: str = \"\"\n    split_params: Optional[Dict[str, Any]] = Field(default_factory=dict)\n    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n\n    @model_validator(mode=\"after\")\n    def validate_and_set_defaults(self):\n        \"\"\"Validates and sets defaults for the SplitterOutput instance.\n\n        Raises:\n            ValueError: If `chunks` is empty or if `chunk_id` length does not match `chunks` length.\n\n        Returns:\n            self (SplitterOutput): The validated and updated instance.\n        \"\"\"\n        if not self.chunks:\n            raise ValueError(\"Chunks list cannot be empty.\")\n\n        if self.chunk_id is not None:\n            if len(self.chunk_id) != len(self.chunks):\n                raise ValueError(\n                    f\"chunk_id length ({len(self.chunk_id)}) does not match chunks length ({len(self.chunks)}).\"\n                )\n        else:\n            self.chunk_id = [str(uuid.uuid4()) for _ in self.chunks]\n\n        if not self.document_id:\n            self.document_id = str(uuid.uuid4())\n\n        return self\n\n    @classmethod\n    def from_chunks(cls, chunks: List[str]) -&gt; \"SplitterOutput\":\n        \"\"\"Create a SplitterOutput from a list of chunks, with all other fields set to their defaults.\n\n        Args:\n            chunks (List[str]): A list of text chunks.\n\n        Returns:\n            SplitterOutput: An instance of SplitterOutput with the given chunks.\n        \"\"\"\n        return cls(chunks=chunks)\n\n    def append_metadata(self, metadata: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Append (update) the metadata dictionary with new key-value pairs.\n\n        Args:\n            metadata (Dict[str, Any]): The metadata to add or update.\n        \"\"\"\n        if self.metadata is None:\n            self.metadata = {}\n        self.metadata.update(metadata)\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.schema.models.SplitterOutput.append_metadata","title":"<code>append_metadata(metadata)</code>","text":"<p>Append (update) the metadata dictionary with new key-value pairs.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Dict[str, Any]</code> <p>The metadata to add or update.</p> required Source code in <code>src/splitter_mr/schema/models.py</code> <pre><code>def append_metadata(self, metadata: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Append (update) the metadata dictionary with new key-value pairs.\n\n    Args:\n        metadata (Dict[str, Any]): The metadata to add or update.\n    \"\"\"\n    if self.metadata is None:\n        self.metadata = {}\n    self.metadata.update(metadata)\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.schema.models.SplitterOutput.from_chunks","title":"<code>from_chunks(chunks)</code>  <code>classmethod</code>","text":"<p>Create a SplitterOutput from a list of chunks, with all other fields set to their defaults.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>List[str]</code> <p>A list of text chunks.</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>An instance of SplitterOutput with the given chunks.</p> Source code in <code>src/splitter_mr/schema/models.py</code> <pre><code>@classmethod\ndef from_chunks(cls, chunks: List[str]) -&gt; \"SplitterOutput\":\n    \"\"\"Create a SplitterOutput from a list of chunks, with all other fields set to their defaults.\n\n    Args:\n        chunks (List[str]): A list of text chunks.\n\n    Returns:\n        SplitterOutput: An instance of SplitterOutput with the given chunks.\n    \"\"\"\n    return cls(chunks=chunks)\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.schema.models.SplitterOutput.validate_and_set_defaults","title":"<code>validate_and_set_defaults()</code>","text":"<p>Validates and sets defaults for the SplitterOutput instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>chunks</code> is empty or if <code>chunk_id</code> length does not match <code>chunks</code> length.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>SplitterOutput</code> <p>The validated and updated instance.</p> Source code in <code>src/splitter_mr/schema/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_and_set_defaults(self):\n    \"\"\"Validates and sets defaults for the SplitterOutput instance.\n\n    Raises:\n        ValueError: If `chunks` is empty or if `chunk_id` length does not match `chunks` length.\n\n    Returns:\n        self (SplitterOutput): The validated and updated instance.\n    \"\"\"\n    if not self.chunks:\n        raise ValueError(\"Chunks list cannot be empty.\")\n\n    if self.chunk_id is not None:\n        if len(self.chunk_id) != len(self.chunks):\n            raise ValueError(\n                f\"chunk_id length ({len(self.chunk_id)}) does not match chunks length ({len(self.chunks)}).\"\n            )\n    else:\n        self.chunk_id = [str(uuid.uuid4()) for _ in self.chunks]\n\n    if not self.document_id:\n        self.document_id = str(uuid.uuid4())\n\n    return self\n</code></pre>"},{"location":"api_reference/splitter/#splitters","title":"Splitters","text":""},{"location":"api_reference/splitter/#basesplitter","title":"BaseSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.base_splitter.BaseSplitter","title":"<code>BaseSplitter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all splitter implementations.</p> <p>This class defines the common interface and utility methods for splitters that divide text or data into smaller chunks, typically for downstream natural language processing tasks or information retrieval. Subclasses should implement the <code>split</code> method, which takes in a dictionary (typically from a document reader) and returns a structured output with the required chunking.</p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>int</code> <p>The maximum number of units (e.g., characters, words, etc.) per chunk.</p> <p>Methods:</p> Name Description <code>split</code> <p>Abstract method. Should be implemented by all subclasses to perform the actual splitting logic.</p> <code>_generate_chunk_ids</code> <p>Generates a list of unique chunk IDs using UUID4, for use in the output.</p> <code>_default_metadata</code> <p>Returns a default (empty) metadata dictionary, which can be extended by subclasses.</p> Source code in <code>src/splitter_mr/splitter/base_splitter.py</code> <pre><code>class BaseSplitter(ABC):\n    \"\"\"\n    Abstract base class for all splitter implementations.\n\n    This class defines the common interface and utility methods for splitters that\n    divide text or data into smaller chunks, typically for downstream natural language\n    processing tasks or information retrieval. Subclasses should implement the `split`\n    method, which takes in a dictionary (typically from a document reader) and returns\n    a structured output with the required chunking.\n\n    Attributes:\n        chunk_size (int): The maximum number of units (e.g., characters, words, etc.) per chunk.\n\n    Methods:\n        split: Abstract method. Should be implemented by all subclasses to perform the actual\n            splitting logic.\n\n        _generate_chunk_ids: Generates a list of unique chunk IDs using UUID4, for use in the output.\n\n        _default_metadata: Returns a default (empty) metadata dictionary, which can be extended by subclasses.\n    \"\"\"\n\n    def __init__(self, chunk_size: int = 1000):\n        \"\"\"\n        Initializer method for BaseSplitter classes\n        \"\"\"\n        self.chunk_size = chunk_size\n\n    @abstractmethod\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Abstract method to split input data into chunks.\n\n        Args:\n            reader_output (ReaderOutput): Input data, typically from a document reader,\n                including the text to split and any relevant metadata.\n\n        Returns:\n            SplitterOutput: A dictionary containing split chunks and associated metadata.\n        \"\"\"\n\n    def _generate_chunk_ids(self, num_chunks: int) -&gt; List[str]:\n        \"\"\"\n        Generate a list of unique chunk identifiers.\n\n        Args:\n            num_chunks (int): Number of chunk IDs to generate.\n\n        Returns:\n            List[str]: List of unique string IDs (UUID4).\n        \"\"\"\n        return [str(uuid.uuid4()) for _ in range(num_chunks)]\n\n    def _default_metadata(self) -&gt; dict:\n        \"\"\"\n        Return a default metadata dictionary.\n\n        Returns:\n            dict: An empty dictionary; subclasses may override to provide additional metadata.\n        \"\"\"\n        return {}\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.base_splitter.BaseSplitter.__init__","title":"<code>__init__(chunk_size=1000)</code>","text":"<p>Initializer method for BaseSplitter classes</p> Source code in <code>src/splitter_mr/splitter/base_splitter.py</code> <pre><code>def __init__(self, chunk_size: int = 1000):\n    \"\"\"\n    Initializer method for BaseSplitter classes\n    \"\"\"\n    self.chunk_size = chunk_size\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.base_splitter.BaseSplitter.split","title":"<code>split(reader_output)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to split input data into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>ReaderOutput</code> <p>Input data, typically from a document reader, including the text to split and any relevant metadata.</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>A dictionary containing split chunks and associated metadata.</p> Source code in <code>src/splitter_mr/splitter/base_splitter.py</code> <pre><code>@abstractmethod\ndef split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Abstract method to split input data into chunks.\n\n    Args:\n        reader_output (ReaderOutput): Input data, typically from a document reader,\n            including the text to split and any relevant metadata.\n\n    Returns:\n        SplitterOutput: A dictionary containing split chunks and associated metadata.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/splitter/#charactersplitter","title":"CharacterSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.character_splitter.CharacterSplitter","title":"<code>CharacterSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>CharacterSplitter splits a given text into overlapping or non-overlapping chunks based on a specified number of characters per chunk.</p> <p>This splitter is configurable with a maximum chunk size (<code>chunk_size</code>) and an overlap between consecutive chunks (<code>chunk_overlap</code>). The overlap can be specified either as an integer (number of characters) or as a float between 0 and 1 (fraction of chunk size). This is particularly useful for downstream NLP tasks where context preservation between chunks is important.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Maximum number of characters per chunk.</p> <code>1000</code> <code>chunk_overlap</code> <code>Union[int, float]</code> <p>Number or percentage of overlapping characters between chunks.</p> <code>0</code> Source code in <code>src/splitter_mr/splitter/splitters/character_splitter.py</code> <pre><code>class CharacterSplitter(BaseSplitter):\n    \"\"\"\n    CharacterSplitter splits a given text into overlapping or non-overlapping chunks\n    based on a specified number of characters per chunk.\n\n    This splitter is configurable with a maximum chunk size (`chunk_size`) and an overlap\n    between consecutive chunks (`chunk_overlap`). The overlap can be specified either as\n    an integer (number of characters) or as a float between 0 and 1 (fraction of chunk size).\n    This is particularly useful for downstream NLP tasks where context preservation between\n    chunks is important.\n\n    Args:\n        chunk_size (int): Maximum number of characters per chunk.\n        chunk_overlap (Union[int, float]): Number or percentage of overlapping characters\n            between chunks.\n    \"\"\"\n\n    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 0):\n        super().__init__(chunk_size)\n        self.chunk_overlap = chunk_overlap\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Splits the input text from the reader_output dictionary into character-based chunks.\n\n        Each chunk contains at most `chunk_size` characters, and adjacent chunks can overlap\n        by a specified number or percentage of characters, according to the `chunk_overlap`\n        parameter set at initialization. Returns a dictionary with the same document metadata,\n        unique chunk identifiers, and the split parameters used.\n\n        Args:\n            reader_output (Dict[str, Any]):\n                Dictionary containing at least a 'text' key (str) and optional document metadata\n                (e.g., 'document_name', 'document_path', etc.).\n\n        Returns:\n            SplitterOutput: Dataclass defining the output structure for all splitters.\n\n        Raises:\n            ValueError: If chunk_overlap is greater than or equal to chunk_size.\n\n        Example:\n            ```python\n            from splitter_mr.splitter import CharacterSplitter\n\n            # This dictionary has been obtained as the output from a Reader object.\n            reader_output = ReaderOutput(\n                text: \"abcdefghijklmnopqrstuvwxyz\",\n                document_name: \"doc.txt\",\n                document_path: \"/path/doc.txt\",\n            )\n            splitter = CharacterSplitter(chunk_size=5, chunk_overlap=2)\n            output = splitter.split(reader_output)\n            print(output[\"chunks\"])\n            ```\n            ```python\n            ['abcde', 'defgh', 'ghijk', ..., 'yz']\n            ```\n        \"\"\"\n        # Initialize variables\n        text = reader_output.text\n        chunk_size = self.chunk_size\n\n        # Determine overlap in characters\n        if isinstance(self.chunk_overlap, float) and 0 &lt;= self.chunk_overlap &lt; 1:\n            overlap = int(chunk_size * self.chunk_overlap)\n        else:\n            overlap = int(self.chunk_overlap)\n        if overlap &gt;= chunk_size:\n            raise ValueError(\"chunk_overlap must be smaller than chunk_size\")\n\n        # Split into chunks\n        chunks = []\n        start = 0\n        while start &lt; len(text):\n            end = start + chunk_size\n            chunks.append(text[start:end])\n            start += chunk_size - overlap if (chunk_size - overlap) &gt; 0 else 1\n\n        # Generate chunk_id and append metadata\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n        metadata = self._default_metadata()\n\n        # Return output\n        output = SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"character_splitter\",\n            split_params={\n                \"chunk_size\": self.chunk_size,\n                \"chunk_overlap\": self.chunk_overlap,\n            },\n            metadata=metadata,\n        )\n        return output\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.character_splitter.CharacterSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Splits the input text from the reader_output dictionary into character-based chunks.</p> <p>Each chunk contains at most <code>chunk_size</code> characters, and adjacent chunks can overlap by a specified number or percentage of characters, according to the <code>chunk_overlap</code> parameter set at initialization. Returns a dictionary with the same document metadata, unique chunk identifiers, and the split parameters used.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>Dict[str, Any]</code> <p>Dictionary containing at least a 'text' key (str) and optional document metadata (e.g., 'document_name', 'document_path', etc.).</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>Dataclass defining the output structure for all splitters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If chunk_overlap is greater than or equal to chunk_size.</p> Example <p><pre><code>from splitter_mr.splitter import CharacterSplitter\n\n# This dictionary has been obtained as the output from a Reader object.\nreader_output = ReaderOutput(\n    text: \"abcdefghijklmnopqrstuvwxyz\",\n    document_name: \"doc.txt\",\n    document_path: \"/path/doc.txt\",\n)\nsplitter = CharacterSplitter(chunk_size=5, chunk_overlap=2)\noutput = splitter.split(reader_output)\nprint(output[\"chunks\"])\n</code></pre> <pre><code>['abcde', 'defgh', 'ghijk', ..., 'yz']\n</code></pre></p> Source code in <code>src/splitter_mr/splitter/splitters/character_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Splits the input text from the reader_output dictionary into character-based chunks.\n\n    Each chunk contains at most `chunk_size` characters, and adjacent chunks can overlap\n    by a specified number or percentage of characters, according to the `chunk_overlap`\n    parameter set at initialization. Returns a dictionary with the same document metadata,\n    unique chunk identifiers, and the split parameters used.\n\n    Args:\n        reader_output (Dict[str, Any]):\n            Dictionary containing at least a 'text' key (str) and optional document metadata\n            (e.g., 'document_name', 'document_path', etc.).\n\n    Returns:\n        SplitterOutput: Dataclass defining the output structure for all splitters.\n\n    Raises:\n        ValueError: If chunk_overlap is greater than or equal to chunk_size.\n\n    Example:\n        ```python\n        from splitter_mr.splitter import CharacterSplitter\n\n        # This dictionary has been obtained as the output from a Reader object.\n        reader_output = ReaderOutput(\n            text: \"abcdefghijklmnopqrstuvwxyz\",\n            document_name: \"doc.txt\",\n            document_path: \"/path/doc.txt\",\n        )\n        splitter = CharacterSplitter(chunk_size=5, chunk_overlap=2)\n        output = splitter.split(reader_output)\n        print(output[\"chunks\"])\n        ```\n        ```python\n        ['abcde', 'defgh', 'ghijk', ..., 'yz']\n        ```\n    \"\"\"\n    # Initialize variables\n    text = reader_output.text\n    chunk_size = self.chunk_size\n\n    # Determine overlap in characters\n    if isinstance(self.chunk_overlap, float) and 0 &lt;= self.chunk_overlap &lt; 1:\n        overlap = int(chunk_size * self.chunk_overlap)\n    else:\n        overlap = int(self.chunk_overlap)\n    if overlap &gt;= chunk_size:\n        raise ValueError(\"chunk_overlap must be smaller than chunk_size\")\n\n    # Split into chunks\n    chunks = []\n    start = 0\n    while start &lt; len(text):\n        end = start + chunk_size\n        chunks.append(text[start:end])\n        start += chunk_size - overlap if (chunk_size - overlap) &gt; 0 else 1\n\n    # Generate chunk_id and append metadata\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n    metadata = self._default_metadata()\n\n    # Return output\n    output = SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"character_splitter\",\n        split_params={\n            \"chunk_size\": self.chunk_size,\n            \"chunk_overlap\": self.chunk_overlap,\n        },\n        metadata=metadata,\n    )\n    return output\n</code></pre>"},{"location":"api_reference/splitter/#wordsplitter","title":"WordSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.word_splitter.WordSplitter","title":"<code>WordSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>WordSplitter splits a given text into overlapping or non-overlapping chunks based on a specified number of words per chunk.</p> <p>This splitter is configurable with a maximum chunk size (<code>chunk_size</code>, in words) and an overlap between consecutive chunks (<code>chunk_overlap</code>). The overlap can be specified either as an integer (number of words) or as a float between 0 and 1 (fraction of chunk size). Useful for NLP tasks where word-based boundaries are important for context preservation.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Maximum number of words per chunk.</p> <code>5</code> <code>chunk_overlap</code> <code>Union[int, float]</code> <p>Number or percentage of overlapping words between chunks.</p> <code>0</code> Source code in <code>src/splitter_mr/splitter/splitters/word_splitter.py</code> <pre><code>class WordSplitter(BaseSplitter):\n    \"\"\"\n    WordSplitter splits a given text into overlapping or non-overlapping chunks\n    based on a specified number of words per chunk.\n\n    This splitter is configurable with a maximum chunk size (`chunk_size`, in words)\n    and an overlap between consecutive chunks (`chunk_overlap`). The overlap can be\n    specified either as an integer (number of words) or as a float between 0 and 1\n    (fraction of chunk size). Useful for NLP tasks where word-based boundaries are\n    important for context preservation.\n\n    Args:\n        chunk_size (int): Maximum number of words per chunk.\n        chunk_overlap (Union[int, float]): Number or percentage of overlapping words between chunks.\n    \"\"\"\n\n    def __init__(self, chunk_size: int = 5, chunk_overlap: Union[int, float] = 0):\n        super().__init__(chunk_size)\n        self.chunk_overlap = chunk_overlap\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Splits the input text from the reader_output dictionary into word-based chunks.\n\n        Each chunk contains at most `chunk_size` words, and adjacent chunks can overlap\n        by a specified number or percentage of words, according to the `chunk_overlap`\n        parameter set at initialization.\n\n        Args:\n            reader_output (Dict[str, Any]):\n                Dictionary containing at least a 'text' key (str) and optional document metadata\n                (e.g., 'document_name', 'document_path', etc.).\n\n        Returns:\n            SplitterOutput: Dataclass defining the output structure for all splitters.\n\n        Raises:\n            ValueError: If chunk_overlap is greater than or equal to chunk_size.\n\n        Example:\n            ```python\n            from splitter_mr.splitter import WordSplitter\n\n            reader_output = ReaderOutput(\n                text: \"The quick brown fox jumps over the lazy dog. Pack my box with five dozen liquor jugs. Sphinx of black quartz, judge my vow.\",\n                document_name: \"pangrams.txt\",\n                document_path: \"/https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/pangrams.txt\",\n            )\n\n            # Split into chunks of 5 words, overlapping by 2 words\n            splitter = WordSplitter(chunk_size=5, chunk_overlap=2)\n            output = splitter.split(reader_output)\n            print(output[\"chunks\"])\n            ```\n            ```python\n            ['The quick brown fox jumps',\n            'fox jumps over the lazy',\n            'over the lazy dog. Pack', ...]\n            ```\n        \"\"\"\n        # Initialize variables\n        text = reader_output.text\n        chunk_size = self.chunk_size\n\n        # Split text into words (using simple whitespace tokenization)\n        words = text.split()\n        total_words = len(words)\n\n        # Determine overlap in characters\n        if isinstance(self.chunk_overlap, float) and 0 &lt;= self.chunk_overlap &lt; 1:\n            overlap = int(chunk_size * self.chunk_overlap)\n        else:\n            overlap = int(self.chunk_overlap)\n        if overlap &gt;= chunk_size:\n            raise ValueError(\"chunk_overlap must be smaller than chunk_size\")\n\n        # Split into chunks\n        chunks = []\n        start = 0\n        step = chunk_size - overlap if (chunk_size - overlap) &gt; 0 else 1\n        while start &lt; total_words:\n            end = start + chunk_size\n            chunk_words = words[start:end]\n            chunks.append(\" \".join(chunk_words))\n            start += step\n\n        # Generate chunk_id and append metadata\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n        metadata = self._default_metadata()\n\n        # Return output\n        output = SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"word_splitter\",\n            split_params={\n                \"chunk_size\": chunk_size,\n                \"chunk_overlap\": self.chunk_overlap,\n            },\n            metadata=metadata,\n        )\n        return output\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.word_splitter.WordSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Splits the input text from the reader_output dictionary into word-based chunks.</p> <p>Each chunk contains at most <code>chunk_size</code> words, and adjacent chunks can overlap by a specified number or percentage of words, according to the <code>chunk_overlap</code> parameter set at initialization.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>Dict[str, Any]</code> <p>Dictionary containing at least a 'text' key (str) and optional document metadata (e.g., 'document_name', 'document_path', etc.).</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>Dataclass defining the output structure for all splitters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If chunk_overlap is greater than or equal to chunk_size.</p> Example <p><pre><code>from splitter_mr.splitter import WordSplitter\n\nreader_output = ReaderOutput(\n    text: \"The quick brown fox jumps over the lazy dog. Pack my box with five dozen liquor jugs. Sphinx of black quartz, judge my vow.\",\n    document_name: \"pangrams.txt\",\n    document_path: \"/https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/pangrams.txt\",\n)\n\n# Split into chunks of 5 words, overlapping by 2 words\nsplitter = WordSplitter(chunk_size=5, chunk_overlap=2)\noutput = splitter.split(reader_output)\nprint(output[\"chunks\"])\n</code></pre> <pre><code>['The quick brown fox jumps',\n'fox jumps over the lazy',\n'over the lazy dog. Pack', ...]\n</code></pre></p> Source code in <code>src/splitter_mr/splitter/splitters/word_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Splits the input text from the reader_output dictionary into word-based chunks.\n\n    Each chunk contains at most `chunk_size` words, and adjacent chunks can overlap\n    by a specified number or percentage of words, according to the `chunk_overlap`\n    parameter set at initialization.\n\n    Args:\n        reader_output (Dict[str, Any]):\n            Dictionary containing at least a 'text' key (str) and optional document metadata\n            (e.g., 'document_name', 'document_path', etc.).\n\n    Returns:\n        SplitterOutput: Dataclass defining the output structure for all splitters.\n\n    Raises:\n        ValueError: If chunk_overlap is greater than or equal to chunk_size.\n\n    Example:\n        ```python\n        from splitter_mr.splitter import WordSplitter\n\n        reader_output = ReaderOutput(\n            text: \"The quick brown fox jumps over the lazy dog. Pack my box with five dozen liquor jugs. Sphinx of black quartz, judge my vow.\",\n            document_name: \"pangrams.txt\",\n            document_path: \"/https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/pangrams.txt\",\n        )\n\n        # Split into chunks of 5 words, overlapping by 2 words\n        splitter = WordSplitter(chunk_size=5, chunk_overlap=2)\n        output = splitter.split(reader_output)\n        print(output[\"chunks\"])\n        ```\n        ```python\n        ['The quick brown fox jumps',\n        'fox jumps over the lazy',\n        'over the lazy dog. Pack', ...]\n        ```\n    \"\"\"\n    # Initialize variables\n    text = reader_output.text\n    chunk_size = self.chunk_size\n\n    # Split text into words (using simple whitespace tokenization)\n    words = text.split()\n    total_words = len(words)\n\n    # Determine overlap in characters\n    if isinstance(self.chunk_overlap, float) and 0 &lt;= self.chunk_overlap &lt; 1:\n        overlap = int(chunk_size * self.chunk_overlap)\n    else:\n        overlap = int(self.chunk_overlap)\n    if overlap &gt;= chunk_size:\n        raise ValueError(\"chunk_overlap must be smaller than chunk_size\")\n\n    # Split into chunks\n    chunks = []\n    start = 0\n    step = chunk_size - overlap if (chunk_size - overlap) &gt; 0 else 1\n    while start &lt; total_words:\n        end = start + chunk_size\n        chunk_words = words[start:end]\n        chunks.append(\" \".join(chunk_words))\n        start += step\n\n    # Generate chunk_id and append metadata\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n    metadata = self._default_metadata()\n\n    # Return output\n    output = SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"word_splitter\",\n        split_params={\n            \"chunk_size\": chunk_size,\n            \"chunk_overlap\": self.chunk_overlap,\n        },\n        metadata=metadata,\n    )\n    return output\n</code></pre>"},{"location":"api_reference/splitter/#sentencesplitter","title":"SentenceSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.sentence_splitter.SentenceSplitter","title":"<code>SentenceSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>SentenceSplitter splits a given text into overlapping or non-overlapping chunks, where each chunk contains a specified number of sentences, and overlap is defined by a number or percentage of words from the end of the previous chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Maximum number of sentences per chunk.</p> <code>5</code> <code>chunk_overlap</code> <code>Union[int, float]</code> <p>Number or percentage of overlapping words between chunks.</p> <code>0</code> <code>separators</code> <code>Union[str, List[str]]</code> <p>Character(s) to split sentences.</p> <code>DEFAULT_SENTENCE_SEPARATORS</code> Source code in <code>src/splitter_mr/splitter/splitters/sentence_splitter.py</code> <pre><code>class SentenceSplitter(BaseSplitter):\n    \"\"\"\n    SentenceSplitter splits a given text into overlapping or non-overlapping chunks,\n    where each chunk contains a specified number of sentences, and overlap is defined\n    by a number or percentage of words from the end of the previous chunk.\n\n    Args:\n        chunk_size (int): Maximum number of sentences per chunk.\n        chunk_overlap (Union[int, float]): Number or percentage of overlapping words between chunks.\n        separators (Union[str, List[str]]): Character(s) to split sentences.\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 5,\n        chunk_overlap: Union[int, float] = 0,\n        separators: Union[str, List[str]] = DEFAULT_SENTENCE_SEPARATORS,\n    ):\n        super().__init__(chunk_size)\n        self.chunk_overlap = chunk_overlap\n\n        if isinstance(separators, list):\n            # Legacy path (NOT recommended): join list with alternation, ensure \"...\" before \".\"\n            parts = sorted({*separators}, key=lambda s: (s != \"...\", s))\n            sep_pattern = \"|\".join(re.escape(s) for s in parts)\n            # Attach trailing quotes/brackets if user insisted on a list\n            self.separators = rf'(?:{sep_pattern})(?:[\"\u201d\u2019\\'\\)\\]\\}}\u00bb]*)\\s*'\n        else:\n            # Recommended path: already a full regex pattern\n            self.separators = separators\n\n        self._sep_re = re.compile(f\"({self.separators})\")\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Splits the input text from the `reader_output` dictionary into sentence-based chunks,\n        allowing for overlap at the word level.\n\n        Each chunk contains at most `chunk_size` sentences, where sentence boundaries are\n        detected using the specified `separators` (e.g., '.', '!', '?').\n        Overlap between consecutive chunks is specified by `chunk_overlap`, which can be an\n        integer (number of words) or a float (fraction of the maximum words in a sentence).\n        This is useful for downstream NLP tasks that require context preservation.\n\n        Args:\n            reader_output (Dict[str, Any]):\n                Dictionary containing at least a 'text' key (str) and optional document metadata,\n                such as 'document_name', 'document_path', 'document_id', etc.\n\n        Returns:\n            SplitterOutput: Dataclass defining the output structure for all splitters.\n\n        Raises:\n            ValueError: If `chunk_overlap` is negative.\n            ValueError: If 'text' is missing in `reader_output`.\n\n        Example:\n            ```python\n            from splitter_mr.splitter import SentenceSplitter\n\n            # Example input: 7 sentences with varied punctuation\n            # This dictionary has been obtained as an output from a Reader class.\n            reader_output = ReaderOutput(\n                text: \"Hello world! How are you? I am fine. Testing sentence splitting. Short. End! And another?\",\n                document_name: \"sample.txt\",\n                document_path: \"/tmp/sample.txt\",\n                document_id: \"123\"\n            )\n\n            # Split into chunks of 3 sentences each, no overlap\n            splitter = SentenceSplitter(chunk_size=3, chunk_overlap=0)\n            result = splitter.split(reader_output)\n            print(result.chunks)\n            ```\n            ```python\n            ['Hello world! How are you? I am fine.',\n             'Testing sentence splitting. Short. End!',\n             'And another?', ...]\n            ```\n        \"\"\"\n        # Initialize variables\n        text = reader_output.text or \"\"\n        chunk_size = self.chunk_size\n\n        # Build sentence list\n        if not text.strip():\n            merged_sentences: List[str] = [\"\"]\n        else:\n            parts = self._sep_re.split(text)  # [text, sep, text, sep, ...]\n            merged_sentences = []\n            i = 0\n            while i &lt; len(parts):\n                segment = (parts[i] or \"\").strip()\n                if i + 1 &lt; len(\n                    parts\n                ):  # we have a separator that belongs to this sentence\n                    sep = parts[i + 1] or \"\"\n                    sentence = (segment + sep).strip()\n                    if sentence:\n                        merged_sentences.append(sentence)\n                    i += 2\n                else:\n                    # tail without terminator\n                    if segment:\n                        merged_sentences.append(segment)\n                    i += 1\n\n            if not merged_sentences:\n                merged_sentences = [\"\"]\n\n        num_sentences = len(merged_sentences)\n\n        # Determine overlap in words\n        if isinstance(self.chunk_overlap, float) and 0 &lt;= self.chunk_overlap &lt; 1:\n            max_sent_words = max((len(s.split()) for s in merged_sentences), default=0)\n            overlap = int(max_sent_words * self.chunk_overlap)\n        else:\n            overlap = int(self.chunk_overlap)\n        if overlap &lt; 0:\n            raise ValueError(\"chunk_overlap must be &gt;= 0\")\n\n        # Build chunks of up to `chunk_size` sentences (single implementation, no duplication)\n        chunks: List[str] = []\n        start = 0\n        while start &lt; num_sentences:\n            end = min(start + chunk_size, num_sentences)\n            chunk_sents = merged_sentences[start:end]\n            chunk_text = \" \".join(chunk_sents)\n\n            if overlap &gt; 0 and chunks:\n                prev_words = chunks[-1].split()\n                overlap_words = (\n                    prev_words[-overlap:] if overlap &lt;= len(prev_words) else prev_words\n                )\n                chunk_text = \" \".join([\" \".join(overlap_words), chunk_text]).strip()\n\n            chunks.append(chunk_text)\n            start += chunk_size\n\n        # Generate chunk_id and append metadata, then return once\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n        metadata = self._default_metadata()\n\n        return SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"sentence_splitter\",\n            split_params={\n                \"chunk_size\": chunk_size,\n                \"chunk_overlap\": self.chunk_overlap,\n                \"separators\": self.separators,\n            },\n            metadata=metadata,\n        )\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.sentence_splitter.SentenceSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Splits the input text from the <code>reader_output</code> dictionary into sentence-based chunks, allowing for overlap at the word level.</p> <p>Each chunk contains at most <code>chunk_size</code> sentences, where sentence boundaries are detected using the specified <code>separators</code> (e.g., '.', '!', '?'). Overlap between consecutive chunks is specified by <code>chunk_overlap</code>, which can be an integer (number of words) or a float (fraction of the maximum words in a sentence). This is useful for downstream NLP tasks that require context preservation.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>Dict[str, Any]</code> <p>Dictionary containing at least a 'text' key (str) and optional document metadata, such as 'document_name', 'document_path', 'document_id', etc.</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>Dataclass defining the output structure for all splitters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>chunk_overlap</code> is negative.</p> <code>ValueError</code> <p>If 'text' is missing in <code>reader_output</code>.</p> Example <p><pre><code>from splitter_mr.splitter import SentenceSplitter\n\n# Example input: 7 sentences with varied punctuation\n# This dictionary has been obtained as an output from a Reader class.\nreader_output = ReaderOutput(\n    text: \"Hello world! How are you? I am fine. Testing sentence splitting. Short. End! And another?\",\n    document_name: \"sample.txt\",\n    document_path: \"/tmp/sample.txt\",\n    document_id: \"123\"\n)\n\n# Split into chunks of 3 sentences each, no overlap\nsplitter = SentenceSplitter(chunk_size=3, chunk_overlap=0)\nresult = splitter.split(reader_output)\nprint(result.chunks)\n</code></pre> <pre><code>['Hello world! How are you? I am fine.',\n 'Testing sentence splitting. Short. End!',\n 'And another?', ...]\n</code></pre></p> Source code in <code>src/splitter_mr/splitter/splitters/sentence_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Splits the input text from the `reader_output` dictionary into sentence-based chunks,\n    allowing for overlap at the word level.\n\n    Each chunk contains at most `chunk_size` sentences, where sentence boundaries are\n    detected using the specified `separators` (e.g., '.', '!', '?').\n    Overlap between consecutive chunks is specified by `chunk_overlap`, which can be an\n    integer (number of words) or a float (fraction of the maximum words in a sentence).\n    This is useful for downstream NLP tasks that require context preservation.\n\n    Args:\n        reader_output (Dict[str, Any]):\n            Dictionary containing at least a 'text' key (str) and optional document metadata,\n            such as 'document_name', 'document_path', 'document_id', etc.\n\n    Returns:\n        SplitterOutput: Dataclass defining the output structure for all splitters.\n\n    Raises:\n        ValueError: If `chunk_overlap` is negative.\n        ValueError: If 'text' is missing in `reader_output`.\n\n    Example:\n        ```python\n        from splitter_mr.splitter import SentenceSplitter\n\n        # Example input: 7 sentences with varied punctuation\n        # This dictionary has been obtained as an output from a Reader class.\n        reader_output = ReaderOutput(\n            text: \"Hello world! How are you? I am fine. Testing sentence splitting. Short. End! And another?\",\n            document_name: \"sample.txt\",\n            document_path: \"/tmp/sample.txt\",\n            document_id: \"123\"\n        )\n\n        # Split into chunks of 3 sentences each, no overlap\n        splitter = SentenceSplitter(chunk_size=3, chunk_overlap=0)\n        result = splitter.split(reader_output)\n        print(result.chunks)\n        ```\n        ```python\n        ['Hello world! How are you? I am fine.',\n         'Testing sentence splitting. Short. End!',\n         'And another?', ...]\n        ```\n    \"\"\"\n    # Initialize variables\n    text = reader_output.text or \"\"\n    chunk_size = self.chunk_size\n\n    # Build sentence list\n    if not text.strip():\n        merged_sentences: List[str] = [\"\"]\n    else:\n        parts = self._sep_re.split(text)  # [text, sep, text, sep, ...]\n        merged_sentences = []\n        i = 0\n        while i &lt; len(parts):\n            segment = (parts[i] or \"\").strip()\n            if i + 1 &lt; len(\n                parts\n            ):  # we have a separator that belongs to this sentence\n                sep = parts[i + 1] or \"\"\n                sentence = (segment + sep).strip()\n                if sentence:\n                    merged_sentences.append(sentence)\n                i += 2\n            else:\n                # tail without terminator\n                if segment:\n                    merged_sentences.append(segment)\n                i += 1\n\n        if not merged_sentences:\n            merged_sentences = [\"\"]\n\n    num_sentences = len(merged_sentences)\n\n    # Determine overlap in words\n    if isinstance(self.chunk_overlap, float) and 0 &lt;= self.chunk_overlap &lt; 1:\n        max_sent_words = max((len(s.split()) for s in merged_sentences), default=0)\n        overlap = int(max_sent_words * self.chunk_overlap)\n    else:\n        overlap = int(self.chunk_overlap)\n    if overlap &lt; 0:\n        raise ValueError(\"chunk_overlap must be &gt;= 0\")\n\n    # Build chunks of up to `chunk_size` sentences (single implementation, no duplication)\n    chunks: List[str] = []\n    start = 0\n    while start &lt; num_sentences:\n        end = min(start + chunk_size, num_sentences)\n        chunk_sents = merged_sentences[start:end]\n        chunk_text = \" \".join(chunk_sents)\n\n        if overlap &gt; 0 and chunks:\n            prev_words = chunks[-1].split()\n            overlap_words = (\n                prev_words[-overlap:] if overlap &lt;= len(prev_words) else prev_words\n            )\n            chunk_text = \" \".join([\" \".join(overlap_words), chunk_text]).strip()\n\n        chunks.append(chunk_text)\n        start += chunk_size\n\n    # Generate chunk_id and append metadata, then return once\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n    metadata = self._default_metadata()\n\n    return SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"sentence_splitter\",\n        split_params={\n            \"chunk_size\": chunk_size,\n            \"chunk_overlap\": self.chunk_overlap,\n            \"separators\": self.separators,\n        },\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api_reference/splitter/#paragraphsplitter","title":"ParagraphSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.paragraph_splitter.ParagraphSplitter","title":"<code>ParagraphSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>ParagraphSplitter splits a given text into overlapping or non-overlapping chunks, where each chunk contains a specified number of paragraphs, and overlap is defined by a number or percentage of words from the end of the previous chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Maximum number of paragraphs per chunk.</p> <code>3</code> <code>chunk_overlap</code> <code>Union[int, float]</code> <p>Number or percentage of overlapping words between chunks.</p> <code>0</code> <code>line_break</code> <code>Union[str, List[str]]</code> <p>Character(s) used to split text into paragraphs.</p> <code>DEFAULT_PARAGRAPH_SEPARATORS</code> Source code in <code>src/splitter_mr/splitter/splitters/paragraph_splitter.py</code> <pre><code>class ParagraphSplitter(BaseSplitter):\n    \"\"\"\n    ParagraphSplitter splits a given text into overlapping or non-overlapping chunks,\n    where each chunk contains a specified number of paragraphs, and overlap is defined\n    by a number or percentage of words from the end of the previous chunk.\n\n    Args:\n        chunk_size (int): Maximum number of paragraphs per chunk.\n        chunk_overlap (Union[int, float]): Number or percentage of overlapping words between chunks.\n        line_break (Union[str, List[str]]): Character(s) used to split text into paragraphs.\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 3,\n        chunk_overlap: Union[int, float] = 0,\n        line_break: Union[str, List[str]] = DEFAULT_PARAGRAPH_SEPARATORS,\n    ):\n        super().__init__(chunk_size)\n        self.chunk_overlap = chunk_overlap\n        self.line_break = line_break if isinstance(line_break, list) else [line_break]\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Splits text in `reader_output['text']` into paragraph-based chunks, with optional word overlap.\n\n        Args:\n            reader_output (Dict[str, Any]): Dictionary containing at least a 'text' key (str)\n                and optional document metadata (e.g., 'document_name', 'document_path').\n\n        Returns:\n            SplitterOutput: Dataclass defining the output structure for all splitters.\n\n        Raises:\n            ValueError: If 'text' is missing from `reader_output` or is not a string.\n\n        Example:\n            ```python\n            from splitter_mr.splitter import ParagraphSplitter\n\n            # This dictionary has been obtained as the output from a Reader object.\n            reader_output = ReaderOutput(\n                text: \"Para 1.\\\\n\\\\nPara 2.\\\\n\\\\nPara 3.\",\n                document_name: \"test.txt\",\n                document_path: \"/tmp/test.txt\"\n            )\n            splitter = ParagraphSplitter(chunk_size=2, chunk_overlap=1, line_break=\"\\\\n\\\\n\")\n            output = splitter.split(reader_output)\n            print(output[\"chunks\"])\n            ```\n            ```python\n            ['Para 1.\\\\n\\\\nPara 2.', '2. Para 3.']\n            ```\n        \"\"\"\n        # Intialize variables\n        text = reader_output.text\n        line_breaks_pattern = \"|\".join(map(re.escape, self.line_break))\n        paragraphs = [p for p in re.split(line_breaks_pattern, text) if p.strip()]\n        num_paragraphs = len(paragraphs)\n\n        # Determine overlap in words\n        if isinstance(self.chunk_overlap, float) and 0 &lt;= self.chunk_overlap &lt; 1:\n            max_para_words = max((len(p.split()) for p in paragraphs), default=0)\n            overlap = int(max_para_words * self.chunk_overlap)\n        else:\n            overlap = int(self.chunk_overlap)\n\n        # Split into chunks\n        chunks = []\n        start = 0\n        while start &lt; num_paragraphs:\n            end = min(start + self.chunk_size, num_paragraphs)\n            chunk_paragraphs = paragraphs[start:end]\n            chunk_text = self.line_break[0].join(chunk_paragraphs)\n            if overlap &gt; 0 and chunks:\n                prev_words = chunks[-1].split()\n                overlap_words = (\n                    prev_words[-overlap:] if overlap &lt;= len(prev_words) else prev_words\n                )\n                chunk_text = (\n                    self.line_break[0]\n                    .join([\" \".join(overlap_words), chunk_text])\n                    .strip()\n                )\n            chunks.append(chunk_text)\n            start += self.chunk_size\n\n        # Generate chunk_id and append metadata\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n        metadata = self._default_metadata()\n\n        # Return output\n        output = SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"paragraph_splitter\",\n            split_params={\n                \"chunk_size\": self.chunk_size,\n                \"chunk_overlap\": self.chunk_overlap,\n                \"line_break\": self.line_break,\n            },\n            metadata=metadata,\n        )\n        return output\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.paragraph_splitter.ParagraphSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Splits text in <code>reader_output['text']</code> into paragraph-based chunks, with optional word overlap.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>Dict[str, Any]</code> <p>Dictionary containing at least a 'text' key (str) and optional document metadata (e.g., 'document_name', 'document_path').</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>Dataclass defining the output structure for all splitters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'text' is missing from <code>reader_output</code> or is not a string.</p> Example <p><pre><code>from splitter_mr.splitter import ParagraphSplitter\n\n# This dictionary has been obtained as the output from a Reader object.\nreader_output = ReaderOutput(\n    text: \"Para 1.\\n\\nPara 2.\\n\\nPara 3.\",\n    document_name: \"test.txt\",\n    document_path: \"/tmp/test.txt\"\n)\nsplitter = ParagraphSplitter(chunk_size=2, chunk_overlap=1, line_break=\"\\n\\n\")\noutput = splitter.split(reader_output)\nprint(output[\"chunks\"])\n</code></pre> <pre><code>['Para 1.\\n\\nPara 2.', '2. Para 3.']\n</code></pre></p> Source code in <code>src/splitter_mr/splitter/splitters/paragraph_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Splits text in `reader_output['text']` into paragraph-based chunks, with optional word overlap.\n\n    Args:\n        reader_output (Dict[str, Any]): Dictionary containing at least a 'text' key (str)\n            and optional document metadata (e.g., 'document_name', 'document_path').\n\n    Returns:\n        SplitterOutput: Dataclass defining the output structure for all splitters.\n\n    Raises:\n        ValueError: If 'text' is missing from `reader_output` or is not a string.\n\n    Example:\n        ```python\n        from splitter_mr.splitter import ParagraphSplitter\n\n        # This dictionary has been obtained as the output from a Reader object.\n        reader_output = ReaderOutput(\n            text: \"Para 1.\\\\n\\\\nPara 2.\\\\n\\\\nPara 3.\",\n            document_name: \"test.txt\",\n            document_path: \"/tmp/test.txt\"\n        )\n        splitter = ParagraphSplitter(chunk_size=2, chunk_overlap=1, line_break=\"\\\\n\\\\n\")\n        output = splitter.split(reader_output)\n        print(output[\"chunks\"])\n        ```\n        ```python\n        ['Para 1.\\\\n\\\\nPara 2.', '2. Para 3.']\n        ```\n    \"\"\"\n    # Intialize variables\n    text = reader_output.text\n    line_breaks_pattern = \"|\".join(map(re.escape, self.line_break))\n    paragraphs = [p for p in re.split(line_breaks_pattern, text) if p.strip()]\n    num_paragraphs = len(paragraphs)\n\n    # Determine overlap in words\n    if isinstance(self.chunk_overlap, float) and 0 &lt;= self.chunk_overlap &lt; 1:\n        max_para_words = max((len(p.split()) for p in paragraphs), default=0)\n        overlap = int(max_para_words * self.chunk_overlap)\n    else:\n        overlap = int(self.chunk_overlap)\n\n    # Split into chunks\n    chunks = []\n    start = 0\n    while start &lt; num_paragraphs:\n        end = min(start + self.chunk_size, num_paragraphs)\n        chunk_paragraphs = paragraphs[start:end]\n        chunk_text = self.line_break[0].join(chunk_paragraphs)\n        if overlap &gt; 0 and chunks:\n            prev_words = chunks[-1].split()\n            overlap_words = (\n                prev_words[-overlap:] if overlap &lt;= len(prev_words) else prev_words\n            )\n            chunk_text = (\n                self.line_break[0]\n                .join([\" \".join(overlap_words), chunk_text])\n                .strip()\n            )\n        chunks.append(chunk_text)\n        start += self.chunk_size\n\n    # Generate chunk_id and append metadata\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n    metadata = self._default_metadata()\n\n    # Return output\n    output = SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"paragraph_splitter\",\n        split_params={\n            \"chunk_size\": self.chunk_size,\n            \"chunk_overlap\": self.chunk_overlap,\n            \"line_break\": self.line_break,\n        },\n        metadata=metadata,\n    )\n    return output\n</code></pre>"},{"location":"api_reference/splitter/#recursivecharactersplitter","title":"RecursiveCharacterSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.recursive_splitter.RecursiveCharacterSplitter","title":"<code>RecursiveCharacterSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>RecursiveCharacterSplitter splits a given text into overlapping or non-overlapping chunks, where each chunk is created repeatedly breaking down the text until it reaches the desired chunk size. This class implements the Langchain RecursiveCharacterTextSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Approximate chunk size, in characters.</p> <code>1000</code> <code>chunk_overlap</code> <code>Union[int, float]</code> <p>Number or percentage of overlapping characters between chunks.</p> <code>0.1</code> <code>separators</code> <code>Union[str, List[str]]</code> <p>Character(s) to recursively split sentences.</p> <code>DEFAULT_RECURSIVE_SEPARATORS</code> Notes <p>More info about the RecursiveCharacterTextSplitter: Langchain Docs.</p> Source code in <code>src/splitter_mr/splitter/splitters/recursive_splitter.py</code> <pre><code>class RecursiveCharacterSplitter(BaseSplitter):\n    \"\"\"\n    RecursiveCharacterSplitter splits a given text into overlapping or non-overlapping chunks,\n    where each chunk is created repeatedly breaking down the text until it reaches the\n    desired chunk size. This class implements the Langchain RecursiveCharacterTextSplitter.\n\n    Args:\n        chunk_size (int): Approximate chunk size, in characters.\n        chunk_overlap (Union[int, float]): Number or percentage of overlapping characters between\n            chunks.\n        separators (Union[str, List[str]]): Character(s) to recursively split sentences.\n\n    Notes:\n        More info about the RecursiveCharacterTextSplitter:\n        [Langchain Docs](https://python.langchain.com/docs/how_to/recursive_text_splitter/).\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 1000,\n        chunk_overlap: Union[int, float] = 0.1,\n        separators: Union[str, List[str]] = DEFAULT_RECURSIVE_SEPARATORS,\n    ):\n        super().__init__(chunk_size)\n        self.chunk_overlap = chunk_overlap\n        self.separators = separators if isinstance(separators, list) else [separators]\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Splits the input text into character-based chunks using a recursive splitting strategy\n        (via Langchain's `RecursiveCharacterTextSplitter`), supporting configurable separators,\n        chunk size, and overlap.\n\n        Args:\n            reader_output (Dict[str, Any]): Dictionary containing at least a 'text' key (str)\n                and optional document metadata (e.g., 'document_name', 'document_path', etc.).\n\n        Returns:\n            SplitterOutput: Dataclass defining the output structure for all splitters.\n\n        Raises:\n            ValueError: If 'text' is missing in `reader_output` or is not a string.\n\n        Example:\n            ```python\n            from splitter_mr.splitter import RecursiveCharacterSplitter\n\n            # This dictionary has been obtained as the output from a Reader object.\n            reader_output = ReaderOutput(\n                text: \"This is a long document.\n                It will be recursively split into smaller chunks using the specified separators.\n                Each chunk will have some overlap with the next.\",\n                document_name: \"sample.txt\",\n                document_path: \"/tmp/sample.txt\"\n            )\n\n            splitter = RecursiveCharacterSplitter(chunk_size=40, chunk_overlap=5)\n            output = splitter.split(reader_output)\n            print(output[\"chunks\"])\n            ```\n            ```python\n            ['This is a long document. It will be', 'be recursively split into smaller chunks', ...]\n            ```\n        \"\"\"\n        # Initialize variables\n        text = reader_output.text\n        chunk_size = self.chunk_size\n\n        # Determine overlap in characters\n        if isinstance(self.chunk_overlap, float) and 0 &lt;= self.chunk_overlap &lt; 1:\n            overlap = int(chunk_size * self.chunk_overlap)\n        else:\n            overlap = int(self.chunk_overlap)\n        if overlap &gt;= chunk_size:\n            raise ValueError(\"chunk_overlap must be smaller than chunk_size\")\n\n        # Split text into sentences\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=self.separators,\n        )\n        texts = splitter.create_documents([text])\n        chunks = [doc.page_content for doc in texts]\n\n        # Generate chunk_id and append metadata\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n        metadata = self._default_metadata()\n\n        # Return output\n        output = SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"recursive_character_splitter\",\n            split_params={\n                \"chunk_size\": chunk_size,\n                \"chunk_overlap\": self.chunk_overlap,\n                \"separators\": self.separators,\n            },\n            metadata=metadata,\n        )\n        return output\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.recursive_splitter.RecursiveCharacterSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Splits the input text into character-based chunks using a recursive splitting strategy (via Langchain's <code>RecursiveCharacterTextSplitter</code>), supporting configurable separators, chunk size, and overlap.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>Dict[str, Any]</code> <p>Dictionary containing at least a 'text' key (str) and optional document metadata (e.g., 'document_name', 'document_path', etc.).</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>Dataclass defining the output structure for all splitters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'text' is missing in <code>reader_output</code> or is not a string.</p> Example <p><pre><code>from splitter_mr.splitter import RecursiveCharacterSplitter\n\n# This dictionary has been obtained as the output from a Reader object.\nreader_output = ReaderOutput(\n    text: \"This is a long document.\n    It will be recursively split into smaller chunks using the specified separators.\n    Each chunk will have some overlap with the next.\",\n    document_name: \"sample.txt\",\n    document_path: \"/tmp/sample.txt\"\n)\n\nsplitter = RecursiveCharacterSplitter(chunk_size=40, chunk_overlap=5)\noutput = splitter.split(reader_output)\nprint(output[\"chunks\"])\n</code></pre> <pre><code>['This is a long document. It will be', 'be recursively split into smaller chunks', ...]\n</code></pre></p> Source code in <code>src/splitter_mr/splitter/splitters/recursive_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Splits the input text into character-based chunks using a recursive splitting strategy\n    (via Langchain's `RecursiveCharacterTextSplitter`), supporting configurable separators,\n    chunk size, and overlap.\n\n    Args:\n        reader_output (Dict[str, Any]): Dictionary containing at least a 'text' key (str)\n            and optional document metadata (e.g., 'document_name', 'document_path', etc.).\n\n    Returns:\n        SplitterOutput: Dataclass defining the output structure for all splitters.\n\n    Raises:\n        ValueError: If 'text' is missing in `reader_output` or is not a string.\n\n    Example:\n        ```python\n        from splitter_mr.splitter import RecursiveCharacterSplitter\n\n        # This dictionary has been obtained as the output from a Reader object.\n        reader_output = ReaderOutput(\n            text: \"This is a long document.\n            It will be recursively split into smaller chunks using the specified separators.\n            Each chunk will have some overlap with the next.\",\n            document_name: \"sample.txt\",\n            document_path: \"/tmp/sample.txt\"\n        )\n\n        splitter = RecursiveCharacterSplitter(chunk_size=40, chunk_overlap=5)\n        output = splitter.split(reader_output)\n        print(output[\"chunks\"])\n        ```\n        ```python\n        ['This is a long document. It will be', 'be recursively split into smaller chunks', ...]\n        ```\n    \"\"\"\n    # Initialize variables\n    text = reader_output.text\n    chunk_size = self.chunk_size\n\n    # Determine overlap in characters\n    if isinstance(self.chunk_overlap, float) and 0 &lt;= self.chunk_overlap &lt; 1:\n        overlap = int(chunk_size * self.chunk_overlap)\n    else:\n        overlap = int(self.chunk_overlap)\n    if overlap &gt;= chunk_size:\n        raise ValueError(\"chunk_overlap must be smaller than chunk_size\")\n\n    # Split text into sentences\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=self.chunk_size,\n        chunk_overlap=self.chunk_overlap,\n        separators=self.separators,\n    )\n    texts = splitter.create_documents([text])\n    chunks = [doc.page_content for doc in texts]\n\n    # Generate chunk_id and append metadata\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n    metadata = self._default_metadata()\n\n    # Return output\n    output = SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"recursive_character_splitter\",\n        split_params={\n            \"chunk_size\": chunk_size,\n            \"chunk_overlap\": self.chunk_overlap,\n            \"separators\": self.separators,\n        },\n        metadata=metadata,\n    )\n    return output\n</code></pre>"},{"location":"api_reference/splitter/#keywordsplitter","title":"KeywordSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.keyword_splitter.KeywordSplitter","title":"<code>KeywordSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Splitter that chunks text around keyword boundaries using regular expressions.</p> <p>This splitter searches the input text for one or more keyword patterns (regex) and creates chunks at each match boundary. You can control how the matched delimiter is attached to the resulting chunks (before/after/both/none) and apply a secondary, size-based re-chunking to respect <code>chunk_size</code>.</p> <p>The splitter emits a :class:<code>~..schema.SplitterOutput</code> with metadata including per-keyword match counts and raw match spans.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Union[List[str], Dict[str, str]]</code> <p>A list of regex pattern strings or a mapping of <code>name -&gt; regex pattern</code>. When a dict is provided, the keys are used in the metadata counts. When a list is provided, synthetic names are generated (<code>k0</code>, <code>k1</code>, ...).</p> required <code>flags</code> <code>int</code> <p>Standard <code>re</code> flags combined with <code>|</code> (e.g., <code>re.IGNORECASE</code>).</p> <code>0</code> <code>include_delimiters</code> <code>str</code> <p>Where to attach the matched keyword delimiter. One of <code>\"none\"</code>, <code>\"before\"</code>, <code>\"after\"</code>, <code>\"both\"</code>. - <code>before</code> (default) appends the match to the preceding chunk. - <code>after</code> prepends the match to the following chunk. - <code>both</code> duplicates the match on both sides. - <code>none</code> omits the delimiter from both sides.</p> <code>'before'</code> <code>chunk_size</code> <code>int</code> <p>Target maximum size (in characters) for each chunk. When a produced chunk exceeds this value, it is soft-wrapped by whitespace using a greedy strategy.</p> <code>100000</code> Notes <ul> <li>All regexes are compiled into one alternation with named groups when   <code>patterns</code> is a dict. This simplifies per-keyword accounting.</li> <li>If the input text is empty or no matches are found, the entire text   becomes a single chunk (subject to size-based re-chunking).</li> </ul> Source code in <code>src/splitter_mr/splitter/splitters/keyword_splitter.py</code> <pre><code>class KeywordSplitter(BaseSplitter):\n    \"\"\"\n    Splitter that chunks text around *keyword* boundaries using regular expressions.\n\n    This splitter searches the input text for one or more *keyword patterns* (regex)\n    and creates chunks at each match boundary. You can control how the matched\n    delimiter is attached to the resulting chunks (before/after/both/none) and apply a\n    secondary, size-based re-chunking to respect ``chunk_size``.\n\n    The splitter emits a :class:`~..schema.SplitterOutput` with metadata including\n    per-keyword match counts and raw match spans.\n\n    Args:\n        patterns (Union[List[str], Dict[str, str]]): A list of regex pattern strings **or** a mapping of\n            ``name -&gt; regex pattern``. When a dict is provided, the keys are used in\n            the metadata counts. When a list is provided, synthetic names are\n            generated (``k0``, ``k1``, ...).\n        flags (int): Standard ``re`` flags combined with ``|`` (e.g., ``re.IGNORECASE``).\n        include_delimiters (str): Where to attach the matched keyword delimiter.\n            One of ``\"none\"``, ``\"before\"``, ``\"after\"``, ``\"both\"``.\n            - ``before`` (default) appends the match to the *preceding* chunk.\n            - ``after`` prepends the match to the *following* chunk.\n            - ``both`` duplicates the match on both sides.\n            - ``none`` omits the delimiter from both sides.\n        chunk_size (int): Target maximum size (in characters) for each chunk. When a\n            produced chunk exceeds this value, it is *soft*-wrapped by whitespace\n            using a greedy strategy.\n\n    Notes:\n        - All regexes are compiled into **one** alternation with *named groups* when\n          ``patterns`` is a dict. This simplifies per-keyword accounting.\n        - If the input text is empty or no matches are found, the entire text\n          becomes a single chunk (subject to size-based re-chunking).\n    \"\"\"\n\n    def __init__(\n        self,\n        patterns: Union[List[str], Dict[str, str]],\n        *,\n        flags: int = 0,\n        include_delimiters: str = \"before\",\n        chunk_size: int = 100000,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the KeywordSplitter.\n\n        Args:\n            patterns (Union[List[str], Dict[str, str]]): Keyword regex patterns.\n            flags (int): Regex flags.\n            include_delimiters (str): How to include delimiters (before, after, both, none).\n            chunk_size (int): Max chunk size in characters.\n        \"\"\"\n        super().__init__(chunk_size=chunk_size)\n        self.include_delimiters = self._validate_include_delimiters(include_delimiters)\n        self.pattern_names, self.compiled = self._compile_patterns(patterns, flags)\n        self.flags = flags\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Split ReaderOutput into keyword-delimited chunks and build structured output.\n\n        Args:\n            reader_output (ReaderOutput): Input document and metadata.\n\n        Returns:\n            SplitterOutput: Output structure with chunked text and metadata.\n        \"\"\"\n        text = reader_output.text or \"\"\n\n        # Ensure document_id is present so it propagates (fixes metadata test)\n        if not reader_output.document_id:\n            reader_output.document_id = str(uuid.uuid4())\n\n        # Primary split by keyword matches (names used for counts)\n        raw_chunks, match_spans, match_names = self._split_by_keywords(text)\n\n        # Secondary size-based re-chunking to respect chunk_size\n        sized_chunks: List[str] = []\n        for ch in raw_chunks:\n            sized_chunks.extend(self._soft_wrap(ch, self.chunk_size))\n        if not sized_chunks:\n            sized_chunks = [\"\"]\n\n        # Generate IDs\n        chunk_ids = self._generate_chunk_ids(len(sized_chunks))\n\n        # Build metadata (ensure counts/spans are always present)\n        matches_meta = {\n            \"counts\": self._count_by_name(match_names),\n            \"spans\": match_spans,\n            \"include_delimiters\": self.include_delimiters,\n            \"flags\": self.flags,\n            \"pattern_names\": self.pattern_names,\n            \"chunk_size\": self.chunk_size,\n        }\n\n        return self._build_output(\n            reader_output=reader_output,\n            chunks=sized_chunks,\n            chunk_ids=chunk_ids,\n            matches_meta=matches_meta,\n        )\n\n    # ---- Internals ------------------------------------------------------ #\n\n    @staticmethod\n    def _validate_include_delimiters(value: str) -&gt; str:\n        \"\"\"\n        Validate and normalize include_delimiters argument.\n\n        Args:\n            value (str): One of {\"none\", \"before\", \"after\", \"both\"}.\n\n        Returns:\n            str: Normalized delimiter mode.\n\n        Raises:\n            ValueError: If the mode is invalid.\n        \"\"\"\n        allowed = {\"none\", \"before\", \"after\", \"both\"}\n        v = value.lower().strip()\n        if v not in allowed:\n            raise ValueError(\n                f\"include_delimiters must be one of {sorted(allowed)}, got {value!r}\"\n            )\n        return v\n\n    @staticmethod\n    def _compile_patterns(\n        patterns: Union[List[str], Dict[str, str]], flags: int\n    ) -&gt; Tuple[List[str], Pattern[str]]:\n        \"\"\"\n        Compile patterns into a single alternation regex.\n\n        If a dict is given, build a pattern with **named** groups to preserve the\n        provided names. If a list is given, synthesize names (k0, k1, ...).\n\n        Args:\n            patterns (Union[List[str], Dict[str, str]]): Patterns or mapping.\n            flags (int): Regex flags.\n\n        Returns:\n            Tuple[List[str], Pattern[str]]: Names and compiled regex.\n        \"\"\"\n        if isinstance(patterns, dict):\n            names = list(patterns.keys())\n            parts = [f\"(?P&lt;{name}&gt;{pat})\" for name, pat in patterns.items()]\n        else:\n            names = [f\"k{i}\" for i in range(len(patterns))]\n            parts = [f\"(?P&lt;{n}&gt;{pat})\" for n, pat in zip(names, patterns)]\n\n        combined = \"|\".join(parts) if parts else r\"(?!x)x\"  # never matches if empty\n        compiled = re.compile(combined, flags)\n        return names, compiled\n\n    def _split_by_keywords(\n        self, text: str\n    ) -&gt; Tuple[List[str], List[Tuple[int, int]], List[str]]:\n        \"\"\"\n        Split ``text`` around matches of ``self.compiled``.\n\n        Respects include_delimiters in {\"before\", \"after\", \"both\", \"none\"}.\n\n        Args:\n            text (str): The text to split.\n\n        Returns:\n            Tuple[List[str], List[Tuple[int, int]], List[str]]:\n                (chunks, spans, names) where `chunks` are before size re-wrapping,\n                spans are (start, end) tuples, and names are group names for each match.\n        \"\"\"\n\n        def _append_chunk(acc: List[str], chunk: str) -&gt; None:\n            # Keep only non-empty (after strip) chunks here; final fallback to [\"\"] is done by caller\n            if chunk and chunk.strip():\n                acc.append(chunk)\n\n        chunks: List[str] = []\n        spans: List[Tuple[int, int]] = []\n        names: List[str] = []\n\n        matches = list(self.compiled.finditer(text))\n        last_idx = 0\n        pending_prefix = \"\"  # used when include_delimiters is \"after\" or \"both\"\n\n        for m in matches:\n            start, end = m.span()\n            match_txt = text[start:end]\n            group_name = m.lastgroup or \"unknown\"\n\n            spans.append((start, end))\n            names.append(group_name)\n\n            # Build the piece between last match end and this match start, prefixing any pending delimiter\n            before_piece = pending_prefix + text[last_idx:start]\n            pending_prefix = \"\"\n\n            # Attach delimiter to the left side if requested\n            if self.include_delimiters in (\"before\", \"both\"):\n                before_piece += match_txt\n\n            _append_chunk(chunks, before_piece)\n\n            # If delimiter should be on the right, carry it forward to prefix next chunk\n            if self.include_delimiters in (\"after\", \"both\"):\n                pending_prefix = match_txt\n\n            last_idx = end\n\n        # Remainder after the last match (may contain pending_prefix)\n        remainder = pending_prefix + text[last_idx:]\n        _append_chunk(chunks, remainder)\n\n        # If no non-empty chunks were appended, return a single empty chunk (tests expect this)\n        if not chunks:\n            return [\"\"], spans, names\n\n        # normalize whitespace trimming for each chunk\n        chunks = [c.strip() for c in chunks if c and c.strip()]\n\n        if not chunks:\n            return [\"\"], spans, names\n\n        return chunks, spans, names\n\n    @staticmethod\n    def _soft_wrap(text: str, max_size: int) -&gt; List[str]:\n        \"\"\"\n        Greedy soft-wrap by whitespace to respect ``max_size``.\n\n        - If ``len(text) &lt;= max_size``: return ``[text]``.\n        - Else: split on whitespace and rebuild lines greedily.\n        - If a single token is longer than ``max_size``, it is hard-split.\n\n        Args:\n            text (str): Text to wrap.\n            max_size (int): Maximum chunk size.\n\n        Returns:\n            List[str]: List of size-constrained chunks.\n        \"\"\"\n        if max_size &lt;= 0 or len(text) &lt;= max_size:\n            return [text] if text else []\n\n        tokens = re.findall(r\"\\S+|\\s+\", text)\n        out: List[str] = []\n        buf = \"\"\n        for tok in tokens:\n            if len(buf) + len(tok) &lt;= max_size:\n                buf += tok\n                continue\n            if buf:\n                out.append(buf)\n                buf = \"\"\n            # token alone is too big -&gt; hard split\n            while len(tok) &gt; max_size:\n                out.append(tok[:max_size])\n                tok = tok[max_size:]\n            buf = tok\n        if buf:\n            out.append(buf)\n        return [c for c in (s.strip() for s in out) if c]\n\n    @staticmethod\n    def _count_by_name(names: Iterable[str]) -&gt; Dict[str, int]:\n        \"\"\"\n        Aggregate match counts by group name (k0/k1/... for list patterns, custom names for dict).\n\n        Args:\n            names (Iterable[str]): Group names.\n\n        Returns:\n            Dict[str, int]: Count of matches per group name.\n        \"\"\"\n        counts: Dict[str, int] = {}\n        for n in names:\n            counts[n] = counts.get(n, 0) + 1\n        return counts\n\n    def _build_output(\n        self,\n        reader_output: ReaderOutput,\n        chunks: List[str],\n        chunk_ids: List[str],\n        matches_meta: Dict[str, object],\n    ) -&gt; SplitterOutput:\n        \"\"\"\n        Assemble a :class:`SplitterOutput` carrying over reader metadata.\n\n        Args:\n            reader_output (ReaderOutput): Input document and metadata.\n            chunks (List[str]): Final list of chunks.\n            chunk_ids (List[str]): Unique chunk IDs.\n            matches_meta (Dict[str, object]): Keyword matches metadata.\n\n        Returns:\n            SplitterOutput: Populated output object.\n        \"\"\"\n        return SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"keyword\",\n            split_params={\n                \"include_delimiters\": self.include_delimiters,\n                \"flags\": self.flags,\n                \"chunk_size\": self.chunk_size,\n                \"pattern_names\": self.pattern_names,\n            },\n            metadata={\n                **(reader_output.metadata or {}),\n                \"keyword_matches\": matches_meta,\n            },\n        )\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.keyword_splitter.KeywordSplitter.__init__","title":"<code>__init__(patterns, *, flags=0, include_delimiters='before', chunk_size=100000)</code>","text":"<p>Initialize the KeywordSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Union[List[str], Dict[str, str]]</code> <p>Keyword regex patterns.</p> required <code>flags</code> <code>int</code> <p>Regex flags.</p> <code>0</code> <code>include_delimiters</code> <code>str</code> <p>How to include delimiters (before, after, both, none).</p> <code>'before'</code> <code>chunk_size</code> <code>int</code> <p>Max chunk size in characters.</p> <code>100000</code> Source code in <code>src/splitter_mr/splitter/splitters/keyword_splitter.py</code> <pre><code>def __init__(\n    self,\n    patterns: Union[List[str], Dict[str, str]],\n    *,\n    flags: int = 0,\n    include_delimiters: str = \"before\",\n    chunk_size: int = 100000,\n) -&gt; None:\n    \"\"\"\n    Initialize the KeywordSplitter.\n\n    Args:\n        patterns (Union[List[str], Dict[str, str]]): Keyword regex patterns.\n        flags (int): Regex flags.\n        include_delimiters (str): How to include delimiters (before, after, both, none).\n        chunk_size (int): Max chunk size in characters.\n    \"\"\"\n    super().__init__(chunk_size=chunk_size)\n    self.include_delimiters = self._validate_include_delimiters(include_delimiters)\n    self.pattern_names, self.compiled = self._compile_patterns(patterns, flags)\n    self.flags = flags\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.keyword_splitter.KeywordSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Split ReaderOutput into keyword-delimited chunks and build structured output.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>ReaderOutput</code> <p>Input document and metadata.</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>Output structure with chunked text and metadata.</p> Source code in <code>src/splitter_mr/splitter/splitters/keyword_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Split ReaderOutput into keyword-delimited chunks and build structured output.\n\n    Args:\n        reader_output (ReaderOutput): Input document and metadata.\n\n    Returns:\n        SplitterOutput: Output structure with chunked text and metadata.\n    \"\"\"\n    text = reader_output.text or \"\"\n\n    # Ensure document_id is present so it propagates (fixes metadata test)\n    if not reader_output.document_id:\n        reader_output.document_id = str(uuid.uuid4())\n\n    # Primary split by keyword matches (names used for counts)\n    raw_chunks, match_spans, match_names = self._split_by_keywords(text)\n\n    # Secondary size-based re-chunking to respect chunk_size\n    sized_chunks: List[str] = []\n    for ch in raw_chunks:\n        sized_chunks.extend(self._soft_wrap(ch, self.chunk_size))\n    if not sized_chunks:\n        sized_chunks = [\"\"]\n\n    # Generate IDs\n    chunk_ids = self._generate_chunk_ids(len(sized_chunks))\n\n    # Build metadata (ensure counts/spans are always present)\n    matches_meta = {\n        \"counts\": self._count_by_name(match_names),\n        \"spans\": match_spans,\n        \"include_delimiters\": self.include_delimiters,\n        \"flags\": self.flags,\n        \"pattern_names\": self.pattern_names,\n        \"chunk_size\": self.chunk_size,\n    }\n\n    return self._build_output(\n        reader_output=reader_output,\n        chunks=sized_chunks,\n        chunk_ids=chunk_ids,\n        matches_meta=matches_meta,\n    )\n</code></pre>"},{"location":"api_reference/splitter/#headersplitter","title":"HeaderSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.header_splitter.HeaderSplitter","title":"<code>HeaderSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Split HTML or Markdown documents into chunks by header levels (H1\u2013H6).</p> <ul> <li>If the input looks like HTML, it is first converted to Markdown using the   project's HtmlToMarkdown utility, which emits ATX-style headings (<code>#</code>, <code>##</code>, ...).</li> <li>If the input is Markdown, Setext-style headings (underlines with <code>===</code> / <code>---</code>)   are normalized to ATX so headers are reliably detected.</li> <li>Splitting is performed with LangChain's MarkdownHeaderTextSplitter.</li> <li>If no headers are detected after conversion/normalization, a safe fallback   splitter (RecursiveCharacterTextSplitter) is used to avoid returning a single,   excessively large chunk.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Size hint for fallback splitting; not used by header splitting itself. Defaults to 1000.</p> <code>1000</code> <code>headers_to_split_on</code> <code>Optional[List[str]]</code> <p>Semantic header names like [\"Header 1\", \"Header 2\"]. If None, all levels 1\u20136 are enabled.</p> <code>None</code> <code>group_header_with_content</code> <code>bool</code> <p>If True (default), headers are kept with their following content (strip_headers=False). If False, headers are stripped from chunks (strip_headers=True).</p> <code>True</code> Example <pre><code>from splitter_mr.splitter import HeaderSplitter\n\nsplitter = HeaderSplitter(headers_to_split_on=[\"Header 1\", \"Header 2\", \"Header 3\"])\noutput = splitter.split(reader_output)  # reader_output.text may be HTML or MD\nfor idx, chunk in enumerate(output.chunks):\n    print(f\"--- Chunk {idx+1} ---\")\n    print(chunk)\n</code></pre> Source code in <code>src/splitter_mr/splitter/splitters/header_splitter.py</code> <pre><code>class HeaderSplitter(BaseSplitter):\n    \"\"\"\n    Split HTML or Markdown documents into chunks by header levels (H1\u2013H6).\n\n    - If the input looks like HTML, it is first converted to Markdown using the\n      project's HtmlToMarkdown utility, which emits ATX-style headings (`#`, `##`, ...).\n    - If the input is Markdown, Setext-style headings (underlines with `===` / `---`)\n      are normalized to ATX so headers are reliably detected.\n    - Splitting is performed with LangChain's MarkdownHeaderTextSplitter.\n    - If no headers are detected after conversion/normalization, a safe fallback\n      splitter (RecursiveCharacterTextSplitter) is used to avoid returning a single,\n      excessively large chunk.\n\n    Args:\n        chunk_size (int, optional): Size hint for fallback splitting; not used by\n            header splitting itself. Defaults to 1000.\n        headers_to_split_on (Optional[List[str]]): Semantic header names like\n            [\"Header 1\", \"Header 2\"]. If None, all levels 1\u20136 are enabled.\n        group_header_with_content (bool, optional): If True (default), headers are\n            kept with their following content (strip_headers=False). If False,\n            headers are stripped from chunks (strip_headers=True).\n\n    Example:\n        ```python\n        from splitter_mr.splitter import HeaderSplitter\n\n        splitter = HeaderSplitter(headers_to_split_on=[\"Header 1\", \"Header 2\", \"Header 3\"])\n        output = splitter.split(reader_output)  # reader_output.text may be HTML or MD\n        for idx, chunk in enumerate(output.chunks):\n            print(f\"--- Chunk {idx+1} ---\")\n            print(chunk)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 1000,\n        headers_to_split_on: Optional[List[str]] = None,\n        *,\n        group_header_with_content: bool = True,\n    ):\n        \"\"\"\n        Initialize the HeaderSplitter.\n\n        Args:\n            chunk_size (int): Used by fallback character splitter if no headers are found.\n            headers_to_split_on (Optional[List[str]]): Semantic headers, e.g. [\"Header 1\", \"Header 2\"].\n                Defaults to all levels 1\u20136.\n            group_header_with_content (bool): Keep headers attached to following content if True.\n        \"\"\"\n        super().__init__(chunk_size)\n        # Default to all 6 levels for robust splitting unless caller narrows it.\n        self.headers_to_split_on = headers_to_split_on or [\n            f\"Header {i}\" for i in range(1, 7)\n        ]\n        self.group_header_with_content = bool(group_header_with_content)\n\n    def _make_tuples(self, filetype: str) -&gt; List[Tuple[str, str]]:\n        \"\"\"\n        Convert semantic header names (e.g., \"Header 2\") into Markdown tokens.\n\n        Args:\n            filetype (str): Only \"md\" is supported (HTML is converted to MD first).\n\n        Returns:\n            List[Tuple[str, str]]: Tuples of (header_token, semantic_name), e.g. (\"##\", \"Header 2\").\n        \"\"\"\n        tuples: List[Tuple[str, str]] = []\n        for header in self.headers_to_split_on:\n            lvl = self._header_level(header)\n            if filetype == \"md\":\n                tuples.append((\"#\" * lvl, header))\n            else:\n                raise ValueError(f\"Unsupported filetype: {filetype!r}\")\n        return tuples\n\n    @staticmethod\n    def _header_level(header: str) -&gt; int:\n        \"\"\"\n        Extract numeric level from a header name like \"Header 2\".\n\n        Raises:\n            ValueError: If the header string is not of the expected form.\n        \"\"\"\n        m = re.match(r\"header\\s*(\\d+)\", header.lower())\n        if not m:\n            raise ValueError(f\"Invalid header: {header}\")\n        return int(m.group(1))\n\n    @staticmethod\n    def _guess_filetype(reader_output: ReaderOutput) -&gt; str:\n        \"\"\"\n        Heuristically determine whether the input is HTML or Markdown.\n\n        Checks filename extensions first, then looks for HTML elements as a hint.\n        \"\"\"\n        name = (reader_output.document_name or \"\").lower()\n        if name.endswith((\".html\", \".htm\")):\n            return \"html\"\n        if name.endswith((\".md\", \".markdown\")):\n            return \"md\"\n\n        soup = BeautifulSoup(reader_output.text or \"\", \"html.parser\")\n        if soup.find(\"html\") or soup.find(re.compile(r\"^h[1-6]$\")) or soup.find(\"div\"):\n            return \"html\"\n        return \"md\"\n\n    @staticmethod\n    def _normalize_setext(md_text: str) -&gt; str:\n        \"\"\"\n        Normalize Setext-style headings to ATX so MarkdownHeaderTextSplitter can detect them.\n\n        H1:  Title\\\\n====  \u2192  # Title\n        H2:  Title\\\\n----  \u2192  ## Title\n        \"\"\"\n        # H1 underlines\n        md_text = re.sub(r\"^(?P&lt;t&gt;[^\\n]+)\\n=+\\s*$\", r\"# \\g&lt;t&gt;\", md_text, flags=re.M)\n        # H2 underlines\n        md_text = re.sub(r\"^(?P&lt;t&gt;[^\\n]+)\\n-+\\s*$\", r\"## \\g&lt;t&gt;\", md_text, flags=re.M)\n        return md_text\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Perform header-based splitting with HTML\u2192Markdown conversion and safe fallback.\n\n        Steps:\n          1) Detect filetype (HTML/MD).\n          2) If HTML, convert to Markdown with HtmlToMarkdown (emits ATX headings).\n          3) If Markdown, normalize Setext headings to ATX.\n          4) Split by headers via MarkdownHeaderTextSplitter.\n          5) If no headers found, fallback to RecursiveCharacterTextSplitter.\n        \"\"\"\n        if not reader_output.text:\n            raise ValueError(\"reader_output.text is empty or None\")\n\n        filetype = self._guess_filetype(reader_output)\n        tuples = self._make_tuples(\"md\")  # Always work in Markdown space.\n\n        text = reader_output.text\n\n        # HTML \u2192 Markdown using the project's converter\n        if filetype == \"html\":\n            text = HtmlToMarkdown().convert(text)\n        else:\n            # Normalize Setext headings if already Markdown\n            text = self._normalize_setext(text)\n\n        # Detect presence of ATX headers (after conversion/normalization)\n        has_headers = bool(re.search(r\"(?m)^\\s*#{1,6}\\s+\\S\", text))\n\n        # Configure header splitter. group_header_with_content -&gt; strip_headers False\n        splitter = MarkdownHeaderTextSplitter(\n            headers_to_split_on=tuples,\n            return_each_line=False,\n            strip_headers=not self.group_header_with_content,\n        )\n\n        docs = splitter.split_text(text) if has_headers else []\n        # Fallback if no headers were found\n        if not docs:\n            rc = RecursiveCharacterTextSplitter(\n                chunk_size=max(1, int(self.chunk_size) or 1000),\n                chunk_overlap=min(200, max(0, int(self.chunk_size) // 10)),\n            )\n            docs = rc.create_documents([text])\n\n        chunks = [doc.page_content for doc in docs]\n\n        return SplitterOutput(\n            chunks=chunks,\n            chunk_id=self._generate_chunk_ids(len(chunks)),\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"header_splitter\",\n            split_params={\n                \"headers_to_split_on\": self.headers_to_split_on,\n                \"group_header_with_content\": self.group_header_with_content,\n            },\n            metadata=self._default_metadata(),\n        )\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.header_splitter.HeaderSplitter.__init__","title":"<code>__init__(chunk_size=1000, headers_to_split_on=None, *, group_header_with_content=True)</code>","text":"<p>Initialize the HeaderSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Used by fallback character splitter if no headers are found.</p> <code>1000</code> <code>headers_to_split_on</code> <code>Optional[List[str]]</code> <p>Semantic headers, e.g. [\"Header 1\", \"Header 2\"]. Defaults to all levels 1\u20136.</p> <code>None</code> <code>group_header_with_content</code> <code>bool</code> <p>Keep headers attached to following content if True.</p> <code>True</code> Source code in <code>src/splitter_mr/splitter/splitters/header_splitter.py</code> <pre><code>def __init__(\n    self,\n    chunk_size: int = 1000,\n    headers_to_split_on: Optional[List[str]] = None,\n    *,\n    group_header_with_content: bool = True,\n):\n    \"\"\"\n    Initialize the HeaderSplitter.\n\n    Args:\n        chunk_size (int): Used by fallback character splitter if no headers are found.\n        headers_to_split_on (Optional[List[str]]): Semantic headers, e.g. [\"Header 1\", \"Header 2\"].\n            Defaults to all levels 1\u20136.\n        group_header_with_content (bool): Keep headers attached to following content if True.\n    \"\"\"\n    super().__init__(chunk_size)\n    # Default to all 6 levels for robust splitting unless caller narrows it.\n    self.headers_to_split_on = headers_to_split_on or [\n        f\"Header {i}\" for i in range(1, 7)\n    ]\n    self.group_header_with_content = bool(group_header_with_content)\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.header_splitter.HeaderSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Perform header-based splitting with HTML\u2192Markdown conversion and safe fallback.</p> Steps <p>1) Detect filetype (HTML/MD). 2) If HTML, convert to Markdown with HtmlToMarkdown (emits ATX headings). 3) If Markdown, normalize Setext headings to ATX. 4) Split by headers via MarkdownHeaderTextSplitter. 5) If no headers found, fallback to RecursiveCharacterTextSplitter.</p> Source code in <code>src/splitter_mr/splitter/splitters/header_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Perform header-based splitting with HTML\u2192Markdown conversion and safe fallback.\n\n    Steps:\n      1) Detect filetype (HTML/MD).\n      2) If HTML, convert to Markdown with HtmlToMarkdown (emits ATX headings).\n      3) If Markdown, normalize Setext headings to ATX.\n      4) Split by headers via MarkdownHeaderTextSplitter.\n      5) If no headers found, fallback to RecursiveCharacterTextSplitter.\n    \"\"\"\n    if not reader_output.text:\n        raise ValueError(\"reader_output.text is empty or None\")\n\n    filetype = self._guess_filetype(reader_output)\n    tuples = self._make_tuples(\"md\")  # Always work in Markdown space.\n\n    text = reader_output.text\n\n    # HTML \u2192 Markdown using the project's converter\n    if filetype == \"html\":\n        text = HtmlToMarkdown().convert(text)\n    else:\n        # Normalize Setext headings if already Markdown\n        text = self._normalize_setext(text)\n\n    # Detect presence of ATX headers (after conversion/normalization)\n    has_headers = bool(re.search(r\"(?m)^\\s*#{1,6}\\s+\\S\", text))\n\n    # Configure header splitter. group_header_with_content -&gt; strip_headers False\n    splitter = MarkdownHeaderTextSplitter(\n        headers_to_split_on=tuples,\n        return_each_line=False,\n        strip_headers=not self.group_header_with_content,\n    )\n\n    docs = splitter.split_text(text) if has_headers else []\n    # Fallback if no headers were found\n    if not docs:\n        rc = RecursiveCharacterTextSplitter(\n            chunk_size=max(1, int(self.chunk_size) or 1000),\n            chunk_overlap=min(200, max(0, int(self.chunk_size) // 10)),\n        )\n        docs = rc.create_documents([text])\n\n    chunks = [doc.page_content for doc in docs]\n\n    return SplitterOutput(\n        chunks=chunks,\n        chunk_id=self._generate_chunk_ids(len(chunks)),\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"header_splitter\",\n        split_params={\n            \"headers_to_split_on\": self.headers_to_split_on,\n            \"group_header_with_content\": self.group_header_with_content,\n        },\n        metadata=self._default_metadata(),\n    )\n</code></pre>"},{"location":"api_reference/splitter/#recursivejsonsplitter","title":"RecursiveJSONSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.json_splitter.RecursiveJSONSplitter","title":"<code>RecursiveJSONSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>RecursiveJSONSplitter splits a JSON string or structure into overlapping or non-overlapping chunks, using the Langchain RecursiveJsonSplitter. This splitter is designed to recursively break down JSON data (including nested objects and arrays) into manageable pieces based on keys, arrays, or other separators, until the desired chunk size is reached.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Maximum chunk size, measured in the number of characters per chunk.</p> <code>1000</code> <code>min_chunk_size</code> <code>int</code> <p>Minimum chunk size, in characters.</p> <code>200</code> Notes <p>See Langchain Docs on RecursiveJsonSplitter.</p> Source code in <code>src/splitter_mr/splitter/splitters/json_splitter.py</code> <pre><code>class RecursiveJSONSplitter(BaseSplitter):\n    \"\"\"\n    RecursiveJSONSplitter splits a JSON string or structure into overlapping or non-overlapping\n    chunks, using the Langchain RecursiveJsonSplitter. This splitter is designed to recursively\n    break down JSON data (including nested objects and arrays) into manageable pieces based on keys,\n    arrays, or other separators, until the desired chunk size is reached.\n\n    Args:\n        chunk_size (int): Maximum chunk size, measured in the number of characters per chunk.\n        min_chunk_size (int): Minimum chunk size, in characters.\n\n    Notes:\n        See [Langchain Docs on RecursiveJsonSplitter](https://python.langchain.com/api_reference/text_splitters/json/langchain_text_splitters.json.RecursiveJsonSplitter.html#langchain_text_splitters.json.RecursiveJsonSplitter).\n    \"\"\"\n\n    def __init__(self, chunk_size: int = 1000, min_chunk_size: int = 200):\n        super().__init__(chunk_size)\n        self.min_chunk_size = min_chunk_size\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Splits the input JSON text from the reader_output dictionary into recursively chunked pieces,\n        allowing for overlap by number or percentage of characters.\n\n        Args:\n            reader_output (Dict[str, Any]):\n                Dictionary containing at least a 'text' key (str) and optional document metadata\n                (e.g., 'document_name', 'document_path', etc.).\n\n        Returns:\n            SplitterOutput: Dataclass defining the output structure for all splitters.\n\n        Raises:\n            ValueError: If the 'text' field is missing from reader_output.\n            json.JSONDecodeError: If the 'text' field contains invalid JSON.\n\n        Example:\n            ```python\n            from splitter_mr.splitter import RecursiveJSONSplitter\n\n            # This dictionary has been obtained from `VanillaReader`\n            reader_output = ReaderOutput(\n                text: '{\"company\": {\"name\": \"TechCorp\", \"employees\": [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}]}}'\n                document_name: \"company_data.json\",\n                document_path: \"/https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/company_data.json\",\n                document_id: \"doc123\",\n                conversion_method: \"vanilla\",\n                ocr_method: None\n            )\n            splitter = RecursiveJSONSplitter(chunk_size=100, min_chunk_size=20)\n            output = splitter.split(reader_output)\n            print(output[\"chunks\"])\n            ```\n            ```python\n            ['{\"company\": {\"name\": \"TechCorp\"}}',\n            '{\"employees\": [{\"name\": \"Alice\"},\n            {\"name\": \"Bob\"}]}']\n            ```\n        \"\"\"\n        # Initialize variables\n        text = json.loads(reader_output.text)\n\n        # Split text into smaller JSON chunks\n        splitter = RecursiveJsonSplitter(\n            max_chunk_size=self.chunk_size,\n            min_chunk_size=int(self.chunk_size - self.min_chunk_size),\n        )\n        chunks = splitter.split_text(json_data=text, convert_lists=True)\n\n        # Generate chunk_ids and metadata\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n        metadata = self._default_metadata()\n\n        # Return output\n        output = SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"recursive_json_splitter\",\n            split_params={\n                \"max_chunk_size\": self.chunk_size,\n                \"min_chunk_size\": self.min_chunk_size,\n            },\n            metadata=metadata,\n        )\n        return output\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.json_splitter.RecursiveJSONSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Splits the input JSON text from the reader_output dictionary into recursively chunked pieces, allowing for overlap by number or percentage of characters.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>Dict[str, Any]</code> <p>Dictionary containing at least a 'text' key (str) and optional document metadata (e.g., 'document_name', 'document_path', etc.).</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>Dataclass defining the output structure for all splitters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'text' field is missing from reader_output.</p> <code>JSONDecodeError</code> <p>If the 'text' field contains invalid JSON.</p> Example <p><pre><code>from splitter_mr.splitter import RecursiveJSONSplitter\n\n# This dictionary has been obtained from `VanillaReader`\nreader_output = ReaderOutput(\n    text: '{\"company\": {\"name\": \"TechCorp\", \"employees\": [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}]}}'\n    document_name: \"company_data.json\",\n    document_path: \"/https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/company_data.json\",\n    document_id: \"doc123\",\n    conversion_method: \"vanilla\",\n    ocr_method: None\n)\nsplitter = RecursiveJSONSplitter(chunk_size=100, min_chunk_size=20)\noutput = splitter.split(reader_output)\nprint(output[\"chunks\"])\n</code></pre> <pre><code>['{\"company\": {\"name\": \"TechCorp\"}}',\n'{\"employees\": [{\"name\": \"Alice\"},\n{\"name\": \"Bob\"}]}']\n</code></pre></p> Source code in <code>src/splitter_mr/splitter/splitters/json_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Splits the input JSON text from the reader_output dictionary into recursively chunked pieces,\n    allowing for overlap by number or percentage of characters.\n\n    Args:\n        reader_output (Dict[str, Any]):\n            Dictionary containing at least a 'text' key (str) and optional document metadata\n            (e.g., 'document_name', 'document_path', etc.).\n\n    Returns:\n        SplitterOutput: Dataclass defining the output structure for all splitters.\n\n    Raises:\n        ValueError: If the 'text' field is missing from reader_output.\n        json.JSONDecodeError: If the 'text' field contains invalid JSON.\n\n    Example:\n        ```python\n        from splitter_mr.splitter import RecursiveJSONSplitter\n\n        # This dictionary has been obtained from `VanillaReader`\n        reader_output = ReaderOutput(\n            text: '{\"company\": {\"name\": \"TechCorp\", \"employees\": [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}]}}'\n            document_name: \"company_data.json\",\n            document_path: \"/https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/company_data.json\",\n            document_id: \"doc123\",\n            conversion_method: \"vanilla\",\n            ocr_method: None\n        )\n        splitter = RecursiveJSONSplitter(chunk_size=100, min_chunk_size=20)\n        output = splitter.split(reader_output)\n        print(output[\"chunks\"])\n        ```\n        ```python\n        ['{\"company\": {\"name\": \"TechCorp\"}}',\n        '{\"employees\": [{\"name\": \"Alice\"},\n        {\"name\": \"Bob\"}]}']\n        ```\n    \"\"\"\n    # Initialize variables\n    text = json.loads(reader_output.text)\n\n    # Split text into smaller JSON chunks\n    splitter = RecursiveJsonSplitter(\n        max_chunk_size=self.chunk_size,\n        min_chunk_size=int(self.chunk_size - self.min_chunk_size),\n    )\n    chunks = splitter.split_text(json_data=text, convert_lists=True)\n\n    # Generate chunk_ids and metadata\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n    metadata = self._default_metadata()\n\n    # Return output\n    output = SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"recursive_json_splitter\",\n        split_params={\n            \"max_chunk_size\": self.chunk_size,\n            \"min_chunk_size\": self.min_chunk_size,\n        },\n        metadata=metadata,\n    )\n    return output\n</code></pre>"},{"location":"api_reference/splitter/#htmltagsplitter","title":"HTMLTagSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.html_tag_splitter.HTMLTagSplitter","title":"<code>HTMLTagSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>HTMLTagSplitter splits HTML content into chunks based on a specified tag. Supports batching and optional Markdown conversion.</p> Behavior <ul> <li>When <code>tag</code> is specified (e.g., tag=\"div\"), finds all matching elements.</li> <li>When <code>tag</code> is None, splits by the most frequent and shallowest tag.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Maximum chunk size in characters (only used when <code>batch=True</code>).</p> <code>1</code> <code>tag</code> <code>str | None</code> <p>HTML tag to split on. If None, auto-detects the best tag.</p> <code>None</code> <code>batch</code> <code>bool</code> <p>If True (default), groups multiple tags into a chunk, not exceeding <code>chunk_size</code>. If False, returns one chunk per tag, ignoring chunk_size.</p> <code>True</code> <code>to_markdown</code> <code>bool</code> <p>If True, converts each chunk to Markdown using HtmlToMarkdown.</p> <code>True</code> Example <p>reader_output = ReaderOutput(text=\"AB\") splitter = HTMLTagSplitter(tag=\"div\", batch=False) splitter.split(reader_output).chunks ['A', 'B'] splitter = HTMLTagSplitter(tag=\"div\", batch=True, chunk_size=100) splitter.split(reader_output).chunks ['AB'] splitter = HTMLTagSplitter(tag=\"div\", batch=False, to_markdown=True) splitter.split(reader_output).chunks ['A', 'B']</p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>int</code> <p>Maximum chunk size.</p> <code>tag</code> <code>Optional[str]</code> <p>Tag to split on.</p> <code>batch</code> <code>bool</code> <p>Whether to group elements into chunks.</p> <code>to_markdown</code> <code>bool</code> <p>Whether to convert each chunk to Markdown.</p> Source code in <code>src/splitter_mr/splitter/splitters/html_tag_splitter.py</code> <pre><code>class HTMLTagSplitter(BaseSplitter):\n    \"\"\"\n    HTMLTagSplitter splits HTML content into chunks based on a specified tag.\n    Supports batching and optional Markdown conversion.\n\n    Behavior:\n      - When `tag` is specified (e.g., tag=\"div\"), finds all matching elements.\n      - When `tag` is None, splits by the most frequent and shallowest tag.\n\n    Args:\n        chunk_size (int): Maximum chunk size in characters (only used when `batch=True`).\n        tag (str | None): HTML tag to split on. If None, auto-detects the best tag.\n        batch (bool): If True (default), groups multiple tags into a chunk, not exceeding `chunk_size`.\n            If False, returns one chunk per tag, ignoring chunk_size.\n        to_markdown (bool): If True, converts each chunk to Markdown using HtmlToMarkdown.\n\n    Example:\n        &gt;&gt;&gt; reader_output = ReaderOutput(text=\"&lt;div&gt;A&lt;/div&gt;&lt;div&gt;B&lt;/div&gt;\")\n        &gt;&gt;&gt; splitter = HTMLTagSplitter(tag=\"div\", batch=False)\n        &gt;&gt;&gt; splitter.split(reader_output).chunks\n        ['&lt;html&gt;&lt;body&gt;&lt;div&gt;A&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;', '&lt;html&gt;&lt;body&gt;&lt;div&gt;B&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;']\n        &gt;&gt;&gt; splitter = HTMLTagSplitter(tag=\"div\", batch=True, chunk_size=100)\n        &gt;&gt;&gt; splitter.split(reader_output).chunks\n        ['&lt;html&gt;&lt;body&gt;&lt;div&gt;A&lt;/div&gt;&lt;div&gt;B&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;']\n        &gt;&gt;&gt; splitter = HTMLTagSplitter(tag=\"div\", batch=False, to_markdown=True)\n        &gt;&gt;&gt; splitter.split(reader_output).chunks\n        ['A', 'B']\n\n    Attributes:\n        chunk_size (int): Maximum chunk size.\n        tag (Optional[str]): Tag to split on.\n        batch (bool): Whether to group elements into chunks.\n        to_markdown (bool): Whether to convert each chunk to Markdown.\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 1,\n        tag: Optional[str] = None,\n        *,\n        batch: bool = True,\n        to_markdown: bool = True,\n    ):\n        \"\"\"\n        Initialize HTMLTagSplitter.\n\n        Args:\n            chunk_size (int): Maximum chunk size, in characters (only for batching).\n            tag (str | None): Tag to split on. If None, auto-detects.\n            batch (bool): If True (default), groups tags up to `chunk_size`.\n            to_markdown (bool): If True (default), convert each chunk to Markdown.\n        \"\"\"\n        super().__init__(chunk_size)\n        self.tag = tag\n        self.batch = batch\n        self.to_markdown = to_markdown\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Splits HTML using the specified tag and batching, with optional Markdown conversion.\n\n        Semantics:\n        - Tables:\n            * batch=False -&gt; one chunk per requested element. If splitting by a row-level tag\n                (e.g. 'tr'), emit a mini-table per row: &lt;thead&gt; once + that row in &lt;tbody&gt;.\n            * batch=True and chunk_size in (0, 1, None) -&gt; all tables in one chunk.\n            * batch=True and chunk_size &gt; 1 -&gt; split each table into multiple chunks\n                by batching &lt;tr&gt; rows (copying a &lt;thead&gt; into every chunk and\n                skipping the header row from the body).\n        - Non-table tags:\n            * batch=False -&gt; one chunk per element.\n            * batch=True and chunk_size in (0, 1, None) -&gt; all elements in one chunk.\n            * batch=True and chunk_size &gt; 1 -&gt; batch by total HTML length.\n\n        Args:\n            reader_output: ReaderOutput containing at least `text`.\n\n        Returns:\n            SplitterOutput\n        \"\"\"\n        html = getattr(reader_output, \"text\", \"\") or \"\"\n        soup = BeautifulSoup(html, \"html.parser\")\n        tag = self.tag or self._auto_tag(soup)\n\n        # Locate elements for the chosen tag.\n        try:\n            elements = soup.find_all(tag)\n            table_children = {\"tr\", \"thead\", \"tbody\", \"th\", \"td\"}\n            # Only escalate to table when batching is enabled. For non-batch,\n            # keep the exact tag so we can emit one chunk per element.\n            if self.batch and tag in table_children:\n                seen = set()\n                parent_tables = []\n                for el in elements:\n                    table = el.find_parent(\"table\")\n                    if table and id(table) not in seen:\n                        seen.add(id(table))\n                        parent_tables.append(table)\n                if parent_tables:\n                    elements = parent_tables\n                    tag = \"table\"\n        except Exception:\n            elements = []\n\n        # -------- helpers -------- #\n\n        def build_doc_with_children(children: List) -&gt; str:\n            \"\"\"Wrap a list of top-level nodes into &lt;html&gt;&lt;body&gt;\u2026&lt;/body&gt;&lt;/html&gt;.\"\"\"\n            doc = BeautifulSoup(\"\", \"html.parser\")\n            html_tag = doc.new_tag(\"html\")\n            body_tag = doc.new_tag(\"body\")\n            html_tag.append(body_tag)\n            doc.append(html_tag)\n            for c in children:\n                body_tag.append(copy.deepcopy(c))\n            return str(doc)\n\n        def extract_table_header_and_rows(table_tag):\n            \"\"\"\n            Return (header_thead, data_rows, header_row_src) where:\n            - header_thead is a &lt;thead&gt; (deep-copied) or None\n            - data_rows is a list of original &lt;tr&gt; nodes that are NOT header rows\n            - header_row_src is the original &lt;tr&gt; used to synthesize &lt;thead&gt; (if any)\n            \"\"\"\n            header = table_tag.find(\"thead\")\n            header_row_src = None\n\n            if header is not None:\n                data_rows = []\n                for tr in table_tag.find_all(\"tr\"):\n                    if tr.find_parent(\"thead\") is not None:\n                        continue\n                    data_rows.append(tr)\n                return copy.deepcopy(header), data_rows, None\n\n            first_tr = table_tag.find(\"tr\")\n            header_thead = None\n            if first_tr is not None:\n                tmp = BeautifulSoup(\"\", \"html.parser\")\n                thead = tmp.new_tag(\"thead\")\n                thead.append(copy.deepcopy(first_tr))\n                header_thead = thead\n                header_row_src = first_tr\n\n            data_rows = []\n            for tr in table_tag.find_all(\"tr\"):\n                if header_row_src is not None and tr is header_row_src:\n                    continue\n                if tr.find_parent(\"thead\") is not None:\n                    continue\n                data_rows.append(tr)\n\n            return header_thead, data_rows, header_row_src\n\n        def build_table_chunk(table_tag, rows_subset: List) -&gt; str:\n            \"\"\"\n            Build a &lt;html&gt;&lt;body&gt;&lt;table&gt;\u2026 chunk with:\n            - original table attributes\n            - a &lt;thead&gt; (original or synthesized)\n            - a &lt;tbody&gt; containing rows_subset\n            \"\"\"\n            header_thead, _, _ = extract_table_header_and_rows(table_tag)\n            doc = BeautifulSoup(\"\", \"html.parser\")\n            html_tag = doc.new_tag(\"html\")\n            body_tag = doc.new_tag(\"body\")\n            html_tag.append(body_tag)\n            doc.append(html_tag)\n\n            new_table = doc.new_tag(\"table\", **table_tag.attrs)\n            if header_thead is not None:\n                new_table.append(copy.deepcopy(header_thead))\n\n            tbody = doc.new_tag(\"tbody\")\n            for r in rows_subset:\n                tbody.append(copy.deepcopy(r))\n            new_table.append(tbody)\n\n            body_tag.append(new_table)\n            return str(doc)\n\n        # -------- main chunking -------- #\n\n        chunks: List[str] = []\n\n        if tag == \"table\":\n            # TABLES: custom batching\n            if not self.batch:\n                # one chunk per table (full)\n                chunks = [build_doc_with_children([el]) for el in elements]\n\n            elif self.chunk_size in (0, 1, None):\n                # all tables together\n                chunks = [build_doc_with_children(elements)] if elements else [\"\"]\n\n            else:\n                # batch rows within each table\n                for table_el in elements:\n                    header_thead, rows, _ = extract_table_header_and_rows(table_el)\n                    if not rows:\n                        chunks.append(build_doc_with_children([table_el]))\n                        continue\n\n                    buf: List = []\n                    for row in rows:\n                        test_buf = buf + [row]\n                        test_html = build_table_chunk(table_el, test_buf)\n                        if len(test_html) &gt; self.chunk_size and buf:\n                            chunks.append(build_table_chunk(table_el, buf))\n                            buf = [row]\n                        else:\n                            buf = test_buf\n                    if buf:\n                        chunks.append(build_table_chunk(table_el, buf))\n\n        else:\n            # NON-TABLE (including table children when batch=False)\n            table_children = {\"tr\", \"thead\", \"tbody\", \"th\", \"td\"}\n\n            if not self.batch:\n                if tag in table_children:\n                    # one chunk per row-like element, but keep header context\n                    for el in elements:\n                        table_el = el.find_parent(\"table\")\n                        if not table_el:\n                            # Fallback: wrap the element as-is\n                            chunks.append(build_doc_with_children([el]))\n                            continue\n                        # skip header-only rows\n                        if el.name == \"tr\" and el.find_parent(\"thead\") is not None:\n                            continue\n                        if el.name in {\"thead\", \"th\"}:\n                            continue\n                        chunks.append(build_table_chunk(table_el, [el]))\n                else:\n                    for el in elements:\n                        chunks.append(build_doc_with_children([el]))\n\n            elif self.chunk_size in (0, 1, None):\n                chunks = [build_doc_with_children(elements)] if elements else [\"\"]\n\n            else:\n                buffer = []\n                for el in elements:\n                    test_buffer = buffer + [el]\n                    test_chunk_str = build_doc_with_children(test_buffer)\n                    if len(test_chunk_str) &gt; self.chunk_size and buffer:\n                        chunks.append(build_doc_with_children(buffer))\n                        buffer = [el]\n                    else:\n                        buffer = test_buffer\n                if buffer:\n                    chunks.append(build_doc_with_children(buffer))\n\n        if not chunks:\n            chunks = [\"\"]\n\n        if self.to_markdown:\n            md = HtmlToMarkdown()\n            chunks = [md.convert(chunk) for chunk in chunks]\n\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n        return SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"html_tag_splitter\",\n            split_params={\n                \"chunk_size\": self.chunk_size,\n                \"tag\": tag,\n                \"batch\": self.batch,\n                \"to_markdown\": self.to_markdown,\n            },\n            metadata=self._default_metadata(),\n        )\n\n    def _auto_tag(self, soup: BeautifulSoup) -&gt; str:\n        \"\"\"\n        Auto-detect the most repeated tag with the highest (shallowest) level of hierarchy.\n        If no repeated tags are found, return the first tag found in &lt;body&gt; or fallback to 'div'.\n        \"\"\"\n        from collections import Counter, defaultdict\n\n        body = soup.find(\"body\")\n        if not body:\n            return \"div\"\n\n        # Traverse all tags in body, tracking tag: (count, min_depth)\n        tag_counter = Counter()\n        tag_min_depth = defaultdict(lambda: float(\"inf\"))\n\n        def traverse(el, depth=0):\n            for child in el.children:\n                if getattr(child, \"name\", None):\n                    tag_counter[child.name] += 1\n                    tag_min_depth[child.name] = min(tag_min_depth[child.name], depth)\n                    traverse(child, depth + 1)\n\n        traverse(body)\n\n        if not tag_counter:\n            # fallback to first tag\n            for t in body.find_all(True, recursive=True):\n                return t.name\n            return \"div\"\n\n        # Find tags with the maximum count\n        max_count = max(tag_counter.values())\n        candidates = [t for t, cnt in tag_counter.items() if cnt == max_count]\n        # Of the most frequent, pick the one with the minimum depth (shallowest)\n        chosen = min(candidates, key=lambda t: tag_min_depth[t])\n        return chosen\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.html_tag_splitter.HTMLTagSplitter.__init__","title":"<code>__init__(chunk_size=1, tag=None, *, batch=True, to_markdown=True)</code>","text":"<p>Initialize HTMLTagSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Maximum chunk size, in characters (only for batching).</p> <code>1</code> <code>tag</code> <code>str | None</code> <p>Tag to split on. If None, auto-detects.</p> <code>None</code> <code>batch</code> <code>bool</code> <p>If True (default), groups tags up to <code>chunk_size</code>.</p> <code>True</code> <code>to_markdown</code> <code>bool</code> <p>If True (default), convert each chunk to Markdown.</p> <code>True</code> Source code in <code>src/splitter_mr/splitter/splitters/html_tag_splitter.py</code> <pre><code>def __init__(\n    self,\n    chunk_size: int = 1,\n    tag: Optional[str] = None,\n    *,\n    batch: bool = True,\n    to_markdown: bool = True,\n):\n    \"\"\"\n    Initialize HTMLTagSplitter.\n\n    Args:\n        chunk_size (int): Maximum chunk size, in characters (only for batching).\n        tag (str | None): Tag to split on. If None, auto-detects.\n        batch (bool): If True (default), groups tags up to `chunk_size`.\n        to_markdown (bool): If True (default), convert each chunk to Markdown.\n    \"\"\"\n    super().__init__(chunk_size)\n    self.tag = tag\n    self.batch = batch\n    self.to_markdown = to_markdown\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.html_tag_splitter.HTMLTagSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Splits HTML using the specified tag and batching, with optional Markdown conversion.</p> <p>Semantics: - Tables:     * batch=False -&gt; one chunk per requested element. If splitting by a row-level tag         (e.g. 'tr'), emit a mini-table per row:  once + that row in .     * batch=True and chunk_size in (0, 1, None) -&gt; all tables in one chunk.     * batch=True and chunk_size &gt; 1 -&gt; split each table into multiple chunks         by batching  rows (copying a  into every chunk and         skipping the header row from the body). - Non-table tags:     * batch=False -&gt; one chunk per element.     * batch=True and chunk_size in (0, 1, None) -&gt; all elements in one chunk.     * batch=True and chunk_size &gt; 1 -&gt; batch by total HTML length. <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>ReaderOutput</code> <p>ReaderOutput containing at least <code>text</code>.</p> required <p>Returns:</p> Type Description <code>SplitterOutput</code> <p>SplitterOutput</p> Source code in <code>src/splitter_mr/splitter/splitters/html_tag_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Splits HTML using the specified tag and batching, with optional Markdown conversion.\n\n    Semantics:\n    - Tables:\n        * batch=False -&gt; one chunk per requested element. If splitting by a row-level tag\n            (e.g. 'tr'), emit a mini-table per row: &lt;thead&gt; once + that row in &lt;tbody&gt;.\n        * batch=True and chunk_size in (0, 1, None) -&gt; all tables in one chunk.\n        * batch=True and chunk_size &gt; 1 -&gt; split each table into multiple chunks\n            by batching &lt;tr&gt; rows (copying a &lt;thead&gt; into every chunk and\n            skipping the header row from the body).\n    - Non-table tags:\n        * batch=False -&gt; one chunk per element.\n        * batch=True and chunk_size in (0, 1, None) -&gt; all elements in one chunk.\n        * batch=True and chunk_size &gt; 1 -&gt; batch by total HTML length.\n\n    Args:\n        reader_output: ReaderOutput containing at least `text`.\n\n    Returns:\n        SplitterOutput\n    \"\"\"\n    html = getattr(reader_output, \"text\", \"\") or \"\"\n    soup = BeautifulSoup(html, \"html.parser\")\n    tag = self.tag or self._auto_tag(soup)\n\n    # Locate elements for the chosen tag.\n    try:\n        elements = soup.find_all(tag)\n        table_children = {\"tr\", \"thead\", \"tbody\", \"th\", \"td\"}\n        # Only escalate to table when batching is enabled. For non-batch,\n        # keep the exact tag so we can emit one chunk per element.\n        if self.batch and tag in table_children:\n            seen = set()\n            parent_tables = []\n            for el in elements:\n                table = el.find_parent(\"table\")\n                if table and id(table) not in seen:\n                    seen.add(id(table))\n                    parent_tables.append(table)\n            if parent_tables:\n                elements = parent_tables\n                tag = \"table\"\n    except Exception:\n        elements = []\n\n    # -------- helpers -------- #\n\n    def build_doc_with_children(children: List) -&gt; str:\n        \"\"\"Wrap a list of top-level nodes into &lt;html&gt;&lt;body&gt;\u2026&lt;/body&gt;&lt;/html&gt;.\"\"\"\n        doc = BeautifulSoup(\"\", \"html.parser\")\n        html_tag = doc.new_tag(\"html\")\n        body_tag = doc.new_tag(\"body\")\n        html_tag.append(body_tag)\n        doc.append(html_tag)\n        for c in children:\n            body_tag.append(copy.deepcopy(c))\n        return str(doc)\n\n    def extract_table_header_and_rows(table_tag):\n        \"\"\"\n        Return (header_thead, data_rows, header_row_src) where:\n        - header_thead is a &lt;thead&gt; (deep-copied) or None\n        - data_rows is a list of original &lt;tr&gt; nodes that are NOT header rows\n        - header_row_src is the original &lt;tr&gt; used to synthesize &lt;thead&gt; (if any)\n        \"\"\"\n        header = table_tag.find(\"thead\")\n        header_row_src = None\n\n        if header is not None:\n            data_rows = []\n            for tr in table_tag.find_all(\"tr\"):\n                if tr.find_parent(\"thead\") is not None:\n                    continue\n                data_rows.append(tr)\n            return copy.deepcopy(header), data_rows, None\n\n        first_tr = table_tag.find(\"tr\")\n        header_thead = None\n        if first_tr is not None:\n            tmp = BeautifulSoup(\"\", \"html.parser\")\n            thead = tmp.new_tag(\"thead\")\n            thead.append(copy.deepcopy(first_tr))\n            header_thead = thead\n            header_row_src = first_tr\n\n        data_rows = []\n        for tr in table_tag.find_all(\"tr\"):\n            if header_row_src is not None and tr is header_row_src:\n                continue\n            if tr.find_parent(\"thead\") is not None:\n                continue\n            data_rows.append(tr)\n\n        return header_thead, data_rows, header_row_src\n\n    def build_table_chunk(table_tag, rows_subset: List) -&gt; str:\n        \"\"\"\n        Build a &lt;html&gt;&lt;body&gt;&lt;table&gt;\u2026 chunk with:\n        - original table attributes\n        - a &lt;thead&gt; (original or synthesized)\n        - a &lt;tbody&gt; containing rows_subset\n        \"\"\"\n        header_thead, _, _ = extract_table_header_and_rows(table_tag)\n        doc = BeautifulSoup(\"\", \"html.parser\")\n        html_tag = doc.new_tag(\"html\")\n        body_tag = doc.new_tag(\"body\")\n        html_tag.append(body_tag)\n        doc.append(html_tag)\n\n        new_table = doc.new_tag(\"table\", **table_tag.attrs)\n        if header_thead is not None:\n            new_table.append(copy.deepcopy(header_thead))\n\n        tbody = doc.new_tag(\"tbody\")\n        for r in rows_subset:\n            tbody.append(copy.deepcopy(r))\n        new_table.append(tbody)\n\n        body_tag.append(new_table)\n        return str(doc)\n\n    # -------- main chunking -------- #\n\n    chunks: List[str] = []\n\n    if tag == \"table\":\n        # TABLES: custom batching\n        if not self.batch:\n            # one chunk per table (full)\n            chunks = [build_doc_with_children([el]) for el in elements]\n\n        elif self.chunk_size in (0, 1, None):\n            # all tables together\n            chunks = [build_doc_with_children(elements)] if elements else [\"\"]\n\n        else:\n            # batch rows within each table\n            for table_el in elements:\n                header_thead, rows, _ = extract_table_header_and_rows(table_el)\n                if not rows:\n                    chunks.append(build_doc_with_children([table_el]))\n                    continue\n\n                buf: List = []\n                for row in rows:\n                    test_buf = buf + [row]\n                    test_html = build_table_chunk(table_el, test_buf)\n                    if len(test_html) &gt; self.chunk_size and buf:\n                        chunks.append(build_table_chunk(table_el, buf))\n                        buf = [row]\n                    else:\n                        buf = test_buf\n                if buf:\n                    chunks.append(build_table_chunk(table_el, buf))\n\n    else:\n        # NON-TABLE (including table children when batch=False)\n        table_children = {\"tr\", \"thead\", \"tbody\", \"th\", \"td\"}\n\n        if not self.batch:\n            if tag in table_children:\n                # one chunk per row-like element, but keep header context\n                for el in elements:\n                    table_el = el.find_parent(\"table\")\n                    if not table_el:\n                        # Fallback: wrap the element as-is\n                        chunks.append(build_doc_with_children([el]))\n                        continue\n                    # skip header-only rows\n                    if el.name == \"tr\" and el.find_parent(\"thead\") is not None:\n                        continue\n                    if el.name in {\"thead\", \"th\"}:\n                        continue\n                    chunks.append(build_table_chunk(table_el, [el]))\n            else:\n                for el in elements:\n                    chunks.append(build_doc_with_children([el]))\n\n        elif self.chunk_size in (0, 1, None):\n            chunks = [build_doc_with_children(elements)] if elements else [\"\"]\n\n        else:\n            buffer = []\n            for el in elements:\n                test_buffer = buffer + [el]\n                test_chunk_str = build_doc_with_children(test_buffer)\n                if len(test_chunk_str) &gt; self.chunk_size and buffer:\n                    chunks.append(build_doc_with_children(buffer))\n                    buffer = [el]\n                else:\n                    buffer = test_buffer\n            if buffer:\n                chunks.append(build_doc_with_children(buffer))\n\n    if not chunks:\n        chunks = [\"\"]\n\n    if self.to_markdown:\n        md = HtmlToMarkdown()\n        chunks = [md.convert(chunk) for chunk in chunks]\n\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n    return SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"html_tag_splitter\",\n        split_params={\n            \"chunk_size\": self.chunk_size,\n            \"tag\": tag,\n            \"batch\": self.batch,\n            \"to_markdown\": self.to_markdown,\n        },\n        metadata=self._default_metadata(),\n    )\n</code></pre>"},{"location":"api_reference/splitter/#rowcolumnsplitter","title":"RowColumnSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.row_column_splitter.RowColumnSplitter","title":"<code>RowColumnSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>RowColumnSplitter splits tabular data (such as CSV, TSV, Markdown tables, or JSON tables) into smaller tables based on rows, columns, or by total character size while preserving row integrity.</p> <p>This splitter supports several modes:</p> <ul> <li>By rows: Split the table into chunks with a fixed number of rows, with optional overlapping     rows between chunks.</li> <li>By columns: Split the table into chunks by columns, with optional overlapping columns between chunks.</li> <li>By chunk size: Split the table into markdown-formatted table chunks, where each chunk contains     as many complete rows as fit under the specified character limit, optionally overlapping a fixed     number of rows between chunks.</li> </ul> <p>This is useful for splitting large tabular files for downstream processing, LLM ingestion, or display, while preserving semantic and structural integrity of the data.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Maximum number of characters per chunk (when using character-based splitting).</p> <code>1000</code> <code>num_rows</code> <code>int</code> <p>Number of rows per chunk. Mutually exclusive with num_cols.</p> <code>0</code> <code>num_cols</code> <code>int</code> <p>Number of columns per chunk. Mutually exclusive with num_rows.</p> <code>0</code> <code>chunk_overlap</code> <code>Union[int, float]</code> <p>Number of overlapping rows or columns between chunks. If a float in (0,1), interpreted as a percentage of rows or columns. If integer, the number of overlapping rows/columns. When chunking by character size, this refers to the number of overlapping rows (not characters).</p> <code>0</code> <p>Supported formats: CSV, TSV, TXT, Markdown table, JSON (tabular: list of dicts or dict of lists).</p> Source code in <code>src/splitter_mr/splitter/splitters/row_column_splitter.py</code> <pre><code>class RowColumnSplitter(BaseSplitter):\n    \"\"\"\n    RowColumnSplitter splits tabular data (such as CSV, TSV, Markdown tables, or JSON tables)\n    into smaller tables based on rows, columns, or by total character size while preserving row integrity.\n\n    This splitter supports several modes:\n\n    - **By rows**: Split the table into chunks with a fixed number of rows, with optional overlapping\n        rows between chunks.\n    - **By columns**: Split the table into chunks by columns, with optional overlapping columns between chunks.\n    - **By chunk size**: Split the table into markdown-formatted table chunks, where each chunk contains\n        as many complete rows as fit under the specified character limit, optionally overlapping a fixed\n        number of rows between chunks.\n\n    This is useful for splitting large tabular files for downstream processing, LLM ingestion,\n    or display, while preserving semantic and structural integrity of the data.\n\n    Args:\n        chunk_size (int): Maximum number of characters per chunk (when using character-based splitting).\n        num_rows (int): Number of rows per chunk. Mutually exclusive with num_cols.\n        num_cols (int): Number of columns per chunk. Mutually exclusive with num_rows.\n        chunk_overlap (Union[int, float]): Number of overlapping rows or columns between chunks.\n            If a float in (0,1), interpreted as a percentage of rows or columns. If integer, the number of\n            overlapping rows/columns. When chunking by character size, this refers to the number of overlapping\n            rows (not characters).\n\n    Supported formats: CSV, TSV, TXT, Markdown table, JSON (tabular: list of dicts or dict of lists).\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 1000,\n        num_rows: int = 0,\n        num_cols: int = 0,\n        chunk_overlap: Union[int, float] = 0,\n    ):\n        super().__init__(chunk_size)\n        self.num_rows = num_rows\n        self.num_cols = num_cols\n        self.chunk_overlap = chunk_overlap\n\n        if num_rows and num_cols:\n            raise ValueError(\"num_rows and num_cols are mutually exclusive\")\n        if isinstance(chunk_overlap, float) and chunk_overlap &gt;= 1:\n            raise ValueError(\"chunk_overlap as float must be &lt; 1\")\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Splits the input tabular data into multiple markdown table chunks according to the specified\n        chunking strategy. Each output chunk is a complete markdown table with header, and will never\n        cut a row in half. The overlap is always applied in terms of full rows or columns.\n\n        Args:\n            reader_output (Dict[str, Any]):\n                Dictionary output from a Reader, containing at least:\n                    - 'text': The tabular data as string.\n                    - 'conversion_method': Format of the input ('csv', 'tsv', 'markdown', 'json', etc.).\n                    - Additional document metadata fields (optional).\n\n        Returns:\n            SplitterOutput: Dataclass defining the output structure for all splitters.\n\n        Raises:\n            ValueError: If both num_rows and num_cols are set.\n            ValueError: If chunk_overlap as float is not in [0,1).\n            ValueError: If chunk_size is too small to fit the header and at least one data row.\n\n        Example:\n            ```python\n            reader_output = ReaderOutput(\n                text: '| id | name |\\\\n|----|------|\\\\n| 1  | A    |\\\\n| 2  | B    |\\\\n| 3  | C    |',\n                conversion_method: \"markdown\",\n                document_name: \"table.md\",\n                document_path: \"/path/table.md\",\n            )\n            splitter = RowColumnSplitter(chunk_size=80, chunk_overlap=20)\n            output = splitter.split(reader_output)\n            for chunk in output[\"chunks\"]:\n                print(\"\\\\n\" + str(chunk) + \"\\\\n\")\n            ```\n            ```python\n            | id   | name   |\n            |------|--------|\n            |  1   | A      |\n            |  2   | B      |\n\n            | id   | name   |\n            |------|--------|\n            |  2   | B      |\n            |  3   | C      |\n            ```\n        \"\"\"\n        # Step 1. Parse the table depending on conversion_method\n        df = self._load_tabular(reader_output)\n        orig_method = reader_output.conversion_method\n        col_names = df.columns.tolist()\n\n        # Step 2. Split logic\n        chunks = []\n        meta_per_chunk = []\n\n        # If splitting strategy is by rows\n        if self.num_rows &gt; 0:\n            overlap = self._get_overlap(self.num_rows)\n            for i in range(\n                0,\n                len(df),\n                self.num_rows - overlap if (self.num_rows - overlap) &gt; 0 else 1,\n            ):\n                chunk_df = df.iloc[i : i + self.num_rows]\n                if not chunk_df.empty:\n                    chunk_str = self._to_str(chunk_df, orig_method)\n                    chunks.append(chunk_str)\n                    meta_per_chunk.append(\n                        {\"rows\": chunk_df.index.tolist(), \"type\": \"row\"}\n                    )\n        # If splitting strategy is by columns\n        elif self.num_cols &gt; 0:\n            overlap = self._get_overlap(self.num_cols)\n            total_cols = len(col_names)\n            for i in range(\n                0,\n                total_cols,\n                self.num_cols - overlap if (self.num_cols - overlap) &gt; 0 else 1,\n            ):\n                sel_cols = col_names[i : i + self.num_cols]\n                if sel_cols:\n                    chunk_df = df[sel_cols]\n                    chunk_str = self._to_str(chunk_df, orig_method, colwise=True)\n                    chunks.append(chunk_str)\n                    meta_per_chunk.append({\"cols\": sel_cols, \"type\": \"column\"})\n        # If splitting strategy is given by the chunk_size\n        else:\n            header_lines = self._get_markdown_header(df)\n            header_length = len(header_lines)\n\n            row_md_list = [self._get_markdown_row(df, i) for i in range(len(df))]\n            row_len_list = [len(r) + 1 for r in row_md_list]  # +1 for newline\n\n            if self.chunk_size &lt; header_length + row_len_list[0]:\n                raise ValueError(\n                    \"chunk_size is too small to fit header and at least one row.\"\n                )\n\n            # Compute overlapping and headers in markdown tables\n            chunks = []\n            meta_per_chunk = []\n            i = 0\n            n = len(row_md_list)\n            overlap = self._get_overlap(1)\n            while i &lt; n:\n                curr_chunk = []\n                curr_len = header_length\n                j = i\n                while j &lt; n and curr_len + row_len_list[j] &lt;= self.chunk_size:\n                    curr_chunk.append(row_md_list[j])\n                    curr_len += row_len_list[j]\n                    j += 1\n\n                rows_in_chunk = j - i\n                chunk_str = header_lines + \"\\n\".join(curr_chunk)\n                chunks.append(chunk_str)\n                meta_per_chunk.append({\"rows\": list(range(i, j)), \"type\": \"char_row\"})\n\n                # --- compute overlap AFTER we know rows_in_chunk ---\n                if isinstance(self.chunk_overlap, float):\n                    overlap_rows = int(rows_in_chunk * self.chunk_overlap)\n                else:\n                    overlap_rows = int(self.chunk_overlap)\n\n                # make sure we don\u2019t loop forever\n                overlap_rows = min(overlap_rows, rows_in_chunk - 1)\n                i = j - overlap_rows\n\n        # Generate chunk_id\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n\n        # Return output\n        output = SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"row_column_splitter\",\n            split_params={\n                \"chunk_size\": self.chunk_size,\n                \"num_rows\": self.num_rows,\n                \"num_cols\": self.num_cols,\n                \"chunk_overlap\": self.chunk_overlap,\n            },\n            metadata={\"chunks\": meta_per_chunk},\n        )\n        return output\n\n    # Helper functions\n\n    def _get_overlap(self, base: int):\n        \"\"\"\n        Returns the overlap value as an integer, based on the configured chunk_overlap.\n\n        If chunk_overlap is a float in (0,1), computes the overlap as a percentage of `base`.\n        If chunk_overlap is an integer, returns it directly.\n\n        Args:\n            base (int): The base number (rows or columns) to compute the overlap from.\n        Returns:\n            int: The overlap as an integer.\n        \"\"\"\n        if isinstance(self.chunk_overlap, float):\n            return int(base * self.chunk_overlap)\n        return int(self.chunk_overlap)\n\n    def _load_tabular(self, reader_output: Dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"\n        Loads and parses the input tabular data from a Reader output dictionary\n        into a pandas DataFrame, based on its format.\n\n        If the input is empty, returns an empty DataFrame.\n        If the input is malformed (e.g., badly formatted markdown/CSV/TSV), a\n        pandas.errors.ParserError is raised.\n\n        Supports Markdown, CSV, TSV, TXT, and tabular JSON.\n\n        Args:\n            reader_output (Dict[str, Any]): Dictionary containing the text and conversion_method.\n\n        Returns:\n            pd.DataFrame: The loaded table as a DataFrame.\n\n        Raises:\n            pandas.errors.ParserError: If the input table is malformed and cannot be parsed.\n        \"\"\"\n        text = reader_output.text\n        # Return a void dataframe is a empty file is provided\n        if not text or not text.strip():\n            return pd.DataFrame()\n        method = reader_output.conversion_method\n        if method == \"markdown\":\n            return self._parse_markdown_table(text)\n        elif method == \"csv\" or method == \"txt\":\n            return pd.read_csv(io.StringIO(text))\n        elif method == \"tsv\":\n            return pd.read_csv(io.StringIO(text), sep=\"\\t\")\n        else:\n            # Try JSON\n            try:\n                js = json.loads(text)\n                if isinstance(js, list) and all(isinstance(row, dict) for row in js):\n                    return pd.DataFrame(js)\n                elif isinstance(js, dict):  # e.g., {col: [vals]}\n                    return pd.DataFrame(js)\n            except Exception:\n                pass\n            # Fallback: try CSV\n            return pd.read_csv(io.StringIO(text))\n\n    def _parse_markdown_table(self, md: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Parses a markdown table string into a pandas DataFrame.\n\n        Ignores non-table lines and trims markdown-specific formatting.\n        Also handles the separator line (---) in the header.\n\n        Args:\n            md (str): The markdown table as a string.\n\n        Returns:\n            pd.DataFrame: Parsed table as a DataFrame.\n\n        Raises:\n            pandas.errors.ParserError: If the markdown table is malformed and cannot be parsed.\n        \"\"\"\n        # Remove any lines not part of the table (e.g., text before/after)\n        table_lines = []\n        started = False\n        for line in md.splitlines():\n            if re.match(r\"^\\s*\\|.*\\|\\s*$\", line):\n                started = True\n                table_lines.append(line.strip())\n            elif started and not line.strip():\n                break  # stop at first blank line after table\n        table_md = \"\\n\".join(table_lines)\n        table_io = io.StringIO(\n            re.sub(\n                r\"^\\s*\\|\",\n                \"\",\n                re.sub(r\"\\|\\s*$\", \"\", table_md, flags=re.MULTILINE),\n                flags=re.MULTILINE,\n            )\n        )\n        try:\n            df = pd.read_csv(table_io, sep=\"|\").rename(\n                lambda x: x.strip(), axis=\"columns\"\n            )\n        except pd.errors.ParserError as e:\n            # Propagate the ParserError for your test to catch\n            raise pd.errors.ParserError(f\"Malformed markdown table: {e}\") from e\n        if not df.empty and all(re.match(r\"^-+$\", str(x).strip()) for x in df.iloc[0]):\n            df = df.drop(df.index[0]).reset_index(drop=True)\n        return df\n\n    def _to_str(self, df: pd.DataFrame, method: str, colwise: bool = False) -&gt; str:\n        \"\"\"\n        Converts a DataFrame chunk to a string for output,\n        either as a markdown table, CSV, or a list of columns.\n\n        Args:\n            df (pd.DataFrame): DataFrame chunk to convert.\n            method (str): Input file format (for output style).\n            colwise (bool): If True, output as a list of columns (used in column chunking).\n\n        Returns:\n            str: The chunk as a formatted string.\n        \"\"\"\n        if colwise:\n            # List of columns: output as a list of lists\n            return (\n                \"[\"\n                + \", \".join(  # noqa: W503\n                    [str([col] + df[col].tolist()) for col in df.columns]  # noqa: W503\n                )\n                + \"]\"  # noqa: W503\n            )\n        if method == \"markdown\" or \"md\":\n            # Use markdown table format\n            return df.to_markdown(index=False)\n        else:\n            # Default to CSV format\n            output = io.StringIO()\n            df.to_csv(output, index=False)\n            return output.getvalue().strip(\"\\n\")\n\n    @staticmethod\n    def _get_markdown_header(df):\n        \"\"\"\n        Returns the header and separator lines for a markdown table as a string.\n\n        Args:\n            df (pd.DataFrame): DataFrame representing the table.\n\n        Returns:\n            str: Markdown table header and separator (with trailing newline).\n        \"\"\"\n\n        lines = df.head(0).to_markdown(index=False).splitlines()\n        return \"\\n\".join(lines[:2]) + \"\\n\"\n\n    @staticmethod\n    def _get_markdown_row(df, row_idx):\n        \"\"\"\n        Returns a single row from the DataFrame formatted as a markdown table row.\n\n        Args:\n            df (pd.DataFrame): DataFrame containing the table.\n            row_idx (int): Index of the row to extract.\n\n        Returns:\n            str: The markdown-formatted row string.\n        \"\"\"\n        row = df.iloc[[row_idx]]\n        # Get the full markdown output (with header),\n        # extract only the last line (the data row)\n        md = row.to_markdown(index=False).splitlines()\n        return md[-1]\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.row_column_splitter.RowColumnSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Splits the input tabular data into multiple markdown table chunks according to the specified chunking strategy. Each output chunk is a complete markdown table with header, and will never cut a row in half. The overlap is always applied in terms of full rows or columns.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>Dict[str, Any]</code> <p>Dictionary output from a Reader, containing at least:     - 'text': The tabular data as string.     - 'conversion_method': Format of the input ('csv', 'tsv', 'markdown', 'json', etc.).     - Additional document metadata fields (optional).</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>Dataclass defining the output structure for all splitters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both num_rows and num_cols are set.</p> <code>ValueError</code> <p>If chunk_overlap as float is not in [0,1).</p> <code>ValueError</code> <p>If chunk_size is too small to fit the header and at least one data row.</p> Example <p><pre><code>reader_output = ReaderOutput(\n    text: '| id | name |\\n|----|------|\\n| 1  | A    |\\n| 2  | B    |\\n| 3  | C    |',\n    conversion_method: \"markdown\",\n    document_name: \"table.md\",\n    document_path: \"/path/table.md\",\n)\nsplitter = RowColumnSplitter(chunk_size=80, chunk_overlap=20)\noutput = splitter.split(reader_output)\nfor chunk in output[\"chunks\"]:\n    print(\"\\n\" + str(chunk) + \"\\n\")\n</code></pre> <pre><code>| id   | name   |\n|------|--------|\n|  1   | A      |\n|  2   | B      |\n\n| id   | name   |\n|------|--------|\n|  2   | B      |\n|  3   | C      |\n</code></pre></p> Source code in <code>src/splitter_mr/splitter/splitters/row_column_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Splits the input tabular data into multiple markdown table chunks according to the specified\n    chunking strategy. Each output chunk is a complete markdown table with header, and will never\n    cut a row in half. The overlap is always applied in terms of full rows or columns.\n\n    Args:\n        reader_output (Dict[str, Any]):\n            Dictionary output from a Reader, containing at least:\n                - 'text': The tabular data as string.\n                - 'conversion_method': Format of the input ('csv', 'tsv', 'markdown', 'json', etc.).\n                - Additional document metadata fields (optional).\n\n    Returns:\n        SplitterOutput: Dataclass defining the output structure for all splitters.\n\n    Raises:\n        ValueError: If both num_rows and num_cols are set.\n        ValueError: If chunk_overlap as float is not in [0,1).\n        ValueError: If chunk_size is too small to fit the header and at least one data row.\n\n    Example:\n        ```python\n        reader_output = ReaderOutput(\n            text: '| id | name |\\\\n|----|------|\\\\n| 1  | A    |\\\\n| 2  | B    |\\\\n| 3  | C    |',\n            conversion_method: \"markdown\",\n            document_name: \"table.md\",\n            document_path: \"/path/table.md\",\n        )\n        splitter = RowColumnSplitter(chunk_size=80, chunk_overlap=20)\n        output = splitter.split(reader_output)\n        for chunk in output[\"chunks\"]:\n            print(\"\\\\n\" + str(chunk) + \"\\\\n\")\n        ```\n        ```python\n        | id   | name   |\n        |------|--------|\n        |  1   | A      |\n        |  2   | B      |\n\n        | id   | name   |\n        |------|--------|\n        |  2   | B      |\n        |  3   | C      |\n        ```\n    \"\"\"\n    # Step 1. Parse the table depending on conversion_method\n    df = self._load_tabular(reader_output)\n    orig_method = reader_output.conversion_method\n    col_names = df.columns.tolist()\n\n    # Step 2. Split logic\n    chunks = []\n    meta_per_chunk = []\n\n    # If splitting strategy is by rows\n    if self.num_rows &gt; 0:\n        overlap = self._get_overlap(self.num_rows)\n        for i in range(\n            0,\n            len(df),\n            self.num_rows - overlap if (self.num_rows - overlap) &gt; 0 else 1,\n        ):\n            chunk_df = df.iloc[i : i + self.num_rows]\n            if not chunk_df.empty:\n                chunk_str = self._to_str(chunk_df, orig_method)\n                chunks.append(chunk_str)\n                meta_per_chunk.append(\n                    {\"rows\": chunk_df.index.tolist(), \"type\": \"row\"}\n                )\n    # If splitting strategy is by columns\n    elif self.num_cols &gt; 0:\n        overlap = self._get_overlap(self.num_cols)\n        total_cols = len(col_names)\n        for i in range(\n            0,\n            total_cols,\n            self.num_cols - overlap if (self.num_cols - overlap) &gt; 0 else 1,\n        ):\n            sel_cols = col_names[i : i + self.num_cols]\n            if sel_cols:\n                chunk_df = df[sel_cols]\n                chunk_str = self._to_str(chunk_df, orig_method, colwise=True)\n                chunks.append(chunk_str)\n                meta_per_chunk.append({\"cols\": sel_cols, \"type\": \"column\"})\n    # If splitting strategy is given by the chunk_size\n    else:\n        header_lines = self._get_markdown_header(df)\n        header_length = len(header_lines)\n\n        row_md_list = [self._get_markdown_row(df, i) for i in range(len(df))]\n        row_len_list = [len(r) + 1 for r in row_md_list]  # +1 for newline\n\n        if self.chunk_size &lt; header_length + row_len_list[0]:\n            raise ValueError(\n                \"chunk_size is too small to fit header and at least one row.\"\n            )\n\n        # Compute overlapping and headers in markdown tables\n        chunks = []\n        meta_per_chunk = []\n        i = 0\n        n = len(row_md_list)\n        overlap = self._get_overlap(1)\n        while i &lt; n:\n            curr_chunk = []\n            curr_len = header_length\n            j = i\n            while j &lt; n and curr_len + row_len_list[j] &lt;= self.chunk_size:\n                curr_chunk.append(row_md_list[j])\n                curr_len += row_len_list[j]\n                j += 1\n\n            rows_in_chunk = j - i\n            chunk_str = header_lines + \"\\n\".join(curr_chunk)\n            chunks.append(chunk_str)\n            meta_per_chunk.append({\"rows\": list(range(i, j)), \"type\": \"char_row\"})\n\n            # --- compute overlap AFTER we know rows_in_chunk ---\n            if isinstance(self.chunk_overlap, float):\n                overlap_rows = int(rows_in_chunk * self.chunk_overlap)\n            else:\n                overlap_rows = int(self.chunk_overlap)\n\n            # make sure we don\u2019t loop forever\n            overlap_rows = min(overlap_rows, rows_in_chunk - 1)\n            i = j - overlap_rows\n\n    # Generate chunk_id\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n\n    # Return output\n    output = SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"row_column_splitter\",\n        split_params={\n            \"chunk_size\": self.chunk_size,\n            \"num_rows\": self.num_rows,\n            \"num_cols\": self.num_cols,\n            \"chunk_overlap\": self.chunk_overlap,\n        },\n        metadata={\"chunks\": meta_per_chunk},\n    )\n    return output\n</code></pre>"},{"location":"api_reference/splitter/#codesplitter","title":"CodeSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.code_splitter.CodeSplitter","title":"<code>CodeSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>CodeSplitter recursively splits source code into programmatically meaningful chunks (functions, classes, methods, etc.) for the given programming language.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Maximum chunk size, in characters.</p> <code>1000</code> <code>language</code> <code>str</code> <p>Programming language (e.g., \"python\", \"java\", \"kotlin\", etc.)</p> <code>'python'</code> Notes <ul> <li>Uses Langchain's RecursiveCharacterTextSplitter and its language-aware <code>from_language</code> method.</li> <li>See Langchain docs: https://python.langchain.com/docs/how_to/code_splitter/</li> </ul> Source code in <code>src/splitter_mr/splitter/splitters/code_splitter.py</code> <pre><code>class CodeSplitter(BaseSplitter):\n    \"\"\"\n    CodeSplitter recursively splits source code into programmatically meaningful chunks\n    (functions, classes, methods, etc.) for the given programming language.\n\n    Args:\n        chunk_size (int): Maximum chunk size, in characters.\n        language (str): Programming language (e.g., \"python\", \"java\", \"kotlin\", etc.)\n\n    Notes:\n        - Uses Langchain's RecursiveCharacterTextSplitter and its language-aware `from_language` method.\n        - See Langchain docs: https://python.langchain.com/docs/how_to/code_splitter/\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 1000,\n        language: str = \"python\",\n    ):\n        super().__init__(chunk_size)\n        self.language = language\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Splits code in `reader_output['text']` according to the syntax of the specified\n        programming language, using function/class boundaries where possible.\n\n        Args:\n            reader_output (ReaderOutput): Object containing at least a 'text' field,\n                plus optional document metadata.\n\n        Returns:\n            SplitterOutput: Dataclass defining the output structure for all splitters.\n\n        Raises:\n            ValueError: If language is not supported.\n\n        Example:\n            ```python\n            from splitter_mr.splitter import CodeSplitter\n\n            reader_output = ReaderOutput(\n                text: \"def foo():\\\\n    pass\\\\n\\\\nclass Bar:\\\\n    def baz(self):\\\\n        pass\",\n                document_name: \"example.py\",\n                document_path: \"/tmp/example.py\"\n            )\n            splitter = CodeSplitter(chunk_size=50, language=\"python\")\n            output = splitter.split(reader_output)\n            print(output.chunks)\n            ```\n            ```python\n            ['def foo():\\\\n    pass\\\\n', 'class Bar:\\\\n    def baz(self):\\\\n        pass']\n            ```\n        \"\"\"\n        # Initialize variables\n        text = reader_output.text\n        chunk_size = self.chunk_size\n\n        # Get Langchain language enum\n        lang_enum = get_langchain_language(self.language)\n\n        splitter = RecursiveCharacterTextSplitter.from_language(\n            language=lang_enum, chunk_size=chunk_size, chunk_overlap=0\n        )\n        texts = splitter.create_documents([text])\n        chunks = [doc.page_content for doc in texts]\n\n        # Generate chunk_id and append metadata\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n        metadata = self._default_metadata()\n\n        # Return output\n        output = SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"code_splitter\",\n            split_params={\"chunk_size\": chunk_size, \"language\": self.language},\n            metadata=metadata,\n        )\n        return output\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.code_splitter.CodeSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Splits code in <code>reader_output['text']</code> according to the syntax of the specified programming language, using function/class boundaries where possible.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>ReaderOutput</code> <p>Object containing at least a 'text' field, plus optional document metadata.</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>Dataclass defining the output structure for all splitters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If language is not supported.</p> Example <p><pre><code>from splitter_mr.splitter import CodeSplitter\n\nreader_output = ReaderOutput(\n    text: \"def foo():\\n    pass\\n\\nclass Bar:\\n    def baz(self):\\n        pass\",\n    document_name: \"example.py\",\n    document_path: \"/tmp/example.py\"\n)\nsplitter = CodeSplitter(chunk_size=50, language=\"python\")\noutput = splitter.split(reader_output)\nprint(output.chunks)\n</code></pre> <pre><code>['def foo():\\n    pass\\n', 'class Bar:\\n    def baz(self):\\n        pass']\n</code></pre></p> Source code in <code>src/splitter_mr/splitter/splitters/code_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Splits code in `reader_output['text']` according to the syntax of the specified\n    programming language, using function/class boundaries where possible.\n\n    Args:\n        reader_output (ReaderOutput): Object containing at least a 'text' field,\n            plus optional document metadata.\n\n    Returns:\n        SplitterOutput: Dataclass defining the output structure for all splitters.\n\n    Raises:\n        ValueError: If language is not supported.\n\n    Example:\n        ```python\n        from splitter_mr.splitter import CodeSplitter\n\n        reader_output = ReaderOutput(\n            text: \"def foo():\\\\n    pass\\\\n\\\\nclass Bar:\\\\n    def baz(self):\\\\n        pass\",\n            document_name: \"example.py\",\n            document_path: \"/tmp/example.py\"\n        )\n        splitter = CodeSplitter(chunk_size=50, language=\"python\")\n        output = splitter.split(reader_output)\n        print(output.chunks)\n        ```\n        ```python\n        ['def foo():\\\\n    pass\\\\n', 'class Bar:\\\\n    def baz(self):\\\\n        pass']\n        ```\n    \"\"\"\n    # Initialize variables\n    text = reader_output.text\n    chunk_size = self.chunk_size\n\n    # Get Langchain language enum\n    lang_enum = get_langchain_language(self.language)\n\n    splitter = RecursiveCharacterTextSplitter.from_language(\n        language=lang_enum, chunk_size=chunk_size, chunk_overlap=0\n    )\n    texts = splitter.create_documents([text])\n    chunks = [doc.page_content for doc in texts]\n\n    # Generate chunk_id and append metadata\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n    metadata = self._default_metadata()\n\n    # Return output\n    output = SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"code_splitter\",\n        split_params={\"chunk_size\": chunk_size, \"language\": self.language},\n        metadata=metadata,\n    )\n    return output\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.code_splitter.get_langchain_language","title":"<code>get_langchain_language(lang_str)</code>","text":"<p>Map a string language name to Langchain Language enum. Raises ValueError if not found.</p> Source code in <code>src/splitter_mr/splitter/splitters/code_splitter.py</code> <pre><code>def get_langchain_language(lang_str: str) -&gt; Language:\n    \"\"\"\n    Map a string language name to Langchain Language enum.\n    Raises ValueError if not found.\n    \"\"\"\n    lookup = {lang.name.lower(): lang for lang in Language}\n    key = lang_str.lower()\n    if key not in lookup:\n        raise ValueError(\n            f\"Unsupported language '{lang_str}'. Supported: {list(lookup.keys())}\"\n        )\n    return lookup[key]\n</code></pre>"},{"location":"api_reference/splitter/#tokensplitter","title":"TokenSplitter","text":""},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.token_splitter.TokenSplitter","title":"<code>TokenSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>TokenSplitter splits a given text into chunks based on token counts derived from different tokenization models or libraries.</p> <p>This splitter supports tokenization via <code>tiktoken</code> (OpenAI tokenizer), <code>spacy</code> (spaCy tokenizer), and <code>nltk</code> (NLTK tokenizer). It allows splitting text into chunks of a maximum number of tokens (<code>chunk_size</code>), using the specified tokenizer model.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Maximum number of tokens per chunk.</p> <code>1000</code> <code>model_name</code> <code>str</code> <p>Specifies the tokenizer and model in the format <code>tokenizer/model</code>. Supported tokenizers are:</p> <ul> <li><code>tiktoken/cl100k_base</code> (OpenAI tokenizer via tiktoken)</li> <li><code>spacy/en_core_web_sm</code> (spaCy English model)</li> <li><code>nltk/punkt_tab</code> (NLTK Punkt tokenizer variant)</li> </ul> <code>DEFAULT_TOKENIZER</code> <code>language</code> <code>str</code> <p>Language code for NLTK tokenizer (default <code>\"english\"</code>).</p> <code>DEFAULT_TOKEN_LANGUAGE</code> Notes <p>More info about the splitting methods by Tokens for Langchain: Langchain Docs.</p> Source code in <code>src/splitter_mr/splitter/splitters/token_splitter.py</code> <pre><code>class TokenSplitter(BaseSplitter):\n    \"\"\"\n    TokenSplitter splits a given text into chunks based on token counts\n    derived from different tokenization models or libraries.\n\n    This splitter supports tokenization via `tiktoken` (OpenAI tokenizer),\n    `spacy` (spaCy tokenizer), and `nltk` (NLTK tokenizer). It allows splitting\n    text into chunks of a maximum number of tokens (`chunk_size`), using the\n    specified tokenizer model.\n\n    Args:\n        chunk_size (int): Maximum number of tokens per chunk.\n        model_name (str): Specifies the tokenizer and model in the format `tokenizer/model`. Supported tokenizers are:\n\n            - `tiktoken/cl100k_base` (OpenAI tokenizer via tiktoken)\n            - `spacy/en_core_web_sm` (spaCy English model)\n            - `nltk/punkt_tab` (NLTK Punkt tokenizer variant)\n\n        language (str): Language code for NLTK tokenizer (default `\"english\"`).\n\n    Notes:\n        More info about the splitting methods by Tokens for Langchain:\n        [Langchain Docs](https://python.langchain.com/docs/how_to/split_by_token/).\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 1000,\n        model_name: str = DEFAULT_TOKENIZER,\n        language: str = DEFAULT_TOKEN_LANGUAGE,\n    ):\n        super().__init__(chunk_size)\n        # Use centralized defaults (already applied via signature) and keep on instance\n        self.model_name = model_name or DEFAULT_TOKENIZER\n        self.language = language or DEFAULT_TOKEN_LANGUAGE\n\n    @staticmethod\n    def list_nltk_punkt_languages():\n        \"\"\"Return a sorted list of available punkt models (languages) for NLTK.\"\"\"\n        models = set()\n        for base in map(Path, nltk.data.path):\n            punkt_dir = base / \"tokenizers\" / \"punkt\"\n            if punkt_dir.exists():\n                models.update(f.stem for f in punkt_dir.glob(\"*.pickle\"))\n        return sorted(models)\n\n    def _parse_model(self) -&gt; tuple[str, str]:\n        \"\"\"Parse `tokenizer/model` and validate the format.\"\"\"\n        if \"/\" not in self.model_name:\n            raise ValueError(\n                \"model_name must be in the format 'tokenizer/model', \"\n                f\"e.g. '{DEFAULT_TOKENIZER}'.\"\n            )\n        tokenizer, model = self.model_name.split(\"/\", 1)\n        return tokenizer, model\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Splits the input text from `reader_output` into token-based chunks using\n        the specified tokenizer.\n\n        Depending on `model_name`, the splitter chooses the appropriate tokenizer:\n\n        - For `tiktoken`, uses `RecursiveCharacterTextSplitter` with tiktoken encoding.\n            e.g.: `tiktoken/cl100k_base`.\n        - For `spacy`, uses `SpacyTextSplitter` with the specified spaCy pipeline.\n            e.g., `spacy/en_core_web_sm`.\n        - For `nltk`, uses `NLTKTextSplitter` with the specified language tokenizer.\n            e.g., `nltk/punkt_tab`.\n\n        Automatically downloads spaCy and NLTK models if missing.\n\n        Args:\n            reader_output (Dict[str, Any]):\n                Dictionary containing at least a 'text' key (str) and optional document metadata,\n                such as 'document_name', 'document_path', 'document_id', etc.\n\n        Returns:\n            SplitterOutput: Dataclass defining the output structure for all splitters.\n\n        Raises:\n            RuntimeError: If a spaCy model specified in `model_name` is not available.\n            ValueError: If an unsupported tokenizer is specified in `model_name`.\n        \"\"\"\n        text = reader_output.text\n        tokenizer, model = self._parse_model()\n\n        if tokenizer == \"tiktoken\":\n            # Validate against installed tiktoken encodings; hint with our common defaults\n            available_models = tiktoken.list_encoding_names()\n            if model not in available_models:\n                raise ValueError(\n                    f\"tiktoken encoding '{model}' is not available. \"\n                    f\"Available encodings include (subset): {TIKTOKEN_DEFAULTS}. \"\n                    f\"Full list from tiktoken: {available_models}\"\n                )\n            splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n                encoding_name=model,\n                chunk_size=self.chunk_size,\n                chunk_overlap=0,\n            )\n\n        elif tokenizer == \"spacy\":\n            if not spacy.util.is_package(model):\n                # Try to download; we surface our recommended list in the error if it fails\n                try:\n                    spacy.cli.download(model)\n                except Exception as e:\n                    raise RuntimeError(\n                        f\"spaCy model '{model}' is not available for download. \"\n                        f\"Common models include: {SPACY_DEFAULTS}\"\n                    ) from e\n            spacy.load(model)\n            MAX_SAFE_LENGTH = 1_000_000\n            if self.chunk_size &gt; MAX_SAFE_LENGTH:\n                warnings.warn(\n                    \"Too many characters: the v2.x parser and NER models require roughly \"\n                    \"1GB of temporary memory per 100,000 characters in the input\",\n                    UserWarning,\n                )\n            splitter = SpacyTextSplitter(\n                chunk_size=self.chunk_size,\n                chunk_overlap=0,\n                max_length=MAX_SAFE_LENGTH,\n                pipeline=model,\n            )\n\n        elif tokenizer == \"nltk\":\n            # Ensure punkt language is present; download our specified default model if missing\n            try:\n                nltk.data.find(f\"tokenizers/punkt/{self.language}.pickle\")\n            except LookupError:\n                # Use constants instead of hard-coded 'punkt_tab'\n                nltk.download(NLTK_DEFAULTS[0])\n            splitter = NLTKTextSplitter(\n                chunk_size=self.chunk_size,\n                chunk_overlap=0,\n                language=self.language,\n            )\n\n        else:\n            raise ValueError(\n                f\"Unsupported tokenizer '{tokenizer}'. Supported tokenizers: {SUPPORTED_TOKENIZERS}\"\n            )\n\n        chunks = splitter.split_text(text)\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n        metadata = self._default_metadata()\n\n        return SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"token_splitter\",\n            split_params={\n                \"chunk_size\": self.chunk_size,\n                \"model_name\": self.model_name,  # keeps centralized default visible\n                \"language\": self.language,  # keeps centralized default visible\n            },\n            metadata=metadata,\n        )\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.token_splitter.TokenSplitter.list_nltk_punkt_languages","title":"<code>list_nltk_punkt_languages()</code>  <code>staticmethod</code>","text":"<p>Return a sorted list of available punkt models (languages) for NLTK.</p> Source code in <code>src/splitter_mr/splitter/splitters/token_splitter.py</code> <pre><code>@staticmethod\ndef list_nltk_punkt_languages():\n    \"\"\"Return a sorted list of available punkt models (languages) for NLTK.\"\"\"\n    models = set()\n    for base in map(Path, nltk.data.path):\n        punkt_dir = base / \"tokenizers\" / \"punkt\"\n        if punkt_dir.exists():\n            models.update(f.stem for f in punkt_dir.glob(\"*.pickle\"))\n    return sorted(models)\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.token_splitter.TokenSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Splits the input text from <code>reader_output</code> into token-based chunks using the specified tokenizer.</p> <p>Depending on <code>model_name</code>, the splitter chooses the appropriate tokenizer:</p> <ul> <li>For <code>tiktoken</code>, uses <code>RecursiveCharacterTextSplitter</code> with tiktoken encoding.     e.g.: <code>tiktoken/cl100k_base</code>.</li> <li>For <code>spacy</code>, uses <code>SpacyTextSplitter</code> with the specified spaCy pipeline.     e.g., <code>spacy/en_core_web_sm</code>.</li> <li>For <code>nltk</code>, uses <code>NLTKTextSplitter</code> with the specified language tokenizer.     e.g., <code>nltk/punkt_tab</code>.</li> </ul> <p>Automatically downloads spaCy and NLTK models if missing.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>Dict[str, Any]</code> <p>Dictionary containing at least a 'text' key (str) and optional document metadata, such as 'document_name', 'document_path', 'document_id', etc.</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>Dataclass defining the output structure for all splitters.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a spaCy model specified in <code>model_name</code> is not available.</p> <code>ValueError</code> <p>If an unsupported tokenizer is specified in <code>model_name</code>.</p> Source code in <code>src/splitter_mr/splitter/splitters/token_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Splits the input text from `reader_output` into token-based chunks using\n    the specified tokenizer.\n\n    Depending on `model_name`, the splitter chooses the appropriate tokenizer:\n\n    - For `tiktoken`, uses `RecursiveCharacterTextSplitter` with tiktoken encoding.\n        e.g.: `tiktoken/cl100k_base`.\n    - For `spacy`, uses `SpacyTextSplitter` with the specified spaCy pipeline.\n        e.g., `spacy/en_core_web_sm`.\n    - For `nltk`, uses `NLTKTextSplitter` with the specified language tokenizer.\n        e.g., `nltk/punkt_tab`.\n\n    Automatically downloads spaCy and NLTK models if missing.\n\n    Args:\n        reader_output (Dict[str, Any]):\n            Dictionary containing at least a 'text' key (str) and optional document metadata,\n            such as 'document_name', 'document_path', 'document_id', etc.\n\n    Returns:\n        SplitterOutput: Dataclass defining the output structure for all splitters.\n\n    Raises:\n        RuntimeError: If a spaCy model specified in `model_name` is not available.\n        ValueError: If an unsupported tokenizer is specified in `model_name`.\n    \"\"\"\n    text = reader_output.text\n    tokenizer, model = self._parse_model()\n\n    if tokenizer == \"tiktoken\":\n        # Validate against installed tiktoken encodings; hint with our common defaults\n        available_models = tiktoken.list_encoding_names()\n        if model not in available_models:\n            raise ValueError(\n                f\"tiktoken encoding '{model}' is not available. \"\n                f\"Available encodings include (subset): {TIKTOKEN_DEFAULTS}. \"\n                f\"Full list from tiktoken: {available_models}\"\n            )\n        splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            encoding_name=model,\n            chunk_size=self.chunk_size,\n            chunk_overlap=0,\n        )\n\n    elif tokenizer == \"spacy\":\n        if not spacy.util.is_package(model):\n            # Try to download; we surface our recommended list in the error if it fails\n            try:\n                spacy.cli.download(model)\n            except Exception as e:\n                raise RuntimeError(\n                    f\"spaCy model '{model}' is not available for download. \"\n                    f\"Common models include: {SPACY_DEFAULTS}\"\n                ) from e\n        spacy.load(model)\n        MAX_SAFE_LENGTH = 1_000_000\n        if self.chunk_size &gt; MAX_SAFE_LENGTH:\n            warnings.warn(\n                \"Too many characters: the v2.x parser and NER models require roughly \"\n                \"1GB of temporary memory per 100,000 characters in the input\",\n                UserWarning,\n            )\n        splitter = SpacyTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=0,\n            max_length=MAX_SAFE_LENGTH,\n            pipeline=model,\n        )\n\n    elif tokenizer == \"nltk\":\n        # Ensure punkt language is present; download our specified default model if missing\n        try:\n            nltk.data.find(f\"tokenizers/punkt/{self.language}.pickle\")\n        except LookupError:\n            # Use constants instead of hard-coded 'punkt_tab'\n            nltk.download(NLTK_DEFAULTS[0])\n        splitter = NLTKTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=0,\n            language=self.language,\n        )\n\n    else:\n        raise ValueError(\n            f\"Unsupported tokenizer '{tokenizer}'. Supported tokenizers: {SUPPORTED_TOKENIZERS}\"\n        )\n\n    chunks = splitter.split_text(text)\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n    metadata = self._default_metadata()\n\n    return SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"token_splitter\",\n        split_params={\n            \"chunk_size\": self.chunk_size,\n            \"model_name\": self.model_name,  # keeps centralized default visible\n            \"language\": self.language,  # keeps centralized default visible\n        },\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api_reference/splitter/#pagedsplitter","title":"PagedSplitter","text":"<p>Splits text by pages for documents that have page structure. Each chunk contains a specified number of pages, with optional word overlap.</p>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.paged_splitter.PagedSplitter","title":"<code>PagedSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Splits a multi-page document into page-based or multi-page chunks using a placeholder marker.</p> <p>Supports overlap in characters between consecutive chunks.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of pages per chunk.</p> <code>1</code> <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping characters to include from the end of the previous chunk.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If chunk_size is less than 1.</p> Source code in <code>src/splitter_mr/splitter/splitters/paged_splitter.py</code> <pre><code>class PagedSplitter(BaseSplitter):\n    \"\"\"\n    Splits a multi-page document into page-based or multi-page chunks using a placeholder marker.\n\n    Supports overlap in characters between consecutive chunks.\n\n    Args:\n        chunk_size (int): Number of pages per chunk.\n        chunk_overlap (int): Number of overlapping characters to include from the end of the previous chunk.\n\n    Raises:\n        ValueError: If chunk_size is less than 1.\n    \"\"\"\n\n    def __init__(self, chunk_size: int = 1, chunk_overlap: int = 0):\n        \"\"\"\n        Args:\n            chunk_size (int): Number of pages per chunk.\n            chunk_overlap (int): Number of overlapping characters to include from the end of the previous chunk.\n        \"\"\"\n        if chunk_size &lt; 1:\n            raise ValueError(\"chunk_size must be \u2265\u202f1\")\n        if chunk_overlap &lt; 0:\n            raise ValueError(\"chunk_overlap must be \u2265\u202f0\")\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"\n        Splits the input text into chunks using the page_placeholder in the ReaderOutput.\n        Optionally adds character overlap between chunks.\n\n        Args:\n            reader_output (ReaderOutput): The output from a reader containing text and metadata.\n\n        Returns:\n            SplitterOutput: The result with chunks and related metadata.\n\n        Raises:\n            ValueError: If the reader_output does not contain a valid page_placeholder.\n\n        Example:\n            ```python\n            from splitter_mr.splitter import PagedSplitter\n\n            reader_output = ReaderOutput(\n                text: \"&lt;!-- page --&gt; Page 1 &lt;!-- page --&gt; This is the page 2.\",\n                document_name: \"test.md\",\n                document_path: \"tmp/test.md\",\n                page_placeholder: \"&lt;!-- page --&gt;\",\n                ...\n            )\n            splitter = PagedSplitter(chunk_size = 1)\n            output = splitter.split(reader_output)\n            print(output[\"chunks\"])\n            ```\n            ```python\n            [\" Page 1 \", \" This is the page 2.\"]\n            ```\n        \"\"\"\n        page_placeholder: str = reader_output.page_placeholder\n\n        if not bool(page_placeholder):\n            raise ValueError(\n                \"The specified file does not contain page placeholders. \"\n                \"Please, use a compatible file extension (pdf, docx, xlsx, pptx) \"\n                \"or read the file using any BaseReader by pages and try again\"\n            )\n\n        # Split the document into pages using the placeholder.\n        pages: List[str] = [\n            page.strip()  # Normalize spacing\n            for page in reader_output.text.split(page_placeholder)\n            if page.strip()\n        ]\n\n        chunks: List[str] = []\n        for i in range(0, len(pages), self.chunk_size):\n            chunk = \"\\n\".join(pages[i : i + self.chunk_size])\n            if self.chunk_overlap &gt; 0 and i &gt; 0 and chunks:\n                # Add character overlap from previous chunk\n                overlap_text = chunks[-1][-self.chunk_overlap :]\n                chunk = overlap_text + chunk\n            chunks.append(chunk)\n\n        # Generate chunk_id and append metadata\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n        metadata = self._default_metadata()\n\n        output = SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"paged_splitter\",\n            split_params={\n                \"chunk_size\": self.chunk_size,\n                \"chunk_overlap\": self.chunk_overlap,\n            },\n            metadata=metadata,\n        )\n        return output\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.paged_splitter.PagedSplitter.__init__","title":"<code>__init__(chunk_size=1, chunk_overlap=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of pages per chunk.</p> <code>1</code> <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping characters to include from the end of the previous chunk.</p> <code>0</code> Source code in <code>src/splitter_mr/splitter/splitters/paged_splitter.py</code> <pre><code>def __init__(self, chunk_size: int = 1, chunk_overlap: int = 0):\n    \"\"\"\n    Args:\n        chunk_size (int): Number of pages per chunk.\n        chunk_overlap (int): Number of overlapping characters to include from the end of the previous chunk.\n    \"\"\"\n    if chunk_size &lt; 1:\n        raise ValueError(\"chunk_size must be \u2265\u202f1\")\n    if chunk_overlap &lt; 0:\n        raise ValueError(\"chunk_overlap must be \u2265\u202f0\")\n    self.chunk_size = chunk_size\n    self.chunk_overlap = chunk_overlap\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.paged_splitter.PagedSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Splits the input text into chunks using the page_placeholder in the ReaderOutput. Optionally adds character overlap between chunks.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>ReaderOutput</code> <p>The output from a reader containing text and metadata.</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>The result with chunks and related metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reader_output does not contain a valid page_placeholder.</p> Example <p><pre><code>from splitter_mr.splitter import PagedSplitter\n\nreader_output = ReaderOutput(\n    text: \"&lt;!-- page --&gt; Page 1 &lt;!-- page --&gt; This is the page 2.\",\n    document_name: \"test.md\",\n    document_path: \"tmp/test.md\",\n    page_placeholder: \"&lt;!-- page --&gt;\",\n    ...\n)\nsplitter = PagedSplitter(chunk_size = 1)\noutput = splitter.split(reader_output)\nprint(output[\"chunks\"])\n</code></pre> <pre><code>[\" Page 1 \", \" This is the page 2.\"]\n</code></pre></p> Source code in <code>src/splitter_mr/splitter/splitters/paged_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"\n    Splits the input text into chunks using the page_placeholder in the ReaderOutput.\n    Optionally adds character overlap between chunks.\n\n    Args:\n        reader_output (ReaderOutput): The output from a reader containing text and metadata.\n\n    Returns:\n        SplitterOutput: The result with chunks and related metadata.\n\n    Raises:\n        ValueError: If the reader_output does not contain a valid page_placeholder.\n\n    Example:\n        ```python\n        from splitter_mr.splitter import PagedSplitter\n\n        reader_output = ReaderOutput(\n            text: \"&lt;!-- page --&gt; Page 1 &lt;!-- page --&gt; This is the page 2.\",\n            document_name: \"test.md\",\n            document_path: \"tmp/test.md\",\n            page_placeholder: \"&lt;!-- page --&gt;\",\n            ...\n        )\n        splitter = PagedSplitter(chunk_size = 1)\n        output = splitter.split(reader_output)\n        print(output[\"chunks\"])\n        ```\n        ```python\n        [\" Page 1 \", \" This is the page 2.\"]\n        ```\n    \"\"\"\n    page_placeholder: str = reader_output.page_placeholder\n\n    if not bool(page_placeholder):\n        raise ValueError(\n            \"The specified file does not contain page placeholders. \"\n            \"Please, use a compatible file extension (pdf, docx, xlsx, pptx) \"\n            \"or read the file using any BaseReader by pages and try again\"\n        )\n\n    # Split the document into pages using the placeholder.\n    pages: List[str] = [\n        page.strip()  # Normalize spacing\n        for page in reader_output.text.split(page_placeholder)\n        if page.strip()\n    ]\n\n    chunks: List[str] = []\n    for i in range(0, len(pages), self.chunk_size):\n        chunk = \"\\n\".join(pages[i : i + self.chunk_size])\n        if self.chunk_overlap &gt; 0 and i &gt; 0 and chunks:\n            # Add character overlap from previous chunk\n            overlap_text = chunks[-1][-self.chunk_overlap :]\n            chunk = overlap_text + chunk\n        chunks.append(chunk)\n\n    # Generate chunk_id and append metadata\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n    metadata = self._default_metadata()\n\n    output = SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"paged_splitter\",\n        split_params={\n            \"chunk_size\": self.chunk_size,\n            \"chunk_overlap\": self.chunk_overlap,\n        },\n        metadata=metadata,\n    )\n    return output\n</code></pre>"},{"location":"api_reference/splitter/#semanticsplitter","title":"SemanticSplitter","text":"<p>Splits text into chunks based on semantic similarity, using an embedding model and a max tokens parameter. Useful for meaningful semantic groupings.</p>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.semantic_splitter.SemanticSplitter","title":"<code>SemanticSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Split text into semantically coherent chunks using embedding similarity.</p> <p>Pipeline:</p> <ul> <li>Split text into sentences via <code>SentenceSplitter</code> (one sentence chunks).</li> <li>Build a sliding window around each sentence (<code>buffer_size</code>).</li> <li>Embed each window with <code>BaseEmbedding</code> (batched).</li> <li>Compute cosine distances between consecutive windows (1 - cosine_sim).</li> <li>Pick breakpoints using a thresholding strategy, or aim for <code>number_of_chunks</code>.</li> <li>Join sentences between breakpoints; enforce minimum size via <code>chunk_size</code>.</li> </ul> Source code in <code>src/splitter_mr/splitter/splitters/semantic_splitter.py</code> <pre><code>class SemanticSplitter(BaseSplitter):\n    \"\"\"\n    Split text into semantically coherent chunks using embedding similarity.\n\n    **Pipeline:**\n\n    - Split text into sentences via `SentenceSplitter` (one sentence chunks).\n    - Build a sliding window around each sentence (`buffer_size`).\n    - Embed each window with `BaseEmbedding` (batched).\n    - Compute cosine *distances* between consecutive windows (1 - cosine_sim).\n    - Pick breakpoints using a thresholding strategy, or aim for `number_of_chunks`.\n    - Join sentences between breakpoints; enforce minimum size via `chunk_size`.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding: BaseEmbedding,\n        *,\n        buffer_size: int = 1,\n        breakpoint_threshold_type: BreakpointThresholdType = \"percentile\",\n        breakpoint_threshold_amount: Optional[float] = None,\n        number_of_chunks: Optional[int] = None,\n        chunk_size: int = 1000,\n    ) -&gt; None:\n        \"\"\"Initialize the semantic splitter.\n\n        Args:\n            embedding (BaseEmbedding): Embedding backend.\n            buffer_size (int): Neighbor window size around each sentence.\n            breakpoint_threshold_type (BreakpointThresholdType): Threshold strategy:\n                \"percentile\" | \"standard_deviation\" | \"interquartile\" | \"gradient\".\n            breakpoint_threshold_amount (Optional[float]): Threshold parameter. If None,\n                uses sensible defaults per strategy (e.g., 95th percentile).\n            number_of_chunks (Optional[int]): If set, pick a threshold that\n                approximately yields this number of chunks (inverse percentile).\n            chunk_size (int): **Minimum** characters required to emit a chunk.\n        \"\"\"\n        super().__init__(chunk_size=chunk_size)\n        self.embedding = embedding\n        self.buffer_size = int(buffer_size)\n        self.breakpoint_threshold_type = cast(\n            BreakpointThresholdType, breakpoint_threshold_type\n        )\n        self.breakpoint_threshold_amount = (\n            DEFAULT_BREAKPOINTS[self.breakpoint_threshold_type]\n            if breakpoint_threshold_amount is None\n            else float(breakpoint_threshold_amount)\n        )\n        self.number_of_chunks = number_of_chunks\n        self._sentence_splitter = SentenceSplitter(\n            chunk_size=1, chunk_overlap=0, separators=[\".\", \"!\", \"?\"]\n        )\n\n    # ---------- Helpers ----------\n\n    def _split_into_sentences(self, reader_output: ReaderOutput) -&gt; List[str]:\n        \"\"\"Split the input text into sentences using `SentenceSplitter` (no overlap).\n\n        Args:\n            reader_output (ReaderOutput): The document to split.\n\n        Returns:\n            List[str]: List of sentences preserving punctuation.\n        \"\"\"\n        sent_out = self._sentence_splitter.split(reader_output)\n        return sent_out.chunks\n\n    def _calculate_sentence_distances(\n        self, single_sentences: List[str]\n    ) -&gt; Tuple[List[float], List[Dict[str, Any]]]:\n        \"\"\"Embed sentence windows (batch) and compute consecutive cosine distances.\n\n        Args:\n            single_sentences (List[str]): Sentences in order.\n\n        Returns:\n            Tuple[List[float], List[Dict[str, Any]]]:\n                - distances between consecutive windows (len = n-1)\n                - sentence dicts enriched with combined text and embeddings\n        \"\"\"\n        # Prepare sentence dicts and combine with buffer\n        sentences = [\n            {\"sentence\": s, \"index\": i} for i, s in enumerate(single_sentences)\n        ]\n        sentences = _combine_sentences(sentences, self.buffer_size)\n\n        # Batch embed all combined sentences\n        windows = [item[\"combined_sentence\"] for item in sentences]\n        embeddings = self.embedding.embed_documents(windows)\n\n        for item, emb in zip(sentences, embeddings):\n            item[\"combined_sentence_embedding\"] = emb\n\n        # Distances (1 - cosine similarity) between consecutive windows\n        n = len(sentences)\n        if n &lt;= 1:\n            return [], sentences\n\n        distances: List[float] = []\n        for i in range(n - 1):\n            sim = _cosine_similaritynp(\n                sentences[i][\"combined_sentence_embedding\"],\n                sentences[i + 1][\"combined_sentence_embedding\"],\n            )\n            dist = 1.0 - sim\n            distances.append(dist)\n            sentences[i][\"distance_to_next\"] = dist\n\n        return distances, sentences\n\n    def _threshold_from_clusters(self, distances: List[float]) -&gt; float:\n        \"\"\"Estimate a percentile threshold to reach `number_of_chunks`.\n\n        Maps desired chunks x\u2208[1, len(distances)] to percentile y\u2208[100, 0].\n\n        Args:\n            distances (List[float]): Consecutive distances.\n\n        Returns:\n            float: Threshold value as a percentile over `distances`.\n        \"\"\"\n        assert self.number_of_chunks is not None\n        x1, y1 = float(len(distances)), 0.0\n        x2, y2 = 1.0, 100.0\n        x = max(min(float(self.number_of_chunks), x1), x2)\n        y = y1 + ((y2 - y1) / (x2 - x1)) * (x - x1) if x2 != x1 else y2\n        y = float(np.clip(y, 0.0, 100.0))\n        return float(np.percentile(distances, y)) if distances else 0.0\n\n    def _calculate_breakpoint_threshold(\n        self, distances: List[float]\n    ) -&gt; Tuple[float, List[float]]:\n        \"\"\"Compute the breakpoint threshold and reference array per selected strategy.\n\n        Args:\n            distances (List[float]): Consecutive distances between windows.\n\n        Returns:\n            Tuple[float, List[float]]: (threshold, reference_array)\n                If strategy == \"gradient\", reference_array is the gradient;\n                otherwise it's `distances`.\n        \"\"\"\n        if not distances:\n            return 0.0, distances\n\n        if self.breakpoint_threshold_type == \"percentile\":\n            return (\n                float(np.percentile(distances, self.breakpoint_threshold_amount)),\n                distances,\n            )\n\n        if self.breakpoint_threshold_type == \"standard_deviation\":\n            mu = float(np.mean(distances))\n            sd = float(np.std(distances))\n            return mu + self.breakpoint_threshold_amount * sd, distances\n\n        if self.breakpoint_threshold_type == \"interquartile\":\n            q1, q3 = np.percentile(distances, [25.0, 75.0])\n            iqr = float(q3 - q1)\n            mu = float(np.mean(distances))\n            return mu + self.breakpoint_threshold_amount * iqr, distances\n\n        if self.breakpoint_threshold_type == \"gradient\":\n            grads = np.gradient(np.asarray(distances, dtype=np.float64)).tolist()\n            thr = float(np.percentile(grads, self.breakpoint_threshold_amount))\n            return thr, grads  # use gradient array as the reference\n\n        raise ValueError(\n            f\"Unexpected breakpoint_threshold_type: {self.breakpoint_threshold_type}\"\n        )\n\n    # ---------- Public API ----------\n\n    def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n        \"\"\"Split the document text into semantically coherent chunks.\n\n        Args:\n            reader_output (ReaderOutput): The document text &amp; metadata.\n\n        Returns:\n            SplitterOutput: Chunks, IDs, metadata, and splitter configuration.\n\n        Notes:\n            - With 1 sentence (or 2 in gradient mode), returns the text/sentences as-is.\n            - Chunks shorter than `chunk_size` (minimum) are skipped and merged forward.\n            - `chunk_size` behaves as the *minimum* chunk size in this splitter.\n        \"\"\"\n        text = reader_output.text\n        if text == \"\" or text is None:\n            raise ValueError(\"No text has been provided\")\n\n        amt = self.breakpoint_threshold_amount\n        if (\n            self.breakpoint_threshold_type in (\"percentile\", \"gradient\")\n            and 0.0 &lt; amt &lt;= 1.0  # noqa: W503\n        ):\n            self.breakpoint_threshold_amount = amt * 100.0\n\n        sentences = self._split_into_sentences(reader_output)\n\n        # Edge cases where thresholds aren't meaningful\n        if len(sentences) &lt;= 1:\n            chunks = sentences if sentences else [text]\n        elif self.breakpoint_threshold_type == \"gradient\" and len(sentences) == 2:\n            chunks = sentences\n        else:\n            distances, sentence_dicts = self._calculate_sentence_distances(sentences)\n\n            if self.number_of_chunks is not None and distances:\n                # Pick top (k-1) distances as breakpoints\n                k = int(self.number_of_chunks)\n                m = max(0, min(k - 1, len(distances)))  # number of cuts to make\n                if m == 0:\n                    indices_above = []  # single chunk\n                else:\n                    # indices of the m largest distances (breaks), sorted in ascending order\n                    idxs = np.argsort(np.asarray(distances))[-m:]\n                    indices_above = sorted(int(i) for i in idxs.tolist())\n            else:\n                threshold, ref_array = self._calculate_breakpoint_threshold(distances)\n                indices_above = [\n                    i for i, val in enumerate(ref_array) if val &gt; threshold\n                ]\n\n            chunks: List[str] = []\n            start_idx = 0\n\n            for idx in indices_above:\n                end = idx + 1  # inclusive slice end\n                candidate = \" \".join(\n                    d[\"sentence\"] for d in sentence_dicts[start_idx:end]\n                ).strip()\n                if len(candidate) &lt; self.chunk_size:\n                    # too small: keep accumulating (do NOT move start_idx)\n                    continue\n                chunks.append(candidate)\n                start_idx = end\n\n            # Tail (always emit whatever remains)\n            if start_idx &lt; len(sentence_dicts):\n                tail = \" \".join(\n                    d[\"sentence\"] for d in sentence_dicts[start_idx:]\n                ).strip()\n                if tail:\n                    chunks.append(tail)\n\n            if not chunks:\n                chunks = [\" \".join(sentences).strip() or (reader_output.text or \"\")]\n\n        # IDs &amp; metadata\n        chunk_ids = self._generate_chunk_ids(len(chunks))\n        metadata = self._default_metadata()\n        model_name = getattr(self.embedding, \"model_name\", None)\n\n        return SplitterOutput(\n            chunks=chunks,\n            chunk_id=chunk_ids,\n            document_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"semantic_splitter\",\n            split_params={\n                \"buffer_size\": self.buffer_size,\n                \"breakpoint_threshold_type\": self.breakpoint_threshold_type,\n                \"breakpoint_threshold_amount\": self.breakpoint_threshold_amount,\n                \"number_of_chunks\": self.number_of_chunks,\n                \"chunk_size\": self.chunk_size,\n                \"model_name\": model_name,\n            },\n            metadata=metadata,\n        )\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.semantic_splitter.SemanticSplitter.__init__","title":"<code>__init__(embedding, *, buffer_size=1, breakpoint_threshold_type='percentile', breakpoint_threshold_amount=None, number_of_chunks=None, chunk_size=1000)</code>","text":"<p>Initialize the semantic splitter.</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>BaseEmbedding</code> <p>Embedding backend.</p> required <code>buffer_size</code> <code>int</code> <p>Neighbor window size around each sentence.</p> <code>1</code> <code>breakpoint_threshold_type</code> <code>BreakpointThresholdType</code> <p>Threshold strategy: \"percentile\" | \"standard_deviation\" | \"interquartile\" | \"gradient\".</p> <code>'percentile'</code> <code>breakpoint_threshold_amount</code> <code>Optional[float]</code> <p>Threshold parameter. If None, uses sensible defaults per strategy (e.g., 95th percentile).</p> <code>None</code> <code>number_of_chunks</code> <code>Optional[int]</code> <p>If set, pick a threshold that approximately yields this number of chunks (inverse percentile).</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>Minimum characters required to emit a chunk.</p> <code>1000</code> Source code in <code>src/splitter_mr/splitter/splitters/semantic_splitter.py</code> <pre><code>def __init__(\n    self,\n    embedding: BaseEmbedding,\n    *,\n    buffer_size: int = 1,\n    breakpoint_threshold_type: BreakpointThresholdType = \"percentile\",\n    breakpoint_threshold_amount: Optional[float] = None,\n    number_of_chunks: Optional[int] = None,\n    chunk_size: int = 1000,\n) -&gt; None:\n    \"\"\"Initialize the semantic splitter.\n\n    Args:\n        embedding (BaseEmbedding): Embedding backend.\n        buffer_size (int): Neighbor window size around each sentence.\n        breakpoint_threshold_type (BreakpointThresholdType): Threshold strategy:\n            \"percentile\" | \"standard_deviation\" | \"interquartile\" | \"gradient\".\n        breakpoint_threshold_amount (Optional[float]): Threshold parameter. If None,\n            uses sensible defaults per strategy (e.g., 95th percentile).\n        number_of_chunks (Optional[int]): If set, pick a threshold that\n            approximately yields this number of chunks (inverse percentile).\n        chunk_size (int): **Minimum** characters required to emit a chunk.\n    \"\"\"\n    super().__init__(chunk_size=chunk_size)\n    self.embedding = embedding\n    self.buffer_size = int(buffer_size)\n    self.breakpoint_threshold_type = cast(\n        BreakpointThresholdType, breakpoint_threshold_type\n    )\n    self.breakpoint_threshold_amount = (\n        DEFAULT_BREAKPOINTS[self.breakpoint_threshold_type]\n        if breakpoint_threshold_amount is None\n        else float(breakpoint_threshold_amount)\n    )\n    self.number_of_chunks = number_of_chunks\n    self._sentence_splitter = SentenceSplitter(\n        chunk_size=1, chunk_overlap=0, separators=[\".\", \"!\", \"?\"]\n    )\n</code></pre>"},{"location":"api_reference/splitter/#splitter_mr.splitter.splitters.semantic_splitter.SemanticSplitter.split","title":"<code>split(reader_output)</code>","text":"<p>Split the document text into semantically coherent chunks.</p> <p>Parameters:</p> Name Type Description Default <code>reader_output</code> <code>ReaderOutput</code> <p>The document text &amp; metadata.</p> required <p>Returns:</p> Name Type Description <code>SplitterOutput</code> <code>SplitterOutput</code> <p>Chunks, IDs, metadata, and splitter configuration.</p> Notes <ul> <li>With 1 sentence (or 2 in gradient mode), returns the text/sentences as-is.</li> <li>Chunks shorter than <code>chunk_size</code> (minimum) are skipped and merged forward.</li> <li><code>chunk_size</code> behaves as the minimum chunk size in this splitter.</li> </ul> Source code in <code>src/splitter_mr/splitter/splitters/semantic_splitter.py</code> <pre><code>def split(self, reader_output: ReaderOutput) -&gt; SplitterOutput:\n    \"\"\"Split the document text into semantically coherent chunks.\n\n    Args:\n        reader_output (ReaderOutput): The document text &amp; metadata.\n\n    Returns:\n        SplitterOutput: Chunks, IDs, metadata, and splitter configuration.\n\n    Notes:\n        - With 1 sentence (or 2 in gradient mode), returns the text/sentences as-is.\n        - Chunks shorter than `chunk_size` (minimum) are skipped and merged forward.\n        - `chunk_size` behaves as the *minimum* chunk size in this splitter.\n    \"\"\"\n    text = reader_output.text\n    if text == \"\" or text is None:\n        raise ValueError(\"No text has been provided\")\n\n    amt = self.breakpoint_threshold_amount\n    if (\n        self.breakpoint_threshold_type in (\"percentile\", \"gradient\")\n        and 0.0 &lt; amt &lt;= 1.0  # noqa: W503\n    ):\n        self.breakpoint_threshold_amount = amt * 100.0\n\n    sentences = self._split_into_sentences(reader_output)\n\n    # Edge cases where thresholds aren't meaningful\n    if len(sentences) &lt;= 1:\n        chunks = sentences if sentences else [text]\n    elif self.breakpoint_threshold_type == \"gradient\" and len(sentences) == 2:\n        chunks = sentences\n    else:\n        distances, sentence_dicts = self._calculate_sentence_distances(sentences)\n\n        if self.number_of_chunks is not None and distances:\n            # Pick top (k-1) distances as breakpoints\n            k = int(self.number_of_chunks)\n            m = max(0, min(k - 1, len(distances)))  # number of cuts to make\n            if m == 0:\n                indices_above = []  # single chunk\n            else:\n                # indices of the m largest distances (breaks), sorted in ascending order\n                idxs = np.argsort(np.asarray(distances))[-m:]\n                indices_above = sorted(int(i) for i in idxs.tolist())\n        else:\n            threshold, ref_array = self._calculate_breakpoint_threshold(distances)\n            indices_above = [\n                i for i, val in enumerate(ref_array) if val &gt; threshold\n            ]\n\n        chunks: List[str] = []\n        start_idx = 0\n\n        for idx in indices_above:\n            end = idx + 1  # inclusive slice end\n            candidate = \" \".join(\n                d[\"sentence\"] for d in sentence_dicts[start_idx:end]\n            ).strip()\n            if len(candidate) &lt; self.chunk_size:\n                # too small: keep accumulating (do NOT move start_idx)\n                continue\n            chunks.append(candidate)\n            start_idx = end\n\n        # Tail (always emit whatever remains)\n        if start_idx &lt; len(sentence_dicts):\n            tail = \" \".join(\n                d[\"sentence\"] for d in sentence_dicts[start_idx:]\n            ).strip()\n            if tail:\n                chunks.append(tail)\n\n        if not chunks:\n            chunks = [\" \".join(sentences).strip() or (reader_output.text or \"\")]\n\n    # IDs &amp; metadata\n    chunk_ids = self._generate_chunk_ids(len(chunks))\n    metadata = self._default_metadata()\n    model_name = getattr(self.embedding, \"model_name\", None)\n\n    return SplitterOutput(\n        chunks=chunks,\n        chunk_id=chunk_ids,\n        document_name=reader_output.document_name,\n        document_path=reader_output.document_path,\n        document_id=reader_output.document_id,\n        conversion_method=reader_output.conversion_method,\n        reader_method=reader_output.reader_method,\n        ocr_method=reader_output.ocr_method,\n        split_method=\"semantic_splitter\",\n        split_params={\n            \"buffer_size\": self.buffer_size,\n            \"breakpoint_threshold_type\": self.breakpoint_threshold_type,\n            \"breakpoint_threshold_amount\": self.breakpoint_threshold_amount,\n            \"number_of_chunks\": self.number_of_chunks,\n            \"chunk_size\": self.chunk_size,\n            \"model_name\": model_name,\n        },\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"examples/examples/","title":"Examples","text":"<p>This section illustrates some use cases with SplitterMR to read documents and split them into smaller chunks.</p>"},{"location":"examples/examples/#text-based-splitting","title":"Text-based splitting","text":""},{"location":"examples/examples/#how-to-split-recusively","title":"How to split recusively","text":"<p>Divide your text recursively by group of words and sentences, based on the character length as your choice.</p>"},{"location":"examples/examples/#how-to-split-by-characters-words-sentences-or-paragraphs","title":"How to split by characters, words, sentences or paragraphs","text":"<p>Divide your text by gramatical groups, with an specific chunk size and with optional chunk overlap.</p>"},{"location":"examples/examples/#how-to-split-by-regular-expressions-or-keywords","title":"How to split by regular expressions or keywords","text":"<p>Divide your text by regular expressions (<code>regex</code> patterns) or specific keywords. </p>"},{"location":"examples/examples/#how-to-split-by-pages","title":"How to split by pages","text":"<p>Divide your files (PDF, Word, Excel, PowerPoint) by gramatical groups, with the desired number of pages and optional chunk overlap.</p>"},{"location":"examples/examples/#how-to-split-your-text-by-tokens","title":"How to split your text by tokens","text":"<p>Divide your text to accomplsih your LLM window context using tokenizers such as <code>Spacy</code>, <code>NLTK</code> and <code>Tiktoken</code>.</p>"},{"location":"examples/examples/#how-to-split-your-text-based-on-semantic-similarity","title":"How to split your text based on semantic similarity","text":"<p>Divide your text into semantically coherent units using clustering embedding-based methods.</p>"},{"location":"examples/examples/#schema-based-splitting","title":"Schema-based splitting","text":""},{"location":"examples/examples/#how-to-split-html-documents-by-tags","title":"How to split HTML documents by tags","text":"<p>Divide the text by tags conserving the HTML schema.</p>"},{"location":"examples/examples/#how-to-split-json-files-recusively","title":"How to split JSON files recusively","text":"<p>Divide your JSON files into valid smaller serialized objects.</p>"},{"location":"examples/examples/#how-to-split-by-headers-for-your-markdown-and-html-files","title":"How to split by Headers for your Markdown and HTML files","text":"<p>Divide your HTML or Markdown files hierarchically by headers.</p>"},{"location":"examples/examples/#how-to-split-your-code-scripts","title":"How to split your code scripts","text":"<p>Divide your scripts written in Java, Javascript, Python, Go and many more programming languages by syntax blocks.</p>"},{"location":"examples/examples/#how-to-split-your-tables-into-smaller-tables","title":"How to split your tables into smaller tables","text":"<p>Divide your tables by a fixed number of rows and columns preserving the headers and overall structure.</p>"},{"location":"examples/examples/#reading-files","title":"Reading files","text":""},{"location":"examples/examples/#how-to-read-a-pdf-file-without-image-processing","title":"How to read a PDF file without image processing","text":"<p>Read your PDF files using three frameworks: <code>PDFPlumber</code>, <code>MarkItDown</code> and <code>Docling</code>. </p>"},{"location":"examples/examples/#how-to-read-a-pdf-file-using-vanilla-reader-for-image-processing","title":"How to read a PDF file using Vanilla Reader for image processing","text":"<p>Read your PDF files and analyze the content using PDFPlumber and Visual Language Models.</p>"},{"location":"examples/examples/#how-to-read-a-pdf-file-using-docling-for-image-processing","title":"How to read a PDF file using Docling for image processing","text":"<p>Read your PDF files and analyze the content using Docling and Visual Language Models.</p>"},{"location":"examples/examples/#how-to-read-a-pdf-file-using-markitdown-for-image-processing","title":"How to read a PDF file using MarkItDown for image processing","text":"<p>Read your PDF files and analyze the content using MarkItDown and Visual Language Models.</p>"},{"location":"examples/examples/#vlm-options-comparison","title":"VLM Options comparison","text":"<p>Coming soon!</p> <p>Note</p> <p>\ud83d\udc68\u200d\ud83d\udcbb Work-in-progress... More examples to come!</p>"},{"location":"examples/examples/#other-examples","title":"Other examples","text":"<p>Warning</p> <p>These examples have been deprecated!</p>"},{"location":"examples/examples/#how-to-read-pdfs-and-analyze-its-content-using-several-visual-language-models","title":"How to read PDFs and analyze its content using several Visual Language Models","text":"<p>Examples about how to read PDF files using PDFPlumber, MarkItDown and Docling frameworks, and read its multimedia content using a visual language model.</p>"},{"location":"examples/pdf/pdf_docling/","title":"Example: Read PDF documents with images using Docling Reader","text":"<p>As we have seen in previous examples, reading a PDF is not a simple task. In this case, we will see how to read a PDF using the Docling framework, and connect this library into Visual Language Models to extract text or get annotations from images.</p>"},{"location":"examples/pdf/pdf_docling/#connecting-to-a-vlm-to-extract-text-and-analyze-images","title":"Connecting to a VLM to extract text and analyze images","text":"<p>For this example, we will use the same document as the previous tutorial.</p> <p>To use a VLM to read images and get annotations, instantiate any model that implements the <code>BaseModel</code> interface (vision variants inherit from it) and pass it into the <code>VanillaReader</code>. Swapping providers only changes the model constructor; your Reader usage remains the same.</p>"},{"location":"examples/pdf/pdf_docling/#supported-models-and-when-to-use-them","title":"Supported models (and when to use them)","text":"Model (docs) When to use Required environment variables <code>OpenAIVisionModel</code> You have an OpenAI API key and want OpenAI cloud. <code>OPENAI_API_KEY</code> (optional: <code>OPENAI_MODEL</code>, defaults to <code>gpt-4o</code>) <code>AzureOpenAIVisionModel</code> You use Azure OpenAI Service. <code>AZURE_OPENAI_API_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_DEPLOYMENT</code>, <code>AZURE_OPENAI_API_VERSION</code> <code>GrokVisionModel</code> You have access to xAI Grok multimodal. <code>XAI_API_KEY</code> (optional: <code>XAI_MODEL</code>, default <code>grok-4</code>) <code>GeminiVisionModel</code> You want Google\u2019s Gemini vision models. <code>GEMINI_API_KEY</code> (also install extras: <code>pip install \"splitter-mr[multimodal]\"</code>) <code>AnthropicVisionModel</code> You have an Anthropic key (Claude Vision). <code>ANTHROPIC_API_KEY</code> (optional: <code>ANTHROPIC_MODEL</code>) <code>HuggingFaceVisionModel</code> You prefer local/open-source/offline inference. Install extras: <code>pip install \"splitter-mr[multimodal]\"</code> (optional: <code>HF_ACCESS_TOKEN</code> if the chosen model requires it) <p>Note on HuggingFace models: Not all HF models are supported (e.g., gated or uncommon architectures). A well-tested option is SmolDocling.</p>"},{"location":"examples/pdf/pdf_docling/#environment-variables","title":"Environment variables","text":"<p>Create a <code>.env</code> file alongside your Python script:</p> Show/hide environment variables needed for every provider OpenAI <pre><code># OpenAI\nOPENAI_API_KEY=&lt;your-api-key&gt;\n# (optional) OPENAI_MODEL=gpt-4o\n</code></pre> Azure OpenAI <pre><code># Azure OpenAI\nAZURE_OPENAI_API_KEY=&lt;your-api-key&gt;\nAZURE_OPENAI_ENDPOINT=&lt;your-endpoint&gt;\nAZURE_OPENAI_API_VERSION=&lt;your-api-version&gt;\nAZURE_OPENAI_DEPLOYMENT=&lt;your-model-name&gt;\n</code></pre> xAI Grok <pre><code># xAI Grok\nXAI_API_KEY=&lt;your-api-key&gt;\n# (optional) XAI_MODEL=grok-4\n</code></pre> Google Gemini <pre><code># Google Gemini\nGEMINI_API_KEY=&lt;your-api-key&gt;\n# Also: pip install \"splitter-mr[multimodal]\"\n</code></pre> Anthropic (Claude Vision) <pre><code># Anthropic (Claude Vision)\nANTHROPIC_API_KEY=&lt;your-api-key&gt;\n# (optional) ANTHROPIC_MODEL=claude-sonnet-4-20250514\n</code></pre> Hugging Face (local/open-source) <pre><code># Hugging Face (optional, only if needed by the model)\nHF_ACCESS_TOKEN=&lt;your-hf-token&gt;\n# Also: pip install \"splitter-mr[multimodal]\"\n</code></pre>"},{"location":"examples/pdf/pdf_docling/#instantiation-examples","title":"Instantiation examples","text":"Show/hide instantiation snippets for all providers OpenAI <pre><code>from splitter_mr.model import OpenAIVisionModel\n\n# Reads OPENAI_API_KEY (and optional OPENAI_MODEL) from .env if present\nmodel = OpenAIVisionModel()\n# or pass explicitly:\n# model = OpenAIVisionModel(api_key=\"...\", model_name=\"gpt-4o\")\n</code></pre> Azure OpenAI <pre><code>from splitter_mr.model import AzureOpenAIVisionModel\n\n# Reads Azure vars from .env if present\nmodel = AzureOpenAIVisionModel()\n# or:\n# model = AzureOpenAIVisionModel(\n#     api_key=\"...\",\n#     azure_endpoint=\"https://&lt;resource&gt;.openai.azure.com/\",\n#     api_version=\"2024-02-15-preview\",\n#     azure_deployment=\"&lt;your-deployment-name&gt;\",\n# )\n</code></pre> xAI Grok <pre><code>from splitter_mr.model import GrokVisionModel\n\n# Reads XAI_API_KEY (and optional XAI_MODEL) from .env\nmodel = GrokVisionModel()\n</code></pre> Google Gemini <pre><code>from splitter_mr.model import GeminiVisionModel\n\n# Requires GEMINI_API_KEY and the 'multimodal' extra installed\nmodel = GeminiVisionModel()\n</code></pre> Anthropic (Claude Vision) <pre><code>from splitter_mr.model import AnthropicVisionModel\n\n# Reads ANTHROPIC_API_KEY (and optional ANTHROPIC_MODEL) from .env\nmodel = AnthropicVisionModel()\n</code></pre> Hugging Face (local/open-source) <pre><code>from splitter_mr.model import HuggingFaceVisionModel\n\n# Token only if the model requires gating\nmodel = HuggingFaceVisionModel()\n</code></pre> <pre><code>from splitter_mr.model import AzureOpenAIVisionModel\nfrom splitter_mr.reader import DoclingReader\n\nfile = \"data/sample_pdf.pdf\"\n\nmodel = AzureOpenAIVisionModel()\n</code></pre> <p>Then, use the <code>read</code> method of this object and read a file as always. Once detected that the file is PDF, it will return a ReaderOutput object containing the extracted text.</p> <pre><code># 1. Read PDF using a Visual Language Model\n\nprint(\"=\" * 80 + \" DoclingReader with VLM \" + \"=\" * 80)\ndocling_reader = DoclingReader(model=model)\ndocling_output = docling_reader.read(file)\n\n# Get Docling ReaderOutput\nprint(docling_output.model_dump_json(indent=4))\n</code></pre> <pre><code>================================================================================ DoclingReader with VLM ================================================================================\n{\n    \"text\": \"## A sample PDF\\n\\nConverting PDF files to other formats, such as Markdown, is a surprisingly complex task due to the nature of the PDF format itself . PDF (Portable Document Format) was designed primarily for preserving the visual layout of documents, making them look the same across different devi\n...\nmmingbird hovers gracefully in front of a bright orange flower, showcasing the beauty of nature and the delicate balance between pollinators and plants.*\",\n    \"document_name\": \"sample_pdf.pdf\",\n    \"document_path\": \"data/sample_pdf.pdf\",\n    \"document_id\": \"69de2a09-2477-4b34-a6a9-c955a44d5f15\",\n    \"conversion_method\": \"markdown\",\n    \"reader_method\": \"docling\",\n    \"ocr_method\": \"es-BPE_GENAI_CLASSIFIER_AGENT-llm-lab-ext-4o-mini\",\n    \"page_placeholder\": \"&lt;!-- page --&gt;\",\n    \"metadata\": {}\n}\n</code></pre> <p>As we can see, the PDF contents along with some metadata information such as the <code>conversion_method</code>, <code>reader_method</code> or the <code>ocr_method</code> have been retrieved. To get the PDF contents, you can simply access to the <code>text</code> attribute as always:</p> <pre><code># Get text attribute from Docling Reader\nprint(docling_output.text)\n</code></pre> <pre><code>## A sample PDF\n\nConverting PDF files to other formats, such as Markdown, is a surprisingly complex task due to the nature of the PDF format itself . PDF (Portable Document Format) was designed primarily for preserving the visual layout of documents, making them look the same across different devices and platforms. However, this design goal introduces several challenges when trying to extract and convert the underlying content into a more flexible, structured format like Markdown.\n\nIlustraci\u00f3n 1\n...\ne conversion tools must blend text extraction, document analysis, and sometimes machine learning techniques (such as OCR or structure recognition) to produce usable, readable, and faithful Markdown output. As a result, perfect conversion is rarely possible, and manual review and cleanup are often required.\n\n&lt;!-- image --&gt;\n*Caption: A vibrant hummingbird hovers gracefully in front of a bright orange flower, showcasing the beauty of nature and the delicate balance between pollinators and plants.*\n</code></pre> <p>As seen, all the images have been described using a caption. </p>"},{"location":"examples/pdf/pdf_docling/#experimenting-with-some-keyword-arguments","title":"Experimenting with some keyword arguments","text":"<p>In case that you have additional requirements to describe these images, you can provide a prompt via a <code>prompt</code> argument:</p> <pre><code>docling_output = docling_reader.read(\n    file, prompt=\"Describe the image briefly in Spanish.\"\n)\n\nprint(docling_output.text)\n</code></pre> <pre><code>## A sample PDF\n\nConverting PDF files to other formats, such as Markdown, is a surprisingly complex task due to the nature of the PDF format itself . PDF (Portable Document Format) was designed primarily for preserving the visual layout of documents, making them look the same across different devices and platforms. However, this design goal introduces several challenges when trying to extract and convert the underlying content into a more flexible, structured format like Markdown.\n\nIlustraci\u00f3n 1\n...\nchine learning techniques (such as OCR or structure recognition) to produce usable, readable, and faithful Markdown output. As a result, perfect conversion is rarely possible, and manual review and cleanup are often required.\n\n&lt;!-- image --&gt;\nLa imagen muestra un colibr\u00ed de plumaje brillante en tonos verdes, suspendido en el aire mientras se alimenta de flores amarillas. Sus alas est\u00e1n en movimiento, lo que resalta su agilidad, y el fondo es difuso, lo que enfoca la atenci\u00f3n en el ave y la flor.\n</code></pre> <p>You can read the PDF scanning the pages as images and extracting its content. To do so, enable the option <code>scan_pdf_pages = True</code>. In case that you want to change the placeholder, you can do it passing the keyword argument <code>placeholder = &lt;your desired placeholder&gt;</code>.</p> <p>Finally, it could be interesting extract the markdown text with the images as embedded content. In that case, activate the option <code>show_base64_images</code>. In that case, it is not necessary to pass the model to the Reader class.</p> <pre><code>docling_reader = DoclingReader()\ndocling_output = docling_reader.read(file, show_base64_images=True)\n\nprint(docling_output.text)\n</code></pre> <pre><code>## A sample PDF\n\nConverting PDF files to other formats, such as Markdown, is a surprisingly complex task due to the nature of the PDF format itself . PDF (Portable Document Format) was designed primarily for preserving the visual layout of documents, making them look the same across different devices and platforms. However, this design goal introduces several challenges when trying to extract and convert the underlying content into a more flexible, structured format like Markdown.\n\nIlustraci\u00f3n 1\n...\nDdJ3Yad2DXUmreusBTxAgDg4pSvJVgmVNZRuDOYBAg9zJJNOPl0Mx2KYpGYWNYRGJY1TRjRmiFapHMYfKtKIGJnSiq2cE9AxhIkdM3w5jQz4Ik0hAwCfg5T0k6yQasCjrhQPgT1/m5/GQRICaaxsx+SuIDo1v2F9UJwJlAAsKHIEonjBJVqNov4oihBRGuWFhy5jPRIQgQK3eYZI6Ggo2hw0tTZvGk5ASudyZMGdl9hS4F2NHJ6ymBpgkn0Ggctuo5F5pHhZzqnNQpXjAXjplkBwijcLoGqjyExIO8zEMvB/54P4AYSZlJgyds3AzQO1fLUoKeHIaq4sWAYEOVi/KIbhpTuQDOwQ7QIjmcDI5pN64iXwP64HUh+wng9VxugUJFaZGUHVEg8wh3rEW1hsx5RCNOlebOE2U0ivY8B4shaBqEQSY5aih5dDUVlVGyLIc3yB3PM8iyJk29XC7yIvv/AFi0ru7UxlyZAAAAAElFTkSuQmCC)\n</code></pre> <p>Of course, remember that the use of a VLM is not mandatory, and you can read the PDF obtaining most of the information.</p>"},{"location":"examples/pdf/pdf_docling/#complete-script","title":"Complete script","text":"<pre><code>from splitter_mr.model import AzureOpenAIVisionModel\nfrom splitter_mr.reader import DoclingReader\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfile = \"data/sample_pdf.pdf\"\n\nmodel = AzureOpenAIVisionModel()\ndocling_reader = DoclingReader(model = model)\n\n# 1. Read PDF using a Visual Language Model\n\ndocling_output = docling_reader.read(file)\nprint(docling_output.model_dump_json(indent=4))  # Get Docling ReaderOutput\nprint(docling_output.text)  # Get text attribute from Docling Reader\n\n# 2. Describe the images using a custom prompt\n\ndocling_output = docling_reader.read(file, prompt = \"Describe the image briefly in Spanish.\")\nprint(docling_output.text)\n\n# 3. Scan PDF pages \n\ndocling_output = docling_reader.read(file, scan_pdf_pages = True)\nprint(docling_output.text)\n\n# 4. Extract images as embedded content\n\ndocling_reader = DoclingReader()\ndocling_output = docling_reader.read(file, show_base64_images = True)\nprint(docling_output.text)\n</code></pre> <p>Note</p> <p>For more on available options, see the DoclingReader class documentation.</p>"},{"location":"examples/pdf/pdf_markitdown/","title":"Example: Reading PDF Documents with Images using MarkItDownReader","text":"<p>As we have seen in previous examples, reading a PDF is not a simple task. In this case, we will see how to read a PDF using the MarkItDown framework, and connect this library to Visual Language Models (VLMs) to extract text or get annotations from images.</p>"},{"location":"examples/pdf/pdf_markitdown/#how-to-connect-a-vlm-to-markitdownreader","title":"How to connect a VLM to MarkItDownReader","text":"<p>For this example, we will use the same document as the previous tutorial.</p> <p>To extract image descriptions or perform OCR, instantiate any model that implements the <code>BaseModel</code> interface (vision variants inherit from it) and pass it into the <code>MarkItDownReader</code>. Swapping providers only changes the model constructor; your Reader usage remains the same.</p>"},{"location":"examples/pdf/pdf_markitdown/#supported-models-and-when-to-use-them","title":"Supported models (and when to use them)","text":"Model (docs) When to use Required environment variables <code>OpenAIVisionModel</code> You have an OpenAI API key and want OpenAI cloud. <code>OPENAI_API_KEY</code> (optional: <code>OPENAI_MODEL</code>, defaults to <code>gpt-4o</code>) <code>AzureOpenAIVisionModel</code> You use Azure OpenAI Service. <code>AZURE_OPENAI_API_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_DEPLOYMENT</code>, <code>AZURE_OPENAI_API_VERSION</code> <code>GrokVisionModel</code> You have access to xAI Grok multimodal. <code>XAI_API_KEY</code> (optional: <code>XAI_MODEL</code>, default <code>grok-4</code>) <code>GeminiVisionModel</code> You want Google\u2019s Gemini vision models. <code>GEMINI_API_KEY</code> (also install extras: <code>pip install \"splitter-mr[multimodal]\"</code>) <code>AnthropicVisionModel</code> You have an Anthropic key (Claude Vision). <code>ANTHROPIC_API_KEY</code> (optional: <code>ANTHROPIC_MODEL</code>) <code>HuggingFaceVisionModel</code> You prefer local/open-source/offline inference. Install extras: <code>pip install \"splitter-mr[multimodal]\"</code> (optional: <code>HF_ACCESS_TOKEN</code> if the chosen model requires it) <p>Note on HuggingFace models: Not all HF models are supported (e.g., gated or uncommon architectures). A well-tested option is SmolDocling.</p>"},{"location":"examples/pdf/pdf_markitdown/#environment-variables","title":"Environment variables","text":"<p>Create a <code>.env</code> file alongside your Python script:</p> Show/hide environment variables needed for every provider OpenAI <pre><code># OpenAI\nOPENAI_API_KEY=&lt;your-api-key&gt;\n# (optional) OPENAI_MODEL=gpt-4o\n</code></pre> Azure OpenAI <pre><code># Azure OpenAI\nAZURE_OPENAI_API_KEY=&lt;your-api-key&gt;\nAZURE_OPENAI_ENDPOINT=&lt;your-endpoint&gt;\nAZURE_OPENAI_API_VERSION=&lt;your-api-version&gt;\nAZURE_OPENAI_DEPLOYMENT=&lt;your-model-name&gt;\n</code></pre> xAI Grok <pre><code># xAI Grok\nXAI_API_KEY=&lt;your-api-key&gt;\n# (optional) XAI_MODEL=grok-4\n</code></pre> Google Gemini <pre><code># Google Gemini\nGEMINI_API_KEY=&lt;your-api-key&gt;\n# Also: pip install \"splitter-mr[multimodal]\"\n</code></pre> Anthropic (Claude Vision) <pre><code># Anthropic (Claude Vision)\nANTHROPIC_API_KEY=&lt;your-api-key&gt;\n# (optional) ANTHROPIC_MODEL=claude-sonnet-4-20250514\n</code></pre> Hugging Face (local/open-source) <pre><code># Hugging Face (optional, only if needed by the model)\nHF_ACCESS_TOKEN=&lt;your-hf-token&gt;\n# Also: pip install \"splitter-mr[multimodal]\"\n</code></pre>"},{"location":"examples/pdf/pdf_markitdown/#instantiation-examples","title":"Instantiation examples","text":"Show/hide instantiation snippets for all providers OpenAI <pre><code>from splitter_mr.model import OpenAIVisionModel\n\n# Reads OPENAI_API_KEY (and optional OPENAI_MODEL) from .env if present\nmodel = OpenAIVisionModel()\n# or pass explicitly:\n# model = OpenAIVisionModel(api_key=\"...\", model_name=\"gpt-4o\")\n</code></pre> Azure OpenAI <pre><code>from splitter_mr.model import AzureOpenAIVisionModel\n\n# Reads Azure vars from .env if present\nmodel = AzureOpenAIVisionModel()\n# or:\n# model = AzureOpenAIVisionModel(\n#     api_key=\"...\",\n#     azure_endpoint=\"https://&lt;resource&gt;.openai.azure.com/\",\n#     api_version=\"2024-02-15-preview\",\n#     azure_deployment=\"&lt;your-deployment-name&gt;\",\n# )\n</code></pre> xAI Grok <pre><code>from splitter_mr.model import GrokVisionModel\n\n# Reads XAI_API_KEY (and optional XAI_MODEL) from .env\nmodel = GrokVisionModel()\n</code></pre> Google Gemini <pre><code>from splitter_mr.model import GeminiVisionModel\n\n# Requires GEMINI_API_KEY and the 'multimodal' extra installed\nmodel = GeminiVisionModel()\n</code></pre> Anthropic (Claude Vision) <pre><code>from splitter_mr.model import AnthropicVisionModel\n\n# Reads ANTHROPIC_API_KEY (and optional ANTHROPIC_MODEL) from .env\nmodel = AnthropicVisionModel()\n</code></pre> Hugging Face (local/open-source) <pre><code>from splitter_mr.model import HuggingFaceVisionModel\n\n# Token only if the model requires gating\nmodel = HuggingFaceVisionModel()\n</code></pre> <pre><code>from splitter_mr.model import AzureOpenAIVisionModel\nfrom splitter_mr.reader import MarkItDownReader\n\nfile = \"data/sample_pdf.pdf\"\nmodel = AzureOpenAIVisionModel()\n</code></pre> <p>Then, you can simply pass the model that you have instantiated to the Reader class:</p> <pre><code>reader = MarkItDownReader(model=model)\noutput = reader.read(file)\n</code></pre> <p>This returns a <code>ReaderOutput</code> object with all document text and extracted image descriptions via the vision model. You can access metadata like <code>output.conversion_method</code>, <code>output.reader_method</code>, <code>output.ocr_method</code>, etc.</p> <p>To retrieve the text content, you can simply access to the <code>text</code> attribute:</p> <pre><code>print(output.text)\n</code></pre> <pre><code>&lt;!-- page --&gt;\n\n# Description:\n# A sample PDF\n\nConverting PDF files to other formats, such as Markdown, is a surprisingly complex task due to the nature of the PDF format itself. PDF (Portable Document Format) was designed primarily for preserving the visual layout of documents, making them look the same across different devices and platforms. However, this design goal introduces several challenges when trying to extract and convert the underlying content into a more flexible, structured format l\n...\nsimple on the surface, converting PDFs to formats like Markdown involves a series of technical and interpretive challenges. Effective conversion tools must blend text extraction, document analysis, and sometimes machine learning techniques (such as OCR or structure recognition) to produce usable, readable, and faithful Markdown output. As a result, perfect conversion is rarely possible, and manual review and cleanup are often required.\n\n![Hummingbird](https://example.com/hummingbird-image)\n```\n</code></pre> <p>With the by-default method, you obtain the text extracted from the PDF as it is shown. This method scan the PDF pages as images and process them using a VLM. The result will be a markdown text with all the images detected in every page. Every page is highlighted with a markdown comment as a placeholder: <code>&lt;!-- page --&gt;</code>. </p>"},{"location":"examples/pdf/pdf_markitdown/#experimenting-with-some-keyword-arguments","title":"Experimenting with some keyword arguments","text":"<p>In case that needed, you can pass use other keyword arguments to process the PDFs.</p> <p>For example, you can customize how to process the images by the VLM using the parameter prompt. For example, in case that you only need an excerpt or a brief description for every page, you can use the following prompt:</p> <pre><code>output = reader.read(\n    file, \n    scan_pdf_pages = True, \n    prompt = \"Return only a short description for these pages\"\n)\n</code></pre> <p>In case that needed, it could be interesting split the PDF pages using another placeholder. You can configure that using the <code>page_placeholder</code> parameter:</p> <pre><code>output = reader.read(\n    file,\n    scan_pdf_pages=True,\n    prompt=\"Return only a short description for these pages\",\n    page_placeholder=\"##\u00a0PAGE\",\n)\nprint(output.text)\n</code></pre> <pre><code>##\u00a0PAGE\n\n# Description:\nThe document discusses the challenges of converting PDF files to more flexible formats like Markdown due to the inherent characteristics of PDFs. It highlights two main issues: the lack of structural information, which complicates the extraction of organized content, and the variability in PDF content types, leading to inconsistent results during conversion.\n\n##\u00a0PAGE\n\n# Description:\n1. **Preservation of Formatting**: Discusses the limitations of Markdown in replicating co\n...\nd special characters in PDFs, focusing on the risks of data loss and corruption during extraction.\n\n##\u00a0PAGE\n\n# Description:\nThe page summarizes the challenges of converting PDFs to Markdown formats, emphasizing the need for advanced tools that integrate text extraction, document analysis, and machine learning. It concludes that perfect conversion is difficult and often requires manual review and adjustments. Additionally, a table of team members with their roles and contact emails is included.\n</code></pre> <p>In comparison, <code>MarkItDownReader</code> offers a faster conversion than Docling but with less options to be configured. In that sense, we cannot obtain directly the <code>base64</code> images from every image detected in our documents, or write image placeholders easily (despite we can do it using a prompt). In addition, you will always get a <code># Description</code> placeholder every time you use a VLM for extraction and captioning in this Reader. </p> <p>As conclusion, using this reader with a VLM can be useful for those use cases when we need to efficiently extract the text from a PDF. In case that you need the highest reliability or customization, it is not the most suitable option.</p>"},{"location":"examples/pdf/pdf_markitdown/#complete-script","title":"Complete script","text":"<pre><code>import os\n\nfrom splitter_mr.model import AzureOpenAIVisionModel\nfrom splitter_mr.reader import MarkItDownReader\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfile = \"data/sample_pdf.pdf\"\nmodel = AzureOpenAIVisionModel()\n# Ensure the output directory exists\noutput_dir = os.path.join(os.path.dirname(__file__), \"markitdown_output\")\nos.makedirs(output_dir, exist_ok=True)\n\ndef save_markdown(output, filename_base):\n    \"\"\"\n    Saves the ReaderOutput.text attribute to a markdown file in the markitdown_output directory.\n\n    Args:\n        output (ReaderOutput): The result object returned from DoclingReader.read().\n        filename_base (str): A descriptive base name for the file (e.g., 'vlm', 'scan_pages').\n    \"\"\"\n    filename = f\"{filename_base}.md\"\n    file_path = os.path.join(output_dir, filename)\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(output.text)\n    print(f\"Saved: {file_path}\")\n\nmarkitdown_reader = MarkItDownReader(model = model)\nmarkitdown_output = markitdown_reader.read(file)\nsave_markdown(markitdown_output, \"vlm\")\n\nmarkitdown_output = markitdown_reader.read(file, scan_pdf_pages = True, prompt = \"Return only a short description for these pages\", page_placeholder = \"##\u00a0PAGE\")\nsave_markdown(markitdown_output, \"custom_vlm\")\n\nmarkitdown_reader = MarkItDownReader()\nmarkitdown_output = markitdown_reader.read(file)\nsave_markdown(markitdown_output, \"no_vlm\")\n</code></pre> <p>Note</p> <p>For more on available options, see the MarkItDownReader class documentation.</p>"},{"location":"examples/pdf/pdf_vanilla/","title":"Example: Read PDF documents with images using Vanilla Reader","text":"<p>In this tutorial we will see how to read a PDF using our custom component, which is based on PDFPlumber. Then, we will connect this reader component into Visual Language Models to extract text or get annotations from images inside the PDF. In addition, we will explore which options we have to analyze and extract the content of the PDF in a custom, fast and comprehensive way. Let's dive in.</p> <p>Note</p> <p>Remember that you can access the complete documentation of this Reader Component in the Developer Guide.</p>"},{"location":"examples/pdf/pdf_vanilla/#how-to-connect-a-vlm-to-vanillareader","title":"How to connect a VLM to VanillaReader","text":"<p>For this tutorial, we will use the same data as the first tutorial. Consult reference here.</p> <p>To extract image descriptions or perform OCR, instantiate any model that implements the <code>BaseModel</code> interface (vision variants inherit from it) and pass it into the <code>VanillaReader</code>. Swapping providers only changes the model constructor; your Reader usage remains the same.</p>"},{"location":"examples/pdf/pdf_vanilla/#supported-models-and-when-to-use-them","title":"Supported models (and when to use them)","text":"Model (docs) When to use Required environment variables <code>OpenAIVisionModel</code> You have an OpenAI API key and want OpenAI cloud. <code>OPENAI_API_KEY</code> (optional: <code>OPENAI_MODEL</code>, defaults to <code>gpt-4o</code>) <code>AzureOpenAIVisionModel</code> You use Azure OpenAI Service. <code>AZURE_OPENAI_API_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_DEPLOYMENT</code>, <code>AZURE_OPENAI_API_VERSION</code> <code>GrokVisionModel</code> You have access to xAI Grok multimodal. <code>XAI_API_KEY</code> (optional: <code>XAI_MODEL</code>, default <code>grok-4</code>) <code>GeminiVisionModel</code> You want Google\u2019s Gemini vision models. <code>GEMINI_API_KEY</code> (also install extras: <code>pip install \"splitter-mr[multimodal]\"</code>) <code>AnthropicVisionModel</code> You have an Anthropic key (Claude Vision). <code>ANTHROPIC_API_KEY</code> (optional: <code>ANTHROPIC_MODEL</code>) <code>HuggingFaceVisionModel</code> You prefer local/open-source/offline inference. Install extras: <code>pip install \"splitter-mr[multimodal]\"</code> (optional: <code>HF_ACCESS_TOKEN</code> if the chosen model requires it) <p>Note on HuggingFace models: Not all HF models are supported (e.g., gated or uncommon architectures). A well-tested option is SmolDocling.</p>"},{"location":"examples/pdf/pdf_vanilla/#environment-variables","title":"Environment variables","text":"Show/hide environment variables needed for every provider OpenAI <pre><code># OpenAI\nOPENAI_API_KEY=&lt;your-api-key&gt;\n# (optional) OPENAI_MODEL=gpt-4o\n</code></pre> Azure OpenAI <pre><code># Azure OpenAI\nAZURE_OPENAI_API_KEY=&lt;your-api-key&gt;\nAZURE_OPENAI_ENDPOINT=&lt;your-endpoint&gt;\nAZURE_OPENAI_API_VERSION=&lt;your-api-version&gt;\nAZURE_OPENAI_DEPLOYMENT=&lt;your-model-name&gt;\n</code></pre> xAI Grok <pre><code># xAI Grok\nXAI_API_KEY=&lt;your-api-key&gt;\n# (optional) XAI_MODEL=grok-4\n</code></pre> Google Gemini <pre><code># Google Gemini\nGEMINI_API_KEY=&lt;your-api-key&gt;\n# Also: pip install \"splitter-mr[multimodal]\"\n</code></pre> Anthropic (Claude Vision) <pre><code># Anthropic (Claude Vision)\nANTHROPIC_API_KEY=&lt;your-api-key&gt;\n# (optional) ANTHROPIC_MODEL=claude-sonnet-4-20250514\n</code></pre> Hugging Face (local/open-source) <pre><code># Hugging Face (optional, only if needed by the model)\nHF_ACCESS_TOKEN=&lt;your-hf-token&gt;\n# Also: pip install \"splitter-mr[multimodal]\"\n</code></pre>"},{"location":"examples/pdf/pdf_vanilla/#instantiation-examples","title":"Instantiation examples","text":"Show/hide instantiation snippets for all providers OpenAI <pre><code>from splitter_mr.model import OpenAIVisionModel\n\n# Reads OPENAI_API_KEY (and optional OPENAI_MODEL) from .env if present\nmodel = OpenAIVisionModel()\n# or pass explicitly:\n# model = OpenAIVisionModel(api_key=\"...\", model_name=\"gpt-4o\")\n</code></pre> Azure OpenAI <pre><code>from splitter_mr.model import AzureOpenAIVisionModel\n\n# Reads Azure vars from .env if present\nmodel = AzureOpenAIVisionModel()\n# or:\n# model = AzureOpenAIVisionModel(\n#     api_key=\"...\",\n#     azure_endpoint=\"https://&lt;resource&gt;.openai.azure.com/\",\n#     api_version=\"2024-02-15-preview\",\n#     azure_deployment=\"&lt;your-deployment-name&gt;\",\n# )\n</code></pre> xAI Grok <pre><code>from splitter_mr.model import GrokVisionModel\n\n# Reads XAI_API_KEY (and optional XAI_MODEL) from .env\nmodel = GrokVisionModel()\n</code></pre> Google Gemini <pre><code>from splitter_mr.model import GeminiVisionModel\n\n# Requires GEMINI_API_KEY and the 'multimodal' extra installed\nmodel = GeminiVisionModel()\n</code></pre> Anthropic (Claude Vision) <pre><code>from splitter_mr.model import AnthropicVisionModel\n\n# Reads ANTHROPIC_API_KEY (and optional ANTHROPIC_MODEL) from .env\nmodel = AnthropicVisionModel()\n</code></pre> Hugging Face (local/open-source) <pre><code>from splitter_mr.model import HuggingFaceVisionModel\n\n# Token only if the model requires gating\nmodel = HuggingFaceVisionModel()\n</code></pre> <pre><code>from splitter_mr.model import AzureOpenAIVisionModel\n\nmodel = AzureOpenAIVisionModel()\n</code></pre> <p>Then, use the Reader component and insert the model as parameter:</p> <pre><code>from splitter_mr.reader import VanillaReader\n\nreader = VanillaReader(model=model)\n</code></pre> <p>Then, you can read the file. The result will be an object from the type <code>ReaderOutput</code>, which is a dictionary containing some metadata about the file. To get the content, you can access to the <code>text</code> attribute:</p> <pre><code>file = \"data/sample_pdf.pdf\"\n\noutput = reader.read(file_path=file)\nprint(output.text)\n</code></pre> <pre><code>&lt;!-- page --&gt;\n\nA sample PDF\nConverting PDF files to other formats, such as Markdown, is a surprisingly\ncomplex task due to the nature of the PDF format itself. PDF (Portable\nDocument Format) was designed primarily for preserving the visual layout of\ndocuments, making them look the same across different devices and\nplatforms. However, this design goal introduces several challenges when trying to\nextract and convert the underlying content into a more flexible, structured format\nlike Markdown.\n\n&lt;!-\n...\nnterpretive challenges. Effective\nconversion tools must blend text extraction, document analysis, and sometimes\nmachine learning techniques (such as OCR or structure recognition) to produce\nusable, readable, and faithful Markdown output. As a result, perfect conversion\nis rarely possible, and manual review and cleanup are often required.\n\n&lt;!-- image --&gt;\n*Caption: A vibrant hummingbird gracefully hovers near orange blossoms, showcasing its iridescent plumage against a soft, blurred background.*\n</code></pre> <p>As observed, all the images have been described by the LLM.</p>"},{"location":"examples/pdf/pdf_vanilla/#experimenting-with-some-keyword-arguments","title":"Experimenting with some keyword arguments","text":"<p>Suppose that you need to simply get the base64 images from the file. Then, you can use the option <code>show_base64_images</code> to get those images:</p> <pre><code>reader = VanillaReader()\noutput = reader.read(file_path=file, show_base64_images=True)\nprint(output.text)\n</code></pre> <pre><code>&lt;!-- page --&gt;\n\nA sample PDF\nConverting PDF files to other formats, such as Markdown, is a surprisingly\ncomplex task due to the nature of the PDF format itself. PDF (Portable\nDocument Format) was designed primarily for preserving the visual layout of\ndocuments, making them look the same across different devices and\nplatforms. However, this design goal introduces several challenges when trying to\nextract and convert the underlying content into a more flexible, structured format\nlike Markdown.\n\n![I\n...\nZoerOkErYlYt8Kd5hqwJ25M3asPNGOzltUzt28ekD/tTPjJ300azYwUpzP3ZN1qass7QcBs6OHfPtVG6MArAQWjXsyvGDmsxaARUqXNuxXWUZTyh2OnkuIzOrJ5I6BTvs6uFzbuw0onSdp5zF2HELkwjGjtPEmAoBr5Z71xR2qKrLxI4GMt1IiWqxpkRmw40TlDUidCsGqVDmgiVG27mEr/UhPTZleWWQdWlXdrbUQS3RsndmOMWOneQUo+bCzotfGHYYvjJ19/gu+HZ3CzvEmAkdwm59BdhNIrMIte7nnNqVXN2hoQVBSq46ds7ybXsgU2JHFvsEYkdVOHhpmm9nwY5zV44dTyOY1EVFMutg1xXVYVWpg/U0Aru5ht1IrcmEdeVAGPlLNzl2cCiYvRBTlFQ5T6i1qVG3Yuyaj2RmrjHHvJqWV43tigRDCHUcOxM81w1TLuaFcj0dv99Csfs/1V9aWHQgYUYAAAAASUVORK5CYII=)\n</code></pre> <p>In addition, you can modify how the image and page placeholders are generated with the options <code>image_placeholder</code> and <code>page_placeholder</code>. Note that in this case we are not using any VLM.</p> <pre><code>reader = VanillaReader()\noutput = reader.read(\n    file_path=file, image_placeholder=\"## Image\", page_placeholder=\"## Page\"\n)\nprint(output.text)\n</code></pre> <pre><code>## Page\n\nA sample PDF\nConverting PDF files to other formats, such as Markdown, is a surprisingly\ncomplex task due to the nature of the PDF format itself. PDF (Portable\nDocument Format) was designed primarily for preserving the visual layout of\ndocuments, making them look the same across different devices and\nplatforms. However, this design goal introduces several challenges when trying to\nextract and convert the underlying content into a more flexible, structured format\nlike Markdown.\n\n## Image\n\n...\narol@example.com |\n\nConclusion\nWhile it may seem simple on the surface, converting PDFs to formats like\nMarkdown involves a series of technical and interpretive challenges. Effective\nconversion tools must blend text extraction, document analysis, and sometimes\nmachine learning techniques (such as OCR or structure recognition) to produce\nusable, readable, and faithful Markdown output. As a result, perfect conversion\nis rarely possible, and manual review and cleanup are often required.\n\n## Image\n</code></pre> <p>But one of the most important features is to scan the PDF as PageImages, to analyze every page with a VLM to extract the content. In order to do that, you can simply activate the option <code>scan_pdf_pages</code>. </p> <pre><code>reader = VanillaReader(model=model)\noutput = reader.read(file_path=file, scan_pdf_pages=True)\nprint(output.text)\n</code></pre> <pre><code>&lt;!-- page --&gt;\n\n# A sample PDF\n\nConverting PDF files to other formats, such as Markdown, is a surprisingly complex task due to the nature of the PDF format itself. PDF (Portable Document Format) was designed primarily for preserving the visual layout of documents, making them look the same across different devices and platforms. However, this design goal introduces several challenges when trying to extract and convert the underlying content into a more flexible, structured format like Markdown.\n\n\n...\ny seem simple on the surface, converting PDFs to formats like Markdown involves a series of technical and interpretive challenges. Effective conversion tools must blend text extraction, document analysis, and sometimes machine learning techniques (such as OCR or structure recognition) to produce usable, readable, and faithful Markdown output. As a result, perfect conversion is rarely possible, and manual review and cleanup are often required.\n\n![Hummingbird](https://example.com/hummingbird.jpg)\n</code></pre> <p>Remember that you can always customize the prompt to get one or other results using the parameter <code>prompt</code>:</p> <pre><code>reader = VanillaReader(model=model)\noutput = reader.read(\n    file_path=file, prompt=\"Extract the content of this resource in html format\"\n)\nprint(output.text)\n</code></pre> <pre><code>&lt;!-- page --&gt;\n\nA sample PDF\nConverting PDF files to other formats, such as Markdown, is a surprisingly\ncomplex task due to the nature of the PDF format itself. PDF (Portable\nDocument Format) was designed primarily for preserving the visual layout of\ndocuments, making them look the same across different devices and\nplatforms. However, this design goal introduces several challenges when trying to\nextract and convert the underlying content into a more flexible, structured format\nlike Markdown.\n\n&lt;!-\n...\n0%;\n            height: auto;\n            border: 2px solid #ccc;\n            border-radius: 10px;\n            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;img src=\"https://example.com/path-to-your-image.jpg\" alt=\"Hummingbird\"&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n```\n\nMake sure to replace `\"https://example.com/path-to-your-image.jpg\"` with the actual URL of your image. This HTML will create a simple webpage that displays the image of the hummingbird with some basic styling.\n</code></pre> <p>To sum up, we can see that <code>VanillaReader</code> is a good option to extract rapidly and efficiently the text content for a PDF file. Remember that you can customize how the extraction is performed. But remember to consult other reading options in the Developer guide or other tutorials.</p> <p>Thank you so much for reading :).</p>"},{"location":"examples/pdf/pdf_vanilla/#complete-script","title":"Complete script","text":"<pre><code>import os\nfrom splitter_mr.reader import VanillaReader\nfrom splitter_mr.model import AzureOpenAIVisionModel\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfile = \"data/sample_pdf.pdf\"\noutput_dir = \"tmp/vanilla_output\"\nos.makedirs(output_dir, exist_ok=True)\n\nmodel = AzureOpenAIVisionModel()\n\n# 1. Default with model\nreader = VanillaReader(model=model)\noutput = reader.read(file_path=file)\nwith open(os.path.join(output_dir, \"output_with_model.txt\"), \"w\", encoding=\"utf-8\") as f:\n    f.write(output.text)\n\n# 2. Default without model, with base64 images shown\nreader = VanillaReader()\noutput = reader.read(file_path=file, show_base64_images=True)\nwith open(os.path.join(output_dir, \"output_with_base64_images.txt\"), \"w\", encoding=\"utf-8\") as f:\n    f.write(output.text)\n\n# 3. Default without model, with placeholders\nreader = VanillaReader()\noutput = reader.read(file_path=file, image_placeholder=\"## Image\", page_placeholder=\"## Page\")\nwith open(os.path.join(output_dir, \"output_with_placeholders.txt\"), \"w\", encoding=\"utf-8\") as f:\n    f.write(output.text)\n\n# 4. With model, scan PDF pages\nreader = VanillaReader(model=model)\noutput = reader.read(file_path=file, scan_pdf_pages=True)\nwith open(os.path.join(output_dir, \"output_scan_pdf_pages.txt\"), \"w\", encoding=\"utf-8\") as f:\n    f.write(output.text)\n\n# 5. With model, custom prompt\nreader = VanillaReader(model=model)\noutput = reader.read(file_path=file, prompt=\"Extract the content of this resource in html format\")\nwith open(os.path.join(output_dir, \"output_html_prompt.txt\"), \"w\", encoding=\"utf-8\") as f:\n    f.write(output.text)\n</code></pre>"},{"location":"examples/pdf/pdf_with_vlm/","title":"Example: Reading files with Visual Language Models to Provide Image Annotations","text":"<p>Warning</p> <p>This tutorial has been redone and it is deprecated. See new versions here:</p> <pre><code>1. [VanillaReader](./pdf_vanilla.md).\n2. [DoclingReader](./pdf_docling.md).\n3. [MarkItDownReader](./pdf_markitdown.md).\n</code></pre> <p>When reading a PDF file or other files which contain images, it can be useful to provide descriptive text alongside those images. Since images in a Markdown file are typically rendered by encoding them in base64 format, you may alternatively want to include a description of each image instead.</p> <p>This is where Visual Language Models (VLMs) come in\u2014to analyze and describe images automatically. In this tutorial, we'll show how to use these models with the library.</p>"},{"location":"examples/pdf/pdf_with_vlm/#step-1-load-a-model","title":"Step 1: Load a Model","text":"<p>To extract image descriptions or perform OCR, instantiate any model that implements the <code>BaseModel</code> interface (vision variants inherit from it).</p>"},{"location":"examples/pdf/pdf_with_vlm/#supported-models-and-when-to-use-them","title":"Supported models (and when to use them)","text":"Model (docs) When to use Required environment variables <code>OpenAIVisionModel</code> You have an OpenAI API key and want OpenAI cloud. <code>OPENAI_API_KEY</code> (optional: <code>OPENAI_MODEL</code>, defaults to <code>gpt-4o</code>) <code>AzureOpenAIVisionModel</code> You use Azure OpenAI Service. <code>AZURE_OPENAI_API_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_DEPLOYMENT</code>, <code>AZURE_OPENAI_API_VERSION</code> <code>GrokVisionModel</code> You have access to xAI Grok multimodal. <code>XAI_API_KEY</code> (optional: <code>XAI_MODEL</code>, default <code>grok-4</code>) <code>GeminiVisionModel</code> You want Google\u2019s Gemini vision models. <code>GEMINI_API_KEY</code> (also install extras: <code>pip install \"splitter-mr[multimodal]\"</code>) <code>AnthropicVisionModel</code> You have an Anthropic key (Claude Vision). <code>ANTHROPIC_API_KEY</code> (optional: <code>ANTHROPIC_MODEL</code>) <code>HuggingFaceVisionModel</code> You prefer local/open-source/offline inference. Install extras: <code>pip install \"splitter-mr[multimodal]\"</code> (optional: <code>HF_ACCESS_TOKEN</code> if the chosen model requires it) <p>Note on HuggingFace models: Not all HF models are supported (e.g., gated or uncommon architectures). A well-tested option is SmolDocling.</p>"},{"location":"examples/pdf/pdf_with_vlm/#environment-variables","title":"Environment variables","text":"Show/hide environment variables needed for every provider OpenAI <pre><code># OpenAI\nOPENAI_API_KEY=&lt;your-api-key&gt;\n# (optional) OPENAI_MODEL=gpt-4o\n</code></pre> Azure OpenAI <pre><code># Azure OpenAI\nAZURE_OPENAI_API_KEY=&lt;your-api-key&gt;\nAZURE_OPENAI_ENDPOINT=&lt;your-endpoint&gt;\nAZURE_OPENAI_API_VERSION=&lt;your-api-version&gt;\nAZURE_OPENAI_DEPLOYMENT=&lt;your-model-name&gt;\n</code></pre> xAI Grok <pre><code># xAI Grok\nXAI_API_KEY=&lt;your-api-key&gt;\n# (optional) XAI_MODEL=grok-4\n</code></pre> Google Gemini <pre><code># Google Gemini\nGEMINI_API_KEY=&lt;your-api-key&gt;\n# Also: pip install \"splitter-mr[multimodal]\"\n</code></pre> Anthropic (Claude Vision) <pre><code># Anthropic (Claude Vision)\nANTHROPIC_API_KEY=&lt;your-api-key&gt;\n# (optional) ANTHROPIC_MODEL=claude-sonnet-4-20250514\n</code></pre> Hugging Face (local/open-source) <pre><code># Hugging Face (optional, only if needed by the model)\nHF_ACCESS_TOKEN=&lt;your-hf-token&gt;\n# Also: pip install \"splitter-mr[multimodal]\"\n</code></pre>"},{"location":"examples/pdf/pdf_with_vlm/#instantiation-examples","title":"Instantiation examples","text":"Show/hide instantiation snippets for all providers OpenAI <pre><code>from splitter_mr.model import OpenAIVisionModel\n\n# Reads OPENAI_API_KEY (and optional OPENAI_MODEL) from .env if present\nmodel = OpenAIVisionModel()\n# or pass explicitly:\n# model = OpenAIVisionModel(api_key=\"...\", model_name=\"gpt-4o\")\n</code></pre> Azure OpenAI <pre><code>from splitter_mr.model import AzureOpenAIVisionModel\n\n# Reads Azure vars from .env if present\nmodel = AzureOpenAIVisionModel()\n# or:\n# model = AzureOpenAIVisionModel(\n#     api_key=\"...\",\n#     azure_endpoint=\"https://&lt;resource&gt;.openai.azure.com/\",\n#     api_version=\"2024-02-15-preview\",\n#     azure_deployment=\"&lt;your-deployment-name&gt;\",\n# )\n</code></pre> xAI Grok <pre><code>from splitter_mr.model import GrokVisionModel\n\n# Reads XAI_API_KEY (and optional XAI_MODEL) from .env\nmodel = GrokVisionModel()\n</code></pre> Google Gemini <pre><code>from splitter_mr.model import GeminiVisionModel\n\n# Requires GEMINI_API_KEY and the 'multimodal' extra installed\nmodel = GeminiVisionModel()\n</code></pre> Anthropic (Claude Vision) <pre><code>from splitter_mr.model import AnthropicVisionModel\n\n# Reads ANTHROPIC_API_KEY (and optional ANTHROPIC_MODEL) from .env\nmodel = AnthropicVisionModel()\n</code></pre> Hugging Face (local/open-source) <pre><code>from splitter_mr.model import HuggingFaceVisionModel\n\n# Token only if the model requires gating\nmodel = HuggingFaceVisionModel()\n</code></pre>"},{"location":"examples/pdf/pdf_with_vlm/#step-2-read-the-file-using-a-vlm","title":"Step 2: Read the file using a VLM","text":"<p>All the implemented Readers support VLMs. To use these VLMs with the Readers, you only need to create the <code>BaseReader</code> classes with an object from <code>BaseVisionModel</code> as argument.</p> <p>Firstly, we will use a <code>VanillaReader</code> class:</p>"},{"location":"examples/pdf/pdf_with_vlm/#read-a-file-using-vanillareader","title":"Read a file using VanillaReader","text":"<pre><code>from splitter_mr.reader import VanillaReader\nfrom splitter_mr.model import AzureOpenAIVisionModel\n\nFILE_PATH = \"data/pdfplumber_example.pdf\"\n\nmodel = AzureOpenAIVisionModel()\nreader = VanillaReader(model=model)\nreader_output = reader.read(file_path=FILE_PATH)\n\nprint(reader_output.text)\n</code></pre> <pre><code>&lt;!-- page --&gt;\n\nAn example of a PDF file\nThis is a PDF file\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam commodo egestas suscipit.\nMorbi sodales mi et lacus laoreet, eu molestie felis sodales. Aenean mattis gravida\ncongue. Suspendisse bibendum malesuada volutpat. Nunc aliquam iaculis ex, sed\nsollicitudin lorem congue et. Pellentesque imperdiet ac sem ac imperdiet. Sed vel enim\nvitae orci scelerisque convallis quis ac purus.\nCras sed neque vel justo auctor interdum a sit amet quam.\n...\ndisse potenti. Cras imperdiet enim vitae\nnunc elementum, non commodo ligula pretium. Vestibulum placerat nec tortor eu\ndapibus. Nullam et ipsum tortor. Nulla imperdiet enim velit, commodo facilisis elit\ntempus quis. Cras in interdum augue.\n\n&lt;!-- image --&gt;\n*Caption: A mysterious figure in a hoodie with glowing, round lenses, evoking a blend of futuristic technology and anonymity.*\n\n| It seems like | This is a table | But I am not sure |\n| --- | --- | --- |\n| About this | What do you think | ? |\n</code></pre> <p>Warning</p> <p>If you dont have the file locally, it is possible that instead of loading the content of the file, it will show only the document path. In order to avoid this behavior, please, use a correct file path on the file to be read. </p> <p>In this case we have read a PDF with an image at the end of the file. When reading the file and priting the content, we can see that the image has been described by the VLM.</p> <p>When using a <code>VanillaReader</code> class, the image is highlighted with a <code>&gt; **Caption**:</code> placeholder by default. But the prompt can be changed using the keyword argument <code>prompt</code>. For example, you can say that you want the Caption to be signalised as a comment <code>&lt;!--- Caption: &gt;:</code></p> <pre><code>from splitter_mr.reader import VanillaReader\n\nPROMPT: str = \"Describe the resource in a concise way: e.g., &lt;!---- Caption: Image shows ...!---&gt;:\"\n\nreader = VanillaReader(model=model)\nreader_output = reader.read(file_path=FILE_PATH, prompt=PROMPT)\n\nprint(reader_output.text)\n</code></pre> <pre><code>&lt;!-- page --&gt;\n\nAn example of a PDF file\nThis is a PDF file\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam commodo egestas suscipit.\nMorbi sodales mi et lacus laoreet, eu molestie felis sodales. Aenean mattis gravida\ncongue. Suspendisse bibendum malesuada volutpat. Nunc aliquam iaculis ex, sed\nsollicitudin lorem congue et. Pellentesque imperdiet ac sem ac imperdiet. Sed vel enim\nvitae orci scelerisque convallis quis ac purus.\nCras sed neque vel justo auctor interdum a sit amet quam.\n...\nor eu\ndapibus. Nullam et ipsum tortor. Nulla imperdiet enim velit, commodo facilisis elit\ntempus quis. Cras in interdum augue.\n\n&lt;!-- image --&gt;\n&lt;!---- Caption: Image shows a figure wearing a teal hoodie, with their hands on their head and a black face featuring glowing circular eyes, set against a dark background. The figure's expression conveys a sense of surprise or shock. ---!&gt;\n\n| It seems like | This is a table | But I am not sure |\n| --- | --- | --- |\n| About this | What do you think | ? |\n</code></pre>"},{"location":"examples/pdf/pdf_with_vlm/#read-a-file-using-markitdownreader","title":"Read a file using MarkItDownReader","text":"<p>In this case, we will read an image file to provide a complete description. So, you simply instantiate the object and pass a model which inherits from a <code>BaseVisionModel</code> object.</p> <pre><code>from splitter_mr.reader import MarkItDownReader\n\nFILE_PATH = \"data/chameleon.jpg\"\n\nmd = MarkItDownReader(model=model)\nmd_reader_output = md.read(file_path=FILE_PATH, prompt=PROMPT)\n\nprint(md_reader_output.text)\n</code></pre> <pre><code>&lt;!-- page --&gt;\n\n# Description:\n&lt;!---- Caption: Image shows a vibrant, colorful lizard peering out from a pink and orange floral background, showcasing its bright features and intricate details against a soft, blurred setting. ---!&gt;\n</code></pre> <p>Original image is:</p> <p></p> <p>As we can see, <code>MarkItDownReader</code> provides a very complete but verbose description of the files that you provide. In addition, it is not capable to analyze the image contents inside a PDF. In contrast, you should provide the image separatedly. </p> <p>Warning</p> <p>You can NOT modify the prompt of the VLM in this method.</p>"},{"location":"examples/pdf/pdf_with_vlm/#read-the-file-using-doclingreader","title":"Read the file using DoclingReader","text":"<p>The same process can be applied to DoclingReader. This time, we will analyze an invoice. So, the code is the following:</p> <pre><code>from splitter_mr.reader import DoclingReader\n\nFILE_PATH = \"data/sunny_farm.pdf\"\n\ndocling = DoclingReader(model=model)\ndocling_output = docling.read(file_path=FILE_PATH)\n\nprint(docling_output.text)\n</code></pre> <pre><code>&lt;!-- image --&gt;\n*Caption: A decorative golden banner, ideal for adding a classic touch to titles or announcements.*\n\n&lt;!-- image --&gt;\n*Caption: A vibrant logo for Sunny Farm, showcasing fresh produce from Victoria, Australia, with a sun emblem symbolizing freshness and quality.*\n\n## 123 Somewhere St, Melbourne VIC 3000 (03) 1234 5678\n\n## Denny Gunawan\n\n221 Queen St Melbourne VIC 3000\n\n$39.60\n\nInvoice Number: #20130304\n\n| Organic Items   | Price/kg   |   Quantity(kg) | Subtotal   |\n|----------------\n...\n image --&gt;\n*Caption: A bold and expressive typography design conveying gratitude, perfect for expressing appreciation and thanks.*\n\n* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam sodales dapibus fermentum. Nunc adipiscing, magna sed scelerisque cursus, erat lectus dapibus urna, sed facilisis leo dui et ipsum.\n\nSubtotal\n\n$36.00\n\nGST (10%)\n\n$3.60\n\nTotal\n\n$39.60\n\n&lt;!-- image --&gt;\n*Caption: A decorative brown banner, perfect for adding a rustic touch to announcements or displays.*\n</code></pre> <p>The result is pretty similar to the observed PDF (https://raw.githubusercontent.com/andreshere00/Splitter_MR/blob/main/data/sunny_farm.pdf)</p> <p>As the same way as <code>VanillaReader</code>, you can change the prompt to provide larger descriptions or whatever you want to. For example:</p> <pre><code>file = \"data/sunny_farm.pdf\"\n\ndocling = DoclingReader(model=model)\ndocling_output = docling.read(file, prompt=\"Provide a long description\")\n\nprint(docling_output.text)\n</code></pre> <pre><code>&lt;!-- image --&gt;\nThe image presents a decorative, elongated banner that has a rustic yet elegant appearance. The banner is designed in a subtle shade of gold, reminiscent of aged parchment or well-worn fabric. Its surface features a natural texture, giving it an organic and artisanal quality. The edges of the banner are slightly frayed, suggesting it has been hand-crafted, adding a touch of vintage charm to its overall aesthetic.\n\nThe banner curls gracefully at both ends, creating a sense of move\n...\ncould also serve as a visual focal point in designs, drawing the viewer\u2019s eye and conveying messages of nostalgia, warmth, and the beauty of tradition.\n\nIn summary, this exquisite ribbon captures the essence of storytelling and personal connection, making it a timeless element perfect for a myriad of creative applications. Its versatile design and beautiful color palette enable it to complement various themes and occasions, ensuring that it remains a cherished item in any decorative collection.\n</code></pre>"},{"location":"examples/pdf/pdf_with_vlm/#conclusion","title":"Conclusion","text":"<p>Although all three methods can read files from various sources, they differ significantly in how VLM analysis is implemented:</p> <ul> <li> <p><code>VanillaReader</code> extracts graphical files from the input and uses a VLM to provide descriptions for these resources. Currently, it is only compatible with PDFs, and the VLM analysis and PDF reading logic are separated. It is the most scalable method for reading files, as it performs a call for every graphical resource in your PDF. However, this can become expensive for documents with a large number of images.</p> </li> <li> <p><code>MarkItDownReader</code> can only transform images into Markdown descriptions. Supported image formats include <code>png</code>, <code>jpg</code>, <code>jpeg</code>, and <code>svg</code>. It cannot provide hybrid methods for reading PDFs with image annotations. While it is fast and cost-effective, it can only process one file at a time and is limited to OpenAI models.</p> </li> <li> <p><code>DoclingReader</code> can read any file you provide using VLMs. If given a PDF, it reads the entire document with the VLM; the same applies to images and other graphical resources. However, it does not distinguish between text and image content, as the analysis is multimodal. As a result, in some cases, it cannot provide specific descriptions for images but instead analyzes the whole document.</p> </li> </ul> <p>Using one or another method depends on your needs!</p> <p>In case that you want more information about available Models, visit Developer guide. Thank you for reading!</p>"},{"location":"examples/pdf/pdf_with_vlm/#complete-script","title":"Complete script","text":"<pre><code>from splitter_mr.model import AzureOpenAIVisionModel\nfrom splitter_mr.reader import DoclingReader, MarkItDownReader, VanillaReader\n\n# Define the model\nmodel = AzureOpenAIVisionModel()\n\n# Readers\n\n## Vanilla Reader\n\nFILE_PATH = \"data/pdfplumber_example.pdf\"\n\nreader = VanillaReader(model = model)\nreader_output = reader.read(file_path = FILE_PATH)\n\nprint(reader_output.text)\n\nPROMPT: str = \"Describe the resource in a concise way: e.g., &lt;!---- Caption: Image shows ...!---&gt;:\"\n\nreader_output_with_dif_prompt = reader.read(\n    FILE_PATH, \n    prompt = PROMPT\n)\n\nprint(reader_output_with_dif_prompt.text)\n\n## MarkItDown Reader\n\nFILE_PATH = \"data/chameleon.jpg\"\n\nmd = MarkItDownReader(model = model)\nmd_reader_output = md.read(file_path = FILE_PATH)\n\nprint(md_reader_output.text)\n\n## Docling Reader\n\nFILE_PATH = \"data/sunny_farm.pdf\"\n\ndocling = DoclingReader(model = model)\ndocling_output = docling.read(\n    file_path = FILE_PATH, \n    prompt = \"Provide a long description\"\n)\n\nprint(docling_output.text)\n</code></pre>"},{"location":"examples/pdf/pdf_without_vlm/","title":"Example: Reading a PDF using several Reading methods","text":"<p>Converting a PDF into a readable format is not an easy task. PDF introduces compression, which often results in a complete loss of formatting. As a result, many tools have been developed to convert PDF to text, each of which works differently.</p> <p>In this example, we will show how to read a PDF file using three readers: <code>VanillaReader</code>, <code>MarkItDownReader</code>, and <code>DoclingReader</code>, and we will observe the differences between each.</p> <p>Note</p> <p>A complete description of each of these classes is defined in the Developer guide.</p>"},{"location":"examples/pdf/pdf_without_vlm/#1-read-pdf-files-using-vanillareader","title":"1. Read PDF files using <code>VanillaReader</code>","text":"<p><code>VanillaReader</code> uses open-source libraries to read many file formats, aiming to preserve the text as a string. However, converting a PDF directly to text results in a complete loss of readability. So, to read PDFs, <code>VanillaReader</code> uses PDFPlumber as the core library. PDFPlumber is a Python library that extracts text, tables, and metadata from PDF files while preserving their layout as much as possible. It is widely used for converting PDF content into readable and structured formats for further processing. Let's see how it works and what results it produces:</p> <p>First, we instantiate our <code>VanillaReader</code> object: </p> <pre><code>from splitter_mr.reader import VanillaReader\n\nreader = VanillaReader()\n</code></pre> <p>To read the file, you simply call to the <code>read</code> method:</p> <pre><code>file = \"data/sample_pdf.pdf\"\nreader_output = reader.read(file)\n</code></pre> <p>The result will be a <code>ReaderOutput</code> object with the following structure:</p> <pre><code>print(reader_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"text\": \"&lt;!-- page --&gt;\\n\\nA sample PDF\\nConverting PDF files to other formats, such as Markdown, is a surprisingly\\ncomplex task due to the nature of the PDF format itself. PDF (Portable\\nDocument Format) was designed primarily for preserving the visual layout of\\ndocuments, making them look the same across different devices and\\nplatforms. However, this design goal introduces several challenges when trying to\\nextract and convert the underlying content into a more flexible, structured for\n...\nstructure recognition) to produce\\nusable, readable, and faithful Markdown output. As a result, perfect conversion\\nis rarely possible, and manual review and cleanup are often required.\\n\\n&lt;!-- image --&gt;\\n\",\n    \"document_name\": \"sample_pdf.pdf\",\n    \"document_path\": \"data/sample_pdf.pdf\",\n    \"document_id\": \"df3944d6-eb56-4d25-ae87-27db76138063\",\n    \"conversion_method\": \"pdf\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"page_placeholder\": \"&lt;!-- page --&gt;\",\n    \"metadata\": {}\n}\n</code></pre> <p>So, we can print the text using this command:</p> <pre><code>print(reader_output.text)\n</code></pre> <pre><code>&lt;!-- page --&gt;\n\nA sample PDF\nConverting PDF files to other formats, such as Markdown, is a surprisingly\ncomplex task due to the nature of the PDF format itself. PDF (Portable\nDocument Format) was designed primarily for preserving the visual layout of\ndocuments, making them look the same across different devices and\nplatforms. However, this design goal introduces several challenges when trying to\nextract and convert the underlying content into a more flexible, structured format\nlike Markdown.\n\n&lt;!-\n...\nxample.com |\n\nConclusion\nWhile it may seem simple on the surface, converting PDFs to formats like\nMarkdown involves a series of technical and interpretive challenges. Effective\nconversion tools must blend text extraction, document analysis, and sometimes\nmachine learning techniques (such as OCR or structure recognition) to produce\nusable, readable, and faithful Markdown output. As a result, perfect conversion\nis rarely possible, and manual review and cleanup are often required.\n\n&lt;!-- image --&gt;\n</code></pre> <p>As we can see from the original file, all the text has been preserved. Bold, italics, etc. are not highlighted, nor are text colors, headers, and font type. Despite that, the format is mostly plain text rather than markdown. In addition, we can observe that images are signaled by a <code>&lt;!-- image --&gt;</code> placeholder, which can be useful to identify where a image has been placed. In the same way, pages are marked with another placeholder: <code>&lt;!-- page --&gt;</code>. The order of the document is preserved.</p> <p>Now, let's see how well the other readers handle markdown conversion:</p>"},{"location":"examples/pdf/pdf_without_vlm/#2-read-pdf-files-using-markitdownreader","title":"2. Read PDF files using <code>MarkItDownReader</code>","text":"<p>The process is analogous to <code>VanillaReader</code>. So, we instantiate the <code>MarkItDownReader</code> class and we call to the read method:</p> <pre><code>from splitter_mr.reader import MarkItDownReader\n\nreader = MarkItDownReader()\nreader_output = reader.read(file)\n\nprint(reader_output.text)\n</code></pre> <pre><code>A sample PDF\n\nConverting PDF files to other formats, such as Markdown, is a surprisingly\ncomplex task due to the nature of the PDF format itself. PDF (Portable\nDocument Format) was designed primarily for preserving the visual layout of\ndocuments, making them look the same across different devices and\nplatforms. However, this design goal introduces several challenges when trying to\nextract and convert the underlying content into a more flexible, structured format\nlike Markdown.\n\nIlustraci\u00f3n 1. Sp\n...\nct Lead  carol@example.com\n\nConclusion\n\nWhile it may seem simple on the surface, converting PDFs to formats like\nMarkdown involves a series of technical and interpretive challenges. Effective\nconversion tools must blend text extraction, document analysis, and sometimes\nmachine learning techniques (such as OCR or structure recognition) to produce\nusable, readable, and faithful Markdown output. As a result, perfect conversion\nis rarely possible, and manual review and cleanup are often required.\n</code></pre> <p>Again, all the text has been preserved. However, we can observe some inconsistencies in line spacing: sometimes there is a single line of separation, while in other cases there are two. Similarly to <code>VanillaReader</code>, text formatting has not been preserved: no headers, no italics, no bold... It is simply plain text.</p>"},{"location":"examples/pdf/pdf_without_vlm/#3-read-pdf-files-using-doclingreader","title":"3. Read PDF files using <code>DoclingReader</code>","text":"<p><code>docling</code> is an open-source Python library designed to analyze and extract structured information from documents, including PDFs. It focuses on preserving the original layout, structure, and semantic elements of documents, making it useful for handling complex formats beyond plain text extraction.</p> <p>Let's see how it works for this use case:</p> <pre><code>from splitter_mr.reader import DoclingReader\n\nFILE_PATH = \"data/sample_pdf.pdf\"\n\nreader = DoclingReader()\nreader_output = reader.read(file_path=FILE_PATH)\nprint(reader_output.text)\n</code></pre> <pre><code>## A sample PDF\n\nConverting PDF files to other formats, such as Markdown, is a surprisingly complex task due to the nature of the PDF format itself . PDF (Portable Document Format) was designed primarily for preserving the visual layout of documents, making them look the same across different devices and platforms. However, this design goal introduces several challenges when trying to extract and convert the underlying content into a more flexible, structured format like Markdown.\n\nIlustraci\u00f3n 1\n...\nple.com |\n\n## Conclusion\n\nWhile it may seem simple on the surface, converting PDFs to formats like Markdown involves a series of technical and interpretive challenges. Effective conversion tools must blend text extraction, document analysis, and sometimes machine learning techniques (such as OCR or structure recognition) to produce usable, readable, and faithful Markdown output. As a result, perfect conversion is rarely possible, and manual review and cleanup are often required.\n\n&lt;!-- image --&gt;\n</code></pre> <p>We can see that the layout is generally better. All the text has been preserved, but markdown format is more present. We can see that headers, tables and lists are markdown formatted, despite bold or italics are not showing. In addition, formulas (<code>&lt;!-- formula-not-decoded --&gt;</code>) and images (<code>&lt;!-- Image --&gt;</code>) are detected too, despite no description or rendering is provided. Sometimes the line spacing is inconsistent as it was in MarkItDown. However, in general terms, it could be said that it is the method that best formats Markdown.</p> <p>So, does this mean you should always use this method to parse PDFs? Not exactly. Let's analyze an additional metric: computation time.</p>"},{"location":"examples/pdf/pdf_without_vlm/#4-measuring-compute-time","title":"4. Measuring compute time","text":"<p>To measure the compute time for every method, we can encapsulate every reading logic into a function and define a decorator which computes a function execution time. Then, we can compare compute times in relative terms. Then, we can compare compute times in relative terms by executing the following code:</p> <pre><code>import time\n\nfrom splitter_mr.reader import DoclingReader, MarkItDownReader, VanillaReader\n\n\ndef timeit(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        elapsed = time.time() - start\n        print(f\"Time taken by '{func.__name__}': {elapsed:.4f} seconds\\n\")\n        return result\n\n    return wrapper\n\n\n@timeit\ndef get_reader_output(file, reader=VanillaReader()):\n    output = reader.read(file)\n    print()\n    return output.text\n\n\nFILE_PATH = \"data/sample_pdf.pdf\"\n\nprint(\"*\" * 20 + \" Vanilla Reader \" + \"*\" * 20)\nvanilla_output = get_reader_output(file, reader=VanillaReader())\n\nprint(\"*\" * 20 + \" MarkItDown Reader \" + \"*\" * 20)\nmarkitdown_output = get_reader_output(file, reader=MarkItDownReader())\n\nprint(\"*\" * 20 + \" Docling Reader \" + \"*\" * 20)\nmarkitdown_output = get_reader_output(file, reader=DoclingReader())\n</code></pre> <pre><code>******************** Vanilla Reader ********************\n\nTime taken by 'get_reader_output': 0.1170 seconds\n\n******************** MarkItDown Reader ********************\n\nTime taken by 'get_reader_output': 0.0624 seconds\n\n******************** Docling Reader ********************\n\nTime taken by 'get_reader_output': 4.4744 seconds\n</code></pre> <p>As we can observe, although DoclingReader offers a really good conversion, it's a resource-intensive method, and therefore takes the longest to return the result. On the other hand, MarkItDownReader, although it preserves the markdown format the least, is the fastest of all. <code>VanillaReader</code> offers a balance between computation time and format preservation.</p>"},{"location":"examples/pdf/pdf_without_vlm/#5-comparison-between-methods","title":"5. Comparison between methods","text":"<p>As we've seen, each method has its advantages and disadvantages. Therefore, choosing a reading method depends on the specific needs of the user.</p> <ul> <li>If you prioritize conversion quality regardless of execution time, <code>DoclingReader</code> will be the best option.</li> <li>If you want a fast conversion that preserves only the text, <code>MarkItDownReader</code> may be your best option.</li> <li>If you want a fast conversion but need to detect images and other graphic elements, <code>VanillaReader</code> is suitable.</li> </ul> <p>Finally, here we present a comparative table of each method, with the strengths and weaknesses of each one:</p> Feature <code>VanillaReader</code> <code>MarkItDownReader</code> <code>DoclingReader</code> Header preservation low mid high Text formatting (bold, italic, etc.) no no partial Text color &amp; highlighting no no no Markdown tables yes no (txt format) yes Markdown lists partial no yes Image placeholders yes no yes Formulas placeholders no no yes Pagination yes yes (<code>split_by_pages = True</code>) yes Execution time low the lowest the highest <p>With this information, we know which method to use. However, there is an element that we have not yet analyzed: the description and annotation of images. Currently, all three methods can describe and annotate images using VLMs. To see how to do this, jump to the next tutorial.</p> <p>Thanks for Reading!</p>"},{"location":"examples/schema/code_splitter/","title":"Example: Splitting a Python Source File into Chunks with <code>CodeSplitter</code>","text":"<p>Suppose you have a Python code file and want to split it into chunks that respect function and class boundaries (rather than just splitting every N characters). The <code>CodeSplitter</code> leverages LangChain's RecursiveCharacterTextSplitter to achieve this, making it ideal for preparing code for LLM ingestion, code review, or annotation.</p> <p></p>"},{"location":"examples/schema/code_splitter/#step-1-read-the-python-source-file","title":"Step 1: Read the Python Source File","text":"<p>We will use the <code>VanillaReader</code> to load our code file. You can provide a local file path (or a URL if your implementation supports it).</p> <p>Note</p> <p>In case that you use <code>MarkItDownReader</code> or <code>DoclingReader</code>, save your files in <code>txt</code> format.</p> <pre><code>from splitter_mr.reader import VanillaReader\n\nreader = VanillaReader()\nreader_output = reader.read(\n    \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/code_example.py\"\n)\n</code></pre> <p>The <code>reader_output</code> is an object containing the raw code and its metadata:</p> <pre><code>print(reader_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"text\": \"from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\\n\\nfrom ...schema import ReaderOutput, SplitterOutput\\nfrom ..base_splitter import BaseSplitter\\n\\n\\ndef get_langchain_language(lang_str: str) -&gt; Language:\\n    \\\"\\\"\\\"\\n    Map a string language name to Langchain Language enum.\\n    Raises ValueError if not found.\\n    \\\"\\\"\\\"\\n    lookup = {lang.name.lower(): lang for lang in Language}\\n    key = lang_str.lower()\\n    if key not in lookup:\\n        raise\n...\nsplit_params={\\\"chunk_size\\\": chunk_size, \\\"language\\\": self.language},\\n            metadata=metadata,\\n        )\\n        return output\\n\",\n    \"document_name\": \"code_example.py\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/code_example.py\",\n    \"document_id\": \"fadd9a15-06ba-488a-8c9c-9fd09ebbe82c\",\n    \"conversion_method\": \"txt\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"page_placeholder\": null,\n    \"metadata\": {}\n}\n</code></pre> <p>To see the code content:</p> <pre><code>print(reader_output.text)\n</code></pre> <pre><code>from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n\nfrom ...schema import ReaderOutput, SplitterOutput\nfrom ..base_splitter import BaseSplitter\n\n\ndef get_langchain_language(lang_str: str) -&gt; Language:\n    \"\"\"\n    Map a string language name to Langchain Language enum.\n    Raises ValueError if not found.\n    \"\"\"\n    lookup = {lang.name.lower(): lang for lang in Language}\n    key = lang_str.lower()\n    if key not in lookup:\n        raise ValueError(\n            f\"Unsuppor\n...\ncument_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"code_splitter\",\n            split_params={\"chunk_size\": chunk_size, \"language\": self.language},\n            metadata=metadata,\n        )\n        return output\n</code></pre>"},{"location":"examples/schema/code_splitter/#step-2-chunk-the-code-using-codesplitter","title":"Step 2: Chunk the Code Using <code>CodeSplitter</code>","text":"<p>To split your code by language-aware logical units, instantiate the <code>CodeSplitter</code>, specifying the <code>chunk_size</code> (maximum number of characters per chunk) and <code>language</code> (e.g., <code>\"python\"</code>):</p> <pre><code>from splitter_mr.splitter import CodeSplitter\n\nsplitter = CodeSplitter(chunk_size=1000, language=\"python\")\nsplitter_output = splitter.split(reader_output)\n</code></pre> <p>The <code>splitter_output</code> contains the split code chunks:</p> <pre><code>print(splitter_output)\n</code></pre> <pre><code>chunks=['from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\\n\\nfrom ...schema import ReaderOutput, SplitterOutput\\nfrom ..base_splitter import BaseSplitter\\n\\n\\ndef get_langchain_language(lang_str: str) -&gt; Language:\\n    \"\"\"\\n    Map a string language name to Langchain Language enum.\\n    Raises ValueError if not found.\\n    \"\"\"\\n    lookup = {lang.name.lower(): lang for lang in Language}\\n    key = lang_str.lower()\\n    if key not in lookup:\\n        raise ValueError(\n...\n945-9485-e915b616319d', 'c2a4cdb9-1cea-40ff-8474-949de5cb3cbb', 'cf065bed-bf46-4984-b3ca-f38297737b56', 'ef15e01d-98ad-4112-a4fd-cef4c2179772'] document_name='code_example.py' document_path='https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/code_example.py' document_id='fadd9a15-06ba-488a-8c9c-9fd09ebbe82c' conversion_method='txt' reader_method='vanilla' ocr_method=None split_method='code_splitter' split_params={'chunk_size': 1000, 'language': 'python'} metadata={}\n</code></pre> <p>To inspect the split results, iterate over the chunks and print them:</p> <pre><code>for idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx} \" + \"=\" * 40)\n    print(chunk)\n</code></pre> <pre><code>======================================== Chunk 0 ========================================\nfrom langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n\nfrom ...schema import ReaderOutput, SplitterOutput\nfrom ..base_splitter import BaseSplitter\n\n\ndef get_langchain_language(lang_str: str) -&gt; Language:\n    \"\"\"\n    Map a string language name to Langchain Language enum.\n    Raises ValueError if not found.\n    \"\"\"\n    lookup = {lang.name.lower(): lang for lang in Language}\n    key = l\n...\nocument_name=reader_output.document_name,\n            document_path=reader_output.document_path,\n            document_id=reader_output.document_id,\n            conversion_method=reader_output.conversion_method,\n            reader_method=reader_output.reader_method,\n            ocr_method=reader_output.ocr_method,\n            split_method=\"code_splitter\",\n            split_params={\"chunk_size\": chunk_size, \"language\": self.language},\n            metadata=metadata,\n        )\n        return output\n</code></pre> <p>And that's it! You now have an efficient, language-aware way to chunk your code files for downstream tasks. </p> <p>Remember that you have plenty of programming languages available: JavaScript, Go, Rust, Java, etc. Currently, the available programming languages are:</p> <pre><code>from typing import Set\n\nSUPPORTED_PROGRAMMING_LANGUAGES: Set[str] = {\n    \"lua\",\n    \"java\",\n    \"ts\",\n    \"tsx\",\n    \"ps1\",\n    \"psm1\",\n    \"psd1\",\n    \"ps1xml\",\n    \"php\",\n    \"php3\",\n    \"php4\",\n    \"php5\",\n    \"phps\",\n    \"phtml\",\n    \"rs\",\n    \"cs\",\n    \"csx\",\n    \"cob\",\n    \"cbl\",\n    \"hs\",\n    \"scala\",\n    \"swift\",\n    \"tex\",\n    \"rb\",\n    \"erb\",\n    \"kt\",\n    \"kts\",\n    \"go\",\n    \"html\",\n    \"htm\",\n    \"rst\",\n    \"ex\",\n    \"exs\",\n    \"md\",\n    \"markdown\",\n    \"proto\",\n    \"sol\",\n    \"c\",\n    \"h\",\n    \"cpp\",\n    \"cc\",\n    \"cxx\",\n    \"c++\",\n    \"hpp\",\n    \"hh\",\n    \"hxx\",\n    \"js\",\n    \"mjs\",\n    \"py\",\n    \"pyw\",\n    \"pyc\",\n    \"pyo\",\n    \"pl\",\n    \"pm\",\n}\n</code></pre> <p>Note</p> <p>Remember that you can visit the LangchainTextSplitter documentation to see the up-to-date information about the available programming languages to split on.</p>"},{"location":"examples/schema/code_splitter/#complete-script","title":"Complete Script","text":"<p>Here is a full example you can run directly:</p> <pre><code>from splitter_mr.reader import VanillaReader\nfrom splitter_mr.splitter import CodeSplitter\n\n# Step 1: Read the code file\nreader = VanillaReader()\nreader_output = reader.read(\"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/code_example.py\")\n\nprint(reader_output.model_dump_json(indent=4))  # See metadata\nprint(reader_output.text)  # See raw code\n\n# Step 2: Split code into logical chunks, max 1000 chars per chunk\nsplitter = CodeSplitter(chunk_size=1000, language=\"python\")\nsplitter_output = splitter.split(reader_output)\n\nprint(splitter_output)  # Print the SplitterOutput object\n\n# Step 3: Visualize code chunks\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\"*40 + f\" Chunk {idx} \" + \"=\"*40)\n    print(chunk)\n</code></pre>"},{"location":"examples/schema/code_splitter/#references","title":"References","text":"<p>LangChain's RecursiveCharacterTextSplitter </p>"},{"location":"examples/schema/header_splitter/","title":"Example: Splitting Structured Documents by Header Levels with <code>HeaderSplitter</code>","text":"<p>Large HTML or Markdown documents often contain multiple sections delineated by headers (<code>&lt;h1&gt;</code>, <code>&lt;h2&gt;</code>, <code>#</code>, <code>##</code>, etc.). Chunking these documents by their headers makes them easier to process, search, or send to an LLM. SplitterMR\u2019s <code>HeaderSplitter</code> (or <code>TagSplitter</code>) allows you to define semantic header levels and split documents accordingly\u2014without manual regex or brittle parsing.</p> <p>This Splitter class implements two different Langchain text splitters. See documentation below:</p> <ul> <li>HTML Header Text Splitter</li> <li>Markdown Header Text Splitter</li> </ul>"},{"location":"examples/schema/header_splitter/#splitting-html-files","title":"Splitting HTML Files","text":""},{"location":"examples/schema/header_splitter/#step-1-read-an-html-file","title":"Step 1: Read an HTML File","text":"<p>We will use the <code>VanillaReader</code> to load a sample HTML file:</p> <pre><code>from splitter_mr.reader import VanillaReader\n\nfile = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/webpage_example.html\"\nreader = VanillaReader()  # you can use the argument html_to_markdown=True\nreader_output = reader.read(file)\n\n# Print metadata and content\nprint(reader_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"text\": \"&lt;!DOCTYPE html&gt;\\n  &lt;html lang='en'&gt;\\n  &lt;head&gt;\\n    &lt;meta charset='UTF-8'&gt;\\n    &lt;meta name='viewport' content='width=device-width, initial-scale=1.0'&gt;\\n    &lt;title&gt;Fancy Example HTML Page&lt;/title&gt;\\n  &lt;/head&gt;\\n  &lt;body&gt;\\n    &lt;h1&gt;Main Title&lt;/h1&gt;\\n    &lt;p&gt;This is an introductory paragraph with some basic content.&lt;/p&gt;\\n    \\n    &lt;h2&gt;Section 1: Introduction&lt;/h2&gt;\\n    &lt;p&gt;This section introduces the topic. Below is a list:&lt;/p&gt;\\n    &lt;ul&gt;\\n      &lt;li&gt;First item&lt;/li&gt;\\n      &lt;li&gt;Second item&lt;/li&gt;\\n\n...\n/div&amp;gt;\\n    &lt;/code&gt;&lt;/pre&gt;\\n\\n    &lt;h2&gt;Conclusion&lt;/h2&gt;\\n    &lt;p&gt;This is the conclusion of the document.&lt;/p&gt;\\n  &lt;/body&gt;\\n  &lt;/html&gt;\",\n    \"document_name\": \"webpage_example.html\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/webpage_example.html\",\n    \"document_id\": \"a4b4563e-a1d3-4292-92eb-721a27d5367c\",\n    \"conversion_method\": \"html\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"page_placeholder\": null,\n    \"metadata\": {}\n}\n</code></pre> <p>The <code>text</code> attribute contains the raw HTML, including headers, paragraphs, lists, tables, images, and more:</p> <pre><code>print(reader_output.text)\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n  &lt;html lang='en'&gt;\n  &lt;head&gt;\n    &lt;meta charset='UTF-8'&gt;\n    &lt;meta name='viewport' content='width=device-width, initial-scale=1.0'&gt;\n    &lt;title&gt;Fancy Example HTML Page&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Main Title&lt;/h1&gt;\n    &lt;p&gt;This is an introductory paragraph with some basic content.&lt;/p&gt;\n\n    &lt;h2&gt;Section 1: Introduction&lt;/h2&gt;\n    &lt;p&gt;This section introduces the topic. Below is a list:&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;First item&lt;/li&gt;\n      &lt;li&gt;Second item&lt;/li&gt;\n      &lt;li&gt;Third item with &lt;stro\n...\nlink.mp4' alt='Example Image'&gt;\n      &lt;video controls width='250' src='example_video_link.mp4' type='video/mp4'&gt;\n      Your browser does not support the video tag.\n    &lt;/video&gt;\n\n    &lt;h2&gt;Section 3: Code Example&lt;/h2&gt;\n    &lt;p&gt;This section contains a code block:&lt;/p&gt;\n    &lt;pre&gt;&lt;code data-lang=\"html\"&gt;\n    &amp;lt;div&amp;gt;\n      &amp;lt;p&amp;gt;This is a paragraph inside a div.&amp;lt;/p&amp;gt;\n    &amp;lt;/div&amp;gt;\n    &lt;/code&gt;&lt;/pre&gt;\n\n    &lt;h2&gt;Conclusion&lt;/h2&gt;\n    &lt;p&gt;This is the conclusion of the document.&lt;/p&gt;\n  &lt;/body&gt;\n  &lt;/html&gt;\n</code></pre>"},{"location":"examples/schema/header_splitter/#step-2-split-the-html-file-by-header-levels","title":"Step 2: Split the HTML File by Header Levels","text":"<p>We create a <code>HeaderSplitter</code> and specify which semantic headers to split on (e.g., <code>\"Header 1\"</code>, <code>\"Header 2\"</code>, <code>\"Header 3\"</code>). There are up to 6 levels of headers available:</p> <pre><code>from splitter_mr.splitter import HeaderSplitter\n\nsplitter = HeaderSplitter(headers_to_split_on=[\"Header 1\", \"Header 2\", \"Header 3\"])\nsplitter_output = splitter.split(reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\nhtml\nFancy Example HTML Page\n\n======================================== Chunk 2 ========================================\n# Main Title  \nThis is an introductory paragraph with some basic content.\n\n======================================== Chunk 3 ========================================\n## Section 1: Introduction  \nThis section introduces the topic. Below is a list:  \n- First item\n- Second item\n- Third item wi\n...\nis section contains an image and a video:  \n![Example Image](example_image_link.mp4)\nYour browser does not support the video tag.\n\n======================================== Chunk 6 ========================================\n## Section 3: Code Example  \nThis section contains a code block:  \n```\n\n&lt;div&gt;\n&lt;p&gt;This is a paragraph inside a div.&lt;/p&gt;\n&lt;/div&gt;\n\n```\n\n======================================== Chunk 7 ========================================\n## Conclusion  \nThis is the conclusion of the document.\n</code></pre> <p>Each chunk corresponds to a logical section or sub-section in the HTML, grouped by headers and their associated content.</p>"},{"location":"examples/schema/header_splitter/#splitting-markdown-file","title":"Splitting Markdown File","text":"<p>The exact same interface works for Markdown files. Just change the path:</p> <pre><code>print(\"Markdown file example\")\n\nfile = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/markdown_example.md\"\nreader = VanillaReader()\nreader_output = reader.read(file)\n\nprint(reader_output.model_dump_json(indent=4))\n</code></pre> <pre><code>Markdown file example\n{\n    \"text\": \"---\\n__Advertisement :)__\\n\\n- __[pica](https://nodeca.github.io/pica/demo/)__ - high quality and fast image\\n  resize in browser.\\n- __[babelfish](https://github.com/nodeca/babelfish/)__ - developer friendly\\n  i18n with plurals support and easy syntax.\\n\\nYou will like those projects!\\n\\n---\\n\\n# h1 Heading 8-)\\n## h2 Heading\\n### h3 Heading\\n#### h4 Heading\\n##### h5 Heading\\n###### h6 Heading\\n\\n\\n## Horizontal Rules\\n\\n___\\n\\n---\\n\\n***\\n\\n\\n## Typograph\n...\n Language\\n\\n### [Custom containers](https://github.com/markdown-it/markdown-it-container)\\n\\n::: warning\\n*here be dragons*\\n:::\\n\",\n    \"document_name\": \"markdown_example.md\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/markdown_example.md\",\n    \"document_id\": \"0be3de0f-2699-4966-9e6c-efdfaf052bfb\",\n    \"conversion_method\": \"txt\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"page_placeholder\": null,\n    \"metadata\": {}\n}\n</code></pre> <pre><code>print(reader_output.text)\n</code></pre> <pre><code>---\n__Advertisement :)__\n\n- __[pica](https://nodeca.github.io/pica/demo/)__ - high quality and fast image\n  resize in browser.\n- __[babelfish](https://github.com/nodeca/babelfish/)__ - developer friendly\n  i18n with plurals support and easy syntax.\n\nYou will like those projects!\n\n---\n\n# h1 Heading 8-)\n## h2 Heading\n### h3 Heading\n#### h4 Heading\n##### h5 Heading\n###### h6 Heading\n\n\n## Horizontal Rules\n\n___\n\n---\n\n***\n\n\n## Typographic replacements\n\nEnable typographer option to see result.\n\n(c) (C)\n...\nsome code, part of Definition 2 }\n\n    Third paragraph of definition 2.\n\n_Compact style:_\n\nTerm 1\n  ~ Definition 1\n\nTerm 2\n  ~ Definition 2a\n  ~ Definition 2b\n\n\n### [Abbreviations](https://github.com/markdown-it/markdown-it-abbr)\n\nThis is HTML abbreviation example.\n\nIt converts \"HTML\", but keep intact partial entries like \"xxxHTMLyyy\" and so on.\n\n*[HTML]: Hyper Text Markup Language\n\n### [Custom containers](https://github.com/markdown-it/markdown-it-container)\n\n::: warning\n*here be dragons*\n:::\n</code></pre> <p>The original markdown file is:</p> <pre><code>---\n__Advertisement :)__\n\n- __[pica](https://nodeca.github.io/pica/demo/)__ - high quality and fast image\n  resize in browser.\n- __[babelfish](https://github.com/nodeca/babelfish/)__ - developer friendly\n  i18n with plurals support and easy syntax.\n\nYou will like those projects!\n\n---\n\n# h1 Heading 8-)\n## h2 Heading\n### h3 Heading\n#### h4 Heading\n##### h5 Heading\n###### h6 Heading\n\n\n## Horizontal Rules\n\n___\n\n---\n\n***\n\n\n## Typographic replacements\n\nEnable typographer option to see result.\n\n(c) (C) (r) (R) (tm) (TM) (p) (P) +-\n\ntest.. test... test..... test?..... test!....\n\n!!!!!! ???? ,,  -- ---\n\n\"Smartypants, double quotes\" and 'single quotes'\n\n\n## Emphasis\n\n**This is bold text**\n\n__This is bold text__\n\n*This is italic text*\n\n_This is italic text_\n\n~~Strikethrough~~\n\n\n## Blockquotes\n\n\n&gt; Blockquotes can also be nested...\n&gt;&gt; ...by using additional greater-than signs right next to each other...\n&gt; &gt; &gt; ...or with spaces between arrows.\n\n...\n</code></pre> <p>To split this text by the level 2 headers (<code>##</code>), we can use the following instructions:</p> <pre><code>splitter = HeaderSplitter(headers_to_split_on=[\"Header 2\"])\nsplitter_output = splitter.split(reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\n---\n__Advertisement :)__  \n- __[pica](https://nodeca.github.io/pica/demo/)__ - high quality and fast image\nresize in browser.\n- __[babelfish](https://github.com/nodeca/babelfish/)__ - developer friendly\ni18n with plurals support and easy syntax.  \nYou will like those projects!  \n---  \n# h1 Heading 8-)  \n## h2 Heading\n### h3 Heading\n#### h4 Heading\n##### h5 Heading\n###### h6 Heading\n\n========================\n...\n some code, part of Definition 2 }  \nThird paragraph of definition 2.  \n_Compact style:_  \nTerm 1\n~ Definition 1  \nTerm 2\n~ Definition 2a\n~ Definition 2b  \n### [Abbreviations](https://github.com/markdown-it/markdown-it-abbr)  \nThis is HTML abbreviation example.  \nIt converts \"HTML\", but keep intact partial entries like \"xxxHTMLyyy\" and so on.  \n*[HTML]: Hyper Text Markup Language  \n### [Custom containers](https://github.com/markdown-it/markdown-it-container)  \n::: warning\n*here be dragons*\n:::\n</code></pre> <p>And that's it! </p> <p>Note that <code>## h2 Heading</code> is not picked as an actual header since there is no blankline between <code>##</code> and the end of the title. Test with other Headers as your choice!</p>"},{"location":"examples/schema/header_splitter/#complete-script","title":"Complete Script","text":"<pre><code>from splitter_mr.reader import VanillaReader\nfrom splitter_mr.splitter import HeaderSplitter\n\n# Step 1: Read the HTML file\nprint(\"HTML file example\")\nfile = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/webpage_example.html\"\nreader = VanillaReader() # you can use the argument html_to_markdown=True to transform directly to markdown\nreader_output = reader.read(file)\nprint(reader_output.model_dump_json(indent=4))\nprint(reader_output.text)\n\nsplitter = HeaderSplitter(headers_to_split_on=[\"Header 1\", \"Header 2\", \"Header 3\"])\nsplitter_output = splitter.split(reader_output)\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\"*40 + f\" Chunk {idx + 1} \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n\n# Step 2: Read the Markdown file\nprint(\"Markdown file example\")\nfile = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/markdown_example.md\"\nreader = VanillaReader()\nreader_output = reader.read(file)\nprint(reader_output.model_dump_json(indent=4))\nprint(reader_output.text)\n\nsplitter = HeaderSplitter(headers_to_split_on=[\"Header 2\"])\nsplitter_output = splitter.split(reader_output)\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\"*40 + f\" Chunk {idx + 1} \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n</code></pre>"},{"location":"examples/schema/html_tag_splitter/","title":"Example: Splitting an HTML Table into Chunks with <code>HTMLTagSplitter</code>","text":"<p>As an example, we will use a dataset of donuts in HTML table format (see reference dataset). The goal is to split the table into groups of rows so that each chunk contains as many <code>&lt;tr&gt;</code> elements as possible, while not exceeding a maximum number of characters per chunk.</p> <p></p>"},{"location":"examples/schema/html_tag_splitter/#step-1-read-the-html-document","title":"Step 1: Read the HTML Document","text":"<p>We will use the <code>VanillaReader</code> to load our HTML table.</p> <pre><code>from splitter_mr.reader import VanillaReader\n\nreader = VanillaReader()  # you can use the argument html_to_markdown to transform the table directly to markdown format.\n\n# You can provide a local path or a URL to your HTML file\nurl = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/sweet_list.html\"\nreader_output = reader.read(url)\n</code></pre> <p>The <code>reader_output</code> object contains the raw HTML and metadata.</p> <pre><code>print(reader_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"text\": \"&lt;table border=\\\"1\\\" cellpadding=\\\"4\\\" cellspacing=\\\"0\\\"&gt;\\n    &lt;thead&gt;\\n      &lt;tr&gt;\\n        &lt;th&gt;id&lt;/th&gt;\\n        &lt;th&gt;type&lt;/th&gt;\\n        &lt;th&gt;name&lt;/th&gt;\\n        &lt;th&gt;batter&lt;/th&gt;\\n        &lt;th&gt;topping&lt;/th&gt;\\n      &lt;/tr&gt;\\n    &lt;/thead&gt;\\n    &lt;tbody&gt;\\n      &lt;tr&gt;&lt;td&gt;0001&lt;/td&gt;&lt;td&gt;donut&lt;/td&gt;&lt;td&gt;Cake&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;\\n      &lt;tr&gt;&lt;td&gt;0001&lt;/td&gt;&lt;td&gt;donut&lt;/td&gt;&lt;td&gt;Cake&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;Glazed&lt;/td&gt;&lt;/tr&gt;\\n      &lt;tr&gt;&lt;td&gt;0001&lt;/td&gt;&lt;td&gt;donut&lt;/td&gt;&lt;td&gt;Cake&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;\n...\ntd&gt;Chocolate&lt;/td&gt;&lt;/tr&gt;\\n      &lt;tr&gt;&lt;td&gt;0006&lt;/td&gt;&lt;td&gt;filled&lt;/td&gt;&lt;td&gt;Filled&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;Maple&lt;/td&gt;&lt;/tr&gt;\\n    &lt;/tbody&gt;\\n  &lt;/table&gt;\",\n    \"document_name\": \"sweet_list.html\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/sweet_list.html\",\n    \"document_id\": \"413c5f15-c7bc-407f-862e-9f392aa46c95\",\n    \"conversion_method\": \"html\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"page_placeholder\": null,\n    \"metadata\": {}\n}\n</code></pre> <p>To see the HTML text:</p> <pre><code>print(reader_output.text)\n</code></pre> <pre><code>&lt;table border=\"1\" cellpadding=\"4\" cellspacing=\"0\"&gt;\n    &lt;thead&gt;\n      &lt;tr&gt;\n        &lt;th&gt;id&lt;/th&gt;\n        &lt;th&gt;type&lt;/th&gt;\n        &lt;th&gt;name&lt;/th&gt;\n        &lt;th&gt;batter&lt;/th&gt;\n        &lt;th&gt;topping&lt;/th&gt;\n      &lt;/tr&gt;\n    &lt;/thead&gt;\n    &lt;tbody&gt;\n      &lt;tr&gt;&lt;td&gt;0001&lt;/td&gt;&lt;td&gt;donut&lt;/td&gt;&lt;td&gt;Cake&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;None&lt;/td&gt;&lt;/tr&gt;\n      &lt;tr&gt;&lt;td&gt;0001&lt;/td&gt;&lt;td&gt;donut&lt;/td&gt;&lt;td&gt;Cake&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;Glazed&lt;/td&gt;&lt;/tr&gt;\n      &lt;tr&gt;&lt;td&gt;0001&lt;/td&gt;&lt;td&gt;donut&lt;/td&gt;&lt;td&gt;Cake&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;Sugar&lt;/td&gt;&lt;/tr&gt;\n      &lt;tr&gt;&lt;td&gt;0001\n...\nd&gt;Glazed&lt;/td&gt;&lt;/tr&gt;\n      &lt;tr&gt;&lt;td&gt;0005&lt;/td&gt;&lt;td&gt;twist&lt;/td&gt;&lt;td&gt;Twist&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;Sugar&lt;/td&gt;&lt;/tr&gt;\n      &lt;tr&gt;&lt;td&gt;0006&lt;/td&gt;&lt;td&gt;filled&lt;/td&gt;&lt;td&gt;Filled&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;Glazed&lt;/td&gt;&lt;/tr&gt;\n      &lt;tr&gt;&lt;td&gt;0006&lt;/td&gt;&lt;td&gt;filled&lt;/td&gt;&lt;td&gt;Filled&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;Powdered Sugar&lt;/td&gt;&lt;/tr&gt;\n      &lt;tr&gt;&lt;td&gt;0006&lt;/td&gt;&lt;td&gt;filled&lt;/td&gt;&lt;td&gt;Filled&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;Chocolate&lt;/td&gt;&lt;/tr&gt;\n      &lt;tr&gt;&lt;td&gt;0006&lt;/td&gt;&lt;td&gt;filled&lt;/td&gt;&lt;td&gt;Filled&lt;/td&gt;&lt;td&gt;Regular&lt;/td&gt;&lt;td&gt;Maple&lt;/td&gt;&lt;/tr&gt;\n    &lt;/tbody&gt;\n  &lt;/table&gt;\n</code></pre> <p>This table can be interpretated in markdown format as:</p> id type name batter topping 0001 donut Cake Regular None 0001 donut Cake Regular Glazed 0001 donut Cake Regular Sugar ... ... ... ... ... 0006 filled Filled Regular Chocolate 0006 filled Filled Regular Maple <p>Note that you can parse directly this table in VanillaReader using the keyword argument <code>html_to_markdown=True</code>. Refer to the class documentation.</p>"},{"location":"examples/schema/html_tag_splitter/#step-2-chunk-the-html-table-using-htmltagsplitter","title":"Step 2: Chunk the HTML Table Using <code>HTMLTagSplitter</code>","text":"<p>To split the table into groups of rows, instantiate the <code>HTMLTagSplitter</code> with the desired tag (in this case, <code>\"tr\"</code> for table rows) and a chunk size in characters.</p> <pre><code>from splitter_mr.splitter import HTMLTagSplitter\n\n# Set chunk_size to the max number of characters you want per chunk\nsplitter = HTMLTagSplitter(chunk_size=400, tag=\"tr\")\nsplitter_output = splitter.split(reader_output)\nprint(splitter_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"chunks\": [\n        \"| id | type | name | batter | topping |\\n| --- | --- | --- | --- | --- |\\n| 0001 | donut | Cake | Regular | None |\\n| 0001 | donut | Cake | Regular | Glazed |\",\n        \"| id | type | name | batter | topping |\\n| --- | --- | --- | --- | --- |\\n| 0001 | donut | Cake | Regular | Sugar |\\n| 0001 | donut | Cake | Regular | Powdered Sugar |\",\n        \"| id | type | name | batter | topping |\\n| --- | --- | --- | --- | --- |\\n| 0001 | donut | Cake | Regular | Chocolate with S\n...\n   \"document_name\": \"sweet_list.html\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/sweet_list.html\",\n    \"document_id\": \"413c5f15-c7bc-407f-862e-9f392aa46c95\",\n    \"conversion_method\": \"html\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"split_method\": \"html_tag_splitter\",\n    \"split_params\": {\n        \"chunk_size\": 400,\n        \"tag\": \"table\",\n        \"batch\": true,\n        \"to_markdown\": true\n    },\n    \"metadata\": {}\n}\n</code></pre> <p>To visualize each chunk, simply iterate through them:</p> <pre><code>for idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\n| id | type | name | batter | topping |\n| --- | --- | --- | --- | --- |\n| 0001 | donut | Cake | Regular | None |\n| 0001 | donut | Cake | Regular | Glazed |\n\n======================================== Chunk 2 ========================================\n| id | type | name | batter | topping |\n| --- | --- | --- | --- | --- |\n| 0001 | donut | Cake | Regular | Sugar |\n| 0001 | donut | Cake | Regular | Powdered Sugar \n...\nd | Regular | Glazed |\n\n======================================== Chunk 24 ========================================\n| id | type | name | batter | topping |\n| --- | --- | --- | --- | --- |\n| 0006 | filled | Filled | Regular | Powdered Sugar |\n| 0006 | filled | Filled | Regular | Chocolate |\n\n======================================== Chunk 25 ========================================\n| id | type | name | batter | topping |\n| --- | --- | --- | --- | --- |\n| 0006 | filled | Filled | Regular | Maple |\n</code></pre> <p>By default the output will be chunks with a valid markdown format. In case that needed, you can directly give the results in HTML format using the argument <code>to_markdown=False</code>. Refer to the class documentation.</p> <p>Chunk 1:</p> id type name batter topping 0001 donut Cake Regular None 0001 donut Cake Regular Glazed 0001 donut Cake Regular Sugar <p>Chunk 2:</p> id type name batter topping 0001 donut Cake Regular Powdered Sugar 0001 donut Cake Regular Chocolate with Sprinkles 0001 donut Cake Regular Chocolate 0001 donut Cake Regular Maple <p>And that's it! You can now flexibly chunk HTML tables for processing, annotation, or LLM ingestion.</p>"},{"location":"examples/schema/html_tag_splitter/#complete-script","title":"Complete Script","text":"<pre><code>from splitter_mr.reader import VanillaReader\nfrom splitter_mr.splitter import HTMLTagSplitter\n\n# Step 1: Read the HTML file\nreader = VanillaReader()\nurl = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/sweet_list.html\"  # Use your path or URL here\nreader_output = reader.read(url)\n\nprint(reader_output.model_dump_json(indent=4))  # Visualize the ReaderOutput object\nprint(reader_output.text)  # See the HTML content\n\n# Step 2: Split by group of &lt;tr&gt; tags, max 400 characters per chunk\nsplitter = HTMLTagSplitter(chunk_size=400, tag=\"tr\")\nsplitter_output = splitter.split(reader_output)\n\nprint(splitter_output.model_dump_json(indent=4))  # Print the SplitterOutput object\n\n# Step 3: Visualize each HTML chunk\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\"*40 + f\" Chunk {idx + 1} \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n</code></pre>"},{"location":"examples/schema/json_splitter/","title":"Example: Splitting JSON Files with <code>RecursiveJSONSplitter</code>","text":"<p>When working with structured data such as invoices, user records, or any other JSON document, it's often necessary to split the data into manageable chunks for downstream processing, storage, or LLM ingestion. SplitterMR provides the <code>RecursiveJSONSplitter</code>, an splitter which divides a JSON structure into key-based chunks, preserving the hierarchy and content integrity. Let's see how it works!</p> <p></p>"},{"location":"examples/schema/json_splitter/#step-1-read-the-json-document","title":"Step 1: Read the JSON Document","text":"<p>First, use the <code>VanillaReader</code> to load the JSON file. You can use other Reader methods as your choice. Note that you can read from an URL, Path or variable.</p> <pre><code>from splitter_mr.reader import VanillaReader\n\nfile = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/invoices.json\"  # Path to your JSON file\nreader = VanillaReader()\nreader_output = reader.read(file)\n\nprint(reader_output.model_dump_json(indent=4))  # Show metadata and summary\n</code></pre> <pre><code>{\n    \"text\": \"[\\n  {\\n    \\\"id\\\": 1,\\n    \\\"name\\\": \\\"Johnson, Smith, and Jones Co.\\\",\\n    \\\"amount\\\": 345.33,\\n    \\\"Remark\\\": \\\"Pays on time\\\"\\n  },\\n  {\\n    \\\"id\\\": 2,\\n    \\\"name\\\": \\\"Sam \\\\\\\"Mad Dog\\\\\\\" Smith\\\",\\n    \\\"amount\\\": 993.44,\\n    \\\"Remark\\\": \\\"\\\"\\n  },\\n  {\\n    \\\"id\\\": 3,\\n    \\\"name\\\": \\\"Barney &amp; Company\\\",\\n    \\\"amount\\\": 0,\\n    \\\"Remark\\\": \\\"Great to work with\\\\nand always pays with cash.\\\"\\n  },\\n  {\\n    \\\"id\\\": 4,\\n    \\\"name\\\": \\\"Johnson's Automotive\\\",\\n    \\\"amount\\\": 2344,\\n    \\\"Remark\\\": \\\"\\\"\\n  }\\n]\",\n    \"document_name\": \"invoices.json\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/invoices.json\",\n    \"document_id\": \"aeb021c6-9707-4058-8b3b-73313e50e3c2\",\n    \"conversion_method\": \"json\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"page_placeholder\": null,\n    \"metadata\": {}\n}\n</code></pre> <p>Accessing to the text attribute you find the following JSON object:</p> <pre><code>print(reader_output.text)\n</code></pre> <pre><code>[\n  {\n    \"id\": 1,\n    \"name\": \"Johnson, Smith, and Jones Co.\",\n    \"amount\": 345.33,\n    \"Remark\": \"Pays on time\"\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Sam \\\"Mad Dog\\\" Smith\",\n    \"amount\": 993.44,\n    \"Remark\": \"\"\n  },\n  {\n    \"id\": 3,\n    \"name\": \"Barney &amp; Company\",\n    \"amount\": 0,\n    \"Remark\": \"Great to work with\\nand always pays with cash.\"\n  },\n  {\n    \"id\": 4,\n    \"name\": \"Johnson's Automotive\",\n    \"amount\": 2344,\n    \"Remark\": \"\"\n  }\n]\n</code></pre> <p>This is a JSON dataset with some sample invoices.</p> id name amount Remark 1 Johnson, Smith, and Jones Co. 345.33 Pays on time 2 Sam \"Mad Dog\" Smith 993.44 3 Barney &amp; Company 0 Great to work with and always pays with cash. 4 Johnson's Automotive 2344"},{"location":"examples/schema/json_splitter/#step-2-split-the-json-document","title":"Step 2: Split the JSON Document","text":"<p>To split the text, instantiate the <code>RecursiveJSONSplitter</code> and split the loaded JSON content:</p> <pre><code>from splitter_mr.splitter import RecursiveJSONSplitter\n\nsplitter = RecursiveJSONSplitter(chunk_size=100, min_chunk_size=20)\nsplitter_output = splitter.split(reader_output)\n\nprint(splitter_output.model_dump_json(indent=4))  # Show the SplitterOutput object\n</code></pre> <pre><code>{\n    \"chunks\": [\n        \"{\\\"0\\\": {\\\"id\\\": 1, \\\"name\\\": \\\"Johnson, Smith, and Jones Co.\\\", \\\"amount\\\": 345.33, \\\"Remark\\\": \\\"Pays on time\\\"}}\",\n        \"{\\\"1\\\": {\\\"id\\\": 2, \\\"name\\\": \\\"Sam \\\\\\\"Mad Dog\\\\\\\" Smith\\\", \\\"amount\\\": 993.44, \\\"Remark\\\": \\\"\\\"}}\",\n        \"{\\\"2\\\": {\\\"id\\\": 3, \\\"name\\\": \\\"Barney &amp; Company\\\", \\\"amount\\\": 0, \\\"Remark\\\": \\\"Great to work with\\\\nand always pays with cash.\\\"}}\",\n        \"{\\\"3\\\": {\\\"id\\\": 4, \\\"name\\\": \\\"Johnson's Automotive\\\", \\\"amount\\\": 2344, \\\"Remark\\\": \\\"\\\"}\n...\n0c-a890-4ec4-959c-d2c9e533f5d2\"\n    ],\n    \"document_name\": \"invoices.json\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/invoices.json\",\n    \"document_id\": \"aeb021c6-9707-4058-8b3b-73313e50e3c2\",\n    \"conversion_method\": \"json\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"split_method\": \"recursive_json_splitter\",\n    \"split_params\": {\n        \"max_chunk_size\": 100,\n        \"min_chunk_size\": 20\n    },\n    \"metadata\": {}\n}\n</code></pre> <p>To inspect every chunk individually, print them as follows:</p> <pre><code>import json\n\nfor i, c in enumerate(splitter_output.chunks, 1):\n    data = next(iter(json.loads(c).values()))\n    print(f\"{'=' * 40} Chunk {i} {'=' * 40}\\n{json.dumps(data, indent=4)}\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\n{\n    \"id\": 1,\n    \"name\": \"Johnson, Smith, and Jones Co.\",\n    \"amount\": 345.33,\n    \"Remark\": \"Pays on time\"\n}\n\n======================================== Chunk 2 ========================================\n{\n    \"id\": 2,\n    \"name\": \"Sam \\\"Mad Dog\\\" Smith\",\n    \"amount\": 993.44,\n    \"Remark\": \"\"\n}\n\n======================================== Chunk 3 ========================================\n{\n    \"id\": 3,\n    \"name\": \"Barney &amp; Company\",\n    \"amount\": 0,\n    \"Remark\": \"Great to work with\\nand always pays with cash.\"\n}\n\n======================================== Chunk 4 ========================================\n{\n    \"id\": 4,\n    \"name\": \"Johnson's Automotive\",\n    \"amount\": 2344,\n    \"Remark\": \"\"\n}\n</code></pre> <p>In markdown format table:</p> id name amount Remark 1 Johnson, Smith, and Jones Co. 345.33 Pays on time id name amount Remark 2 Sam \"Mad Dog\" Smith 993.44 id name amount Remark 3 Barney &amp; Company 0 Great to work with and always pays with cash. id name amount Remark 4 Johnson's Automotive 2344 <p>And that's it! As you can see, a chunk for every row in the JSON table has been generated. </p> <p>Note</p> <p>All the objects obtained by the <code>SplitterOutput</code> <code>.chunks</code> attribute are <code>dict</code>. In case that you need to transform it into <code>str</code> elements, you need to process them with the instruction <code>str(chunk[idx])</code>.</p>"},{"location":"examples/schema/json_splitter/#complete-script","title":"Complete Script","text":"<pre><code>import json\nfrom splitter_mr.reader import VanillaReader\nfrom splitter_mr.splitter import RecursiveJSONSplitter\n\nfile = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/invoices.json\"\n\nreader = VanillaReader() # Load a Reader\nreader_output = reader.read(file) # Read the file\n\nprint(reader_output) # ReaderOutput object\nprint(reader_output.text) # Visualize the file\n\nsplitter = RecursiveJSONSplitter(chunk_size = 100, min_chunk_size=20) # Instantiate the Splitter class\nsplitter_output = splitter.split(reader_output) # Split the text\n\nprint(splitter_output) # SplitterOutput object\n\n# Visualize every chunk\nfor i, c in enumerate(splitter_output.chunks, 1):\n    data = next(iter(json.loads(c).values()))\n    print(f\"{'='*40} Chunk {i} {'='*40}\\n{json.dumps(data, indent=2)}\\n\")\n</code></pre>"},{"location":"examples/schema/json_splitter/#references","title":"References","text":"<p>RecursiveJSONSplitter API (Langchain)</p>"},{"location":"examples/schema/row_column_splitter/","title":"Example: Splitting Tabular Data with <code>RowColumnSplitter</code>","text":"<p>Tabular files such as CSVs, TSVs, or Markdown tables are ubiquitous in business and data workflows, but can become too large for direct LLM ingestion, annotation, or analysis. SplitterMR <code>RowColumnSplitter</code> provides flexible chunking for tabular data, letting you split tables by rows, columns, or character size\u2014while preserving the structural integrity of each chunk.</p> <p></p>"},{"location":"examples/schema/row_column_splitter/#step-1-read-the-tabular-file","title":"Step 1: Read the Tabular File","text":"<p>Let's use the <code>VanillaReader</code> to load a CSV file:</p> <pre><code>from splitter_mr.reader import VanillaReader\n\nfile = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/invoices.csv\"\nreader = VanillaReader()\nreader_output = reader.read(file)\n\n# Print metadata and content\nprint(reader_output)\n</code></pre> <pre><code>text='id,name,amount,Remark\\n1,\"Johnson, Smith, and Jones Co.\",345.33,Pays on time\\n2,\"Sam \"\"Mad Dog\"\" Smith\",993.44,\\n3,Barney &amp; Company,0,\"Great to work with and always pays with cash.\"\\n4,Johnson\\'s Automotive,2344,' document_name='invoices.csv' document_path='https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/invoices.csv' document_id='587fd2ce-2a41-4ab2-a9ad-f09ba3db1fe3' conversion_method='txt' reader_method='vanilla' ocr_method=None page_placeholder=None metadata={}\n</code></pre> <p>The content file is extracted accessing to the <code>text</code> attribute:</p> <pre><code>print(reader_output.text)\n</code></pre> <pre><code>id,name,amount,Remark\n1,\"Johnson, Smith, and Jones Co.\",345.33,Pays on time\n2,\"Sam \"\"Mad Dog\"\" Smith\",993.44,\n3,Barney &amp; Company,0,\"Great to work with and always pays with cash.\"\n4,Johnson's Automotive,2344,\n</code></pre> <p>Transformed into a markdown table will be:</p> id name amount Remark 1 Johnson, Smith, and Jones Co. 345.33 Pays on time 2 Sam \"Mad Dog\" Smith 993.44 nan 3 Barney &amp; Company 0 Great to work with and always pays with cash. 4 Johnson's Automotive 2344 nan"},{"location":"examples/schema/row_column_splitter/#step-2-split-the-table","title":"Step 2: Split the Table","text":""},{"location":"examples/schema/row_column_splitter/#21-split-by-character-size-row-wise-preserving-full-rows","title":"2.1. Split by Character Size (row-wise, preserving full rows)","text":"<p>Split into chunks such that each chunk's markdown table representation stays under a character limit:</p> <pre><code>from splitter_mr.splitter import RowColumnSplitter\n\nsplitter = RowColumnSplitter(chunk_size=200)\nsplitter_output = splitter.split(reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\n| id   | name   | amount   | Remark   |\n|------|--------|----------|----------|\n|    1 | Johnson, Smith, and Jones Co. |   345.33 | Pays on time |\n|    2 | Sam \"Mad Dog\" Smith |   993.44 |      nan |\n\n======================================== Chunk 2 ========================================\n| id   | name   | amount   | Remark   |\n|------|--------|----------|----------|\n|    3 | Barney &amp; Company |        0 | Great to work with and always pays with cash. |\n\n======================================== Chunk 3 ========================================\n| id   | name   | amount   | Remark   |\n|------|--------|----------|----------|\n|    4 | Johnson's Automotive |     2344 |      nan |\n</code></pre> <p>Chunk 1:</p> id name amount Remark 1 Johnson, Smith, and Jones Co. 345.33 Pays on time 2 Sam \"Mad Dog\" Smith 993.44 nan <p>Chunk 2:</p> id name amount Remark 3 Barney &amp; Company 0 Great to work with and always pays with cash. <p>Chunk 3:</p> id name amount Remark 4 Johnson's Automotive 2344 nan <p>Each output chunk is a valid markdown table with the header and as many full rows as will fit the character size. </p> <p>Note</p> <p>No chunk will ever split a row or a column in half.</p>"},{"location":"examples/schema/row_column_splitter/#22-split-by-a-fixed-number-of-rows","title":"2.2. Split by a Fixed Number of Rows","text":"<p>Set <code>num_rows</code> to split the table into smaller tables, each with a fixed number of rows:</p> <pre><code>splitter = RowColumnSplitter(num_rows=2)\nsplitter_output = splitter.split(reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\n|   id | name                          |   amount | Remark       |\n|-----:|:------------------------------|---------:|:-------------|\n|    1 | Johnson, Smith, and Jones Co. |   345.33 | Pays on time |\n|    2 | Sam \"Mad Dog\" Smith           |   993.44 | nan          |\n\n======================================== Chunk 2 ========================================\n|   id | name                 |   amount | Remark                                        |\n|-----:|:---------------------|---------:|:----------------------------------------------|\n|    3 | Barney &amp; Company     |        0 | Great to work with and always pays with cash. |\n|    4 | Johnson's Automotive |     2344 | nan                                           |\n</code></pre> <p>The output will be:</p> <p>Chunk 1:</p> id name amount Remark 1 Johnson, Smith, and Jones Co. 345.33 Pays on time 2 Sam \"Mad Dog\" Smith 993.44 nan <p>Chunk 2:</p> id name amount Remark 3 Barney &amp; Company 0 Great to work with and always pays with cash. 4 Johnson's Automotive 2344 nan"},{"location":"examples/schema/row_column_splitter/#23-split-by-a-fixed-number-of-columns","title":"2.3. Split by a Fixed Number of Columns","text":"<p>Set <code>num_cols</code> to split the table into column groups, each containing a fixed set of columns (e.g., for wide tables):</p> <pre><code>splitter = RowColumnSplitter(num_cols=2)\nsplitter_output = splitter.split(reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\n[['id', 1, 2, 3, 4], ['name', 'Johnson, Smith, and Jones Co.', 'Sam \"Mad Dog\" Smith', 'Barney &amp; Company', \"Johnson's Automotive\"]]\n\n======================================== Chunk 2 ========================================\n[['amount', 345.33, 993.44, 0.0, 2344.0], ['Remark', 'Pays on time', nan, 'Great to work with and always pays with cash.', nan]]\n</code></pre>"},{"location":"examples/schema/row_column_splitter/#24-add-overlapping-rowscolumns","title":"2.4. Add Overlapping Rows/Columns","text":"<p>Use <code>chunk_overlap</code> (int or float between 0 and 1) to specify how many rows or columns are repeated between consecutive chunks for context preservation:</p> <pre><code>splitter = RowColumnSplitter(chunk_size=150, chunk_overlap=0.2)\nsplitter_output = splitter.split(reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\n| id   | name   | amount   | Remark   |\n|------|--------|----------|----------|\n|    1 | Johnson, Smith, and Jones Co. |   345.33 | Pays on time |\n\n======================================== Chunk 2 ========================================\n| id   | name   | amount   | Remark   |\n|------|--------|----------|----------|\n|    2 | Sam \"Mad Dog\" Smith |   993.44 |      nan |\n\n======================================== Chunk 3 ========================================\n| id   | name   | amount   | Remark   |\n|------|--------|----------|----------|\n\n\n======================================== Chunk 4 ========================================\n| id   | name   | amount   | Remark   |\n|------|--------|----------|----------|\n|    4 | Johnson's Automotive |     2344 |      nan |\n</code></pre> <p>The output is a table with an overlapping row column:</p> <p>Chunk 1:</p> id name amount Remark 1 Johnson, Smith, and Jones Co. 345.33 Pays on time 2 Sam \"Mad Dog\" Smith 993.44 nan 3 Barney &amp; Company 0 Great to work with and always pays with cash. <p>Chunk 2:</p> id name amount Remark 3 Barney &amp; Company 0 Great to work with and always pays with cash. 4 Johnson's Automotive 2344 nan <p>Note</p> <p><code>chunk_overlap</code> parameter can be used by rows or columns.</p> <p>And that's it! In this example we have used a CSV files, but we can process other file formats. The compatible file extensions are: <code>csv</code>, <code>tsv</code>, <code>md</code>, <code>txt</code> and tabular <code>json</code>. Parquet files which are processed as JSON can be processed as well. </p> <p>Warning</p> <p>Setting both <code>num_rows</code> and <code>num_cols</code> will raise an error. If <code>chunk_overlap</code> is a float, it is interpreted as a percentage (e.g., <code>0.2</code> means 20% overlap).</p>"},{"location":"examples/schema/row_column_splitter/#3-use-cases","title":"3. Use cases","text":"<p><code>RowColumnSplitter</code> is useful for the following use cases:</p> <ul> <li>For splitting large tabular datasets into LLM-friendly or context-aware chunks.</li> <li>For preserving row/column integrity in <code>csv</code>/<code>tsv</code>/<code>markdown</code> data.</li> <li>When you need easy chunking with overlap for annotation, document search, or analysis.</li> </ul>"},{"location":"examples/schema/row_column_splitter/#complete-script","title":"Complete script","text":"<pre><code>from splitter_mr.reader import VanillaReader\nfrom splitter_mr.splitter import RowColumnSplitter\n\nfile = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/invoices.csv\"\n\nreader = VanillaReader()\nreader_output = reader.read(file)\n\n# Visualize the ReaderOutput object\nprint(reader_output.model_dump_json(indent=4))\n\n# Access to the text content\nprint(reader_output.text)\n\nprint(\"*\"*20 + \" Split by rows based on chunk size \" + \"*\"*20)\n\nsplitter = RowColumnSplitter(chunk_size=200)\nsplitter_output = splitter.split(reader_output)\n\nprint(splitter_output.model_dump_json(indent=4))\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\"*40 + f\" Chunk {idx + 1} \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n\nprint(\"*\"*20 + \" Split by an specific number of rows \" + \"*\"*20)\n\nsplitter = RowColumnSplitter(num_rows=2)\nsplitter_output = splitter.split(reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\"*40 + f\" Chunk {idx + 1} \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n\nprint(\"*\"*20 + \" Split by an specific number of columns \" + \"*\"*20)\n\nsplitter = RowColumnSplitter(num_cols=2)\nsplitter_output = splitter.split(reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\"*40 + f\" Chunk {idx + 1} \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n\nprint(\"*\"*20 + \" Split with overlap \" + \"*\"*20)\n\nsplitter = RowColumnSplitter(chunk_size=300, chunk_overlap=0.4)\nsplitter_output = splitter.split(reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\"*40 + f\" Chunk {idx + 1} \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n</code></pre>"},{"location":"examples/text/fixed_splitter/","title":"Example: Splitting Text Files with <code>CharacterSplitter</code>, <code>WordSplitter</code>, <code>SentenceSplitter</code>, <code>ParagraphSplitter</code>","text":"<p>When processing a plain text file, such as an e-book or an instruction guidebook, for downstream tasks like LLM ingestion, annotation, or search, it is often necessary to divide it into smaller, manageable chunks.</p> <p>SplitterMR provides the functionality to segment such files into groups of characters, words, sentences, or paragraphs. Furthermore, it allows for overlapping chunks to maintain contextual continuity. This example will illustrate the application of each splitter, utilizing the first chapter of \"El Famoso Hidalgo Don Quijote de la Mancha\" (original language) as the sample text.</p> <p></p>"},{"location":"examples/text/fixed_splitter/#step-1-read-the-text-document","title":"Step 1: Read the Text Document","text":"<p>We will use Vanilla Reader to load our text file. The result will be <code>ReaderOutput</code>, a Pydantic object with the following fields:</p> <pre><code>from splitter_mr.reader import VanillaReader\n\nreader = VanillaReader()\ndata = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/quijote_example.txt\"  # Path to your file\nreader_output = reader.read(data)\nprint(reader_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"text\": \"Cap\u00edtulo Primero\\n\\nQue trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha\\n\\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv\u00eda un hidalgo de los de lanza en astillero, adarga antigua, roc\u00edn flaco y galgo corredor. Una olla de algo m\u00e1s vaca que carnero, salpic\u00f3n las m\u00e1s noches, duelos y quebrantos los s\u00e1bados, lentejas los viernes, alg\u00fan palomino de a\u00f1adidura los domingos, consum\u00edan las tres partes de su hacienda. \n...\ntural del Toboso, nombre a su parecer m\u00fasico y peregrino y significativo, como todos los dem\u00e1s que a \u00e9l y a sus cosas hab\u00eda puesto.\",\n    \"document_name\": \"quijote_example.txt\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/quijote_example.txt\",\n    \"document_id\": \"955e6c5b-3779-4328-a1aa-c3e7a60a4cfd\",\n    \"conversion_method\": \"txt\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"page_placeholder\": null,\n    \"metadata\": {}\n}\n</code></pre> <p>If you want to extract the text, you can access the content via the <code>text</code> attribute:</p> <pre><code>print(reader_output.text)\n</code></pre> <pre><code>Cap\u00edtulo Primero\n\nQue trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha\n\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv\u00eda un hidalgo de los de lanza en astillero, adarga antigua, roc\u00edn flaco y galgo corredor. Una olla de algo m\u00e1s vaca que carnero, salpic\u00f3n las m\u00e1s noches, duelos y quebrantos los s\u00e1bados, lentejas los viernes, alg\u00fan palomino de a\u00f1adidura los domingos, consum\u00edan las tres partes de su hacienda. El resto della conc\n...\nquien \u00e9l un tiempo anduvo enamorado, aunque seg\u00fan se entiende, ella jam\u00e1s lo supo ni se di\u00f3 cata de ello. Llam\u00e1base Aldonza Lorenzo, y a esta le pareci\u00f3 ser bien darle t\u00edtulo de se\u00f1ora de sus pensamientos; y busc\u00e1ndole nombre que no desdijese mucho del suyo, y que tirase y se encaminase al de princesa y gran se\u00f1ora, vino a llamarla DULCINEA DEL TOBOSO, porque era natural del Toboso, nombre a su parecer m\u00fasico y peregrino y significativo, como todos los dem\u00e1s que a \u00e9l y a sus cosas hab\u00eda puesto.\n</code></pre>"},{"location":"examples/text/fixed_splitter/#step-2-split-the-document","title":"Step 2: Split the Document","text":"<p>We will try four different splitting strategies: by characters, words, sentences, and paragraphs. Remember that you can adjust the chunk size as needed.</p> <pre><code>from splitter_mr.splitter import (\n    CharacterSplitter,\n    WordSplitter,\n    SentenceSplitter,\n    ParagraphSplitter,\n)\n</code></pre>"},{"location":"examples/text/fixed_splitter/#21-split-by-characters","title":"2.1. Split by Characters","text":"<p>Firstly, we will test the character-based splitting strategy. To do this, you can instantiate the <code>CharacterSplitter</code> class with the splitting attributes as your choice and pass the reader's output to the split method of this class. Accessing the <code>SplitterOutput</code> object's content is then straightforward:</p> <pre><code>char_splitter = CharacterSplitter(chunk_size=100)\nchar_splitter_output = char_splitter.split(reader_output)\n\nprint(char_splitter_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"chunks\": [\n        \"Cap\u00edtulo Primero\\n\\nQue trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha\\n\\n\",\n        \"En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv\u00eda un hidalg\",\n        \"o de los de lanza en astillero, adarga antigua, roc\u00edn flaco y galgo corredor. Una olla de algo m\u00e1s v\",\n        \"aca que carnero, salpic\u00f3n las m\u00e1s noches, duelos y quebrantos los s\u00e1bados, lentejas los viernes, alg\",\n        \"\u00fan palomino de a\u00f1adid\n...\n3f-5343-4cb3-8df5-cb6aa27aaf91\"\n    ],\n    \"document_name\": \"quijote_example.txt\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/quijote_example.txt\",\n    \"document_id\": \"955e6c5b-3779-4328-a1aa-c3e7a60a4cfd\",\n    \"conversion_method\": \"txt\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"split_method\": \"character_splitter\",\n    \"split_params\": {\n        \"chunk_size\": 100,\n        \"chunk_overlap\": 0\n    },\n    \"metadata\": {}\n}\n</code></pre> <p>To visualize each chunk, you can use the following instruction:</p> <pre><code>for idx, chunk in enumerate(char_splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\nCap\u00edtulo Primero\n\nQue trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha\n\n\n\n======================================== Chunk 2 ========================================\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv\u00eda un hidalg\n\n======================================== Chunk 3 ========================================\no de los de lanza en astil\n...\nunk 103 ========================================\njese mucho del suyo, y que tirase y se encaminase al de princesa y gran se\u00f1ora, vino a llamarla DULC\n\n======================================== Chunk 104 ========================================\nINEA DEL TOBOSO, porque era natural del Toboso, nombre a su parecer m\u00fasico y peregrino y significati\n\n======================================== Chunk 105 ========================================\nvo, como todos los dem\u00e1s que a \u00e9l y a sus cosas hab\u00eda puesto.\n</code></pre> <p>As you can see, the final characters of \"hidalgo\" are cut by this method. So how can we avoid cutting words? Introducing <code>WordSplitter</code>.</p>"},{"location":"examples/text/fixed_splitter/#22-split-by-words","title":"2.2. Split by Words","text":"<p>To use the <code>WordSplitter</code>, instantiate the class with your desired parameters (you can consult the Developer guide for information on available parameters). Then, split the content using the previous output from the Reader. To visualize the chunks, you need to access the <code>chunks</code> attribute in the <code>SplitterOutput</code> object:</p> <pre><code>word_splitter = WordSplitter(chunk_size=20)\nword_splitter_output = word_splitter.split(reader_output)\n\nfor idx, chunk in enumerate(word_splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\nCap\u00edtulo Primero Que trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha En un lugar\n\n======================================== Chunk 2 ========================================\nde la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv\u00eda un hidalgo de los de\n\n======================================== Chunk 3 ========================================\nlanza en astillero\n...\nde sus pensamientos; y busc\u00e1ndole nombre que no desdijese mucho del suyo, y que tirase y se encaminase\n\n======================================== Chunk 94 ========================================\nal de princesa y gran se\u00f1ora, vino a llamarla DULCINEA DEL TOBOSO, porque era natural del Toboso, nombre a su\n\n======================================== Chunk 95 ========================================\nparecer m\u00fasico y peregrino y significativo, como todos los dem\u00e1s que a \u00e9l y a sus cosas hab\u00eda puesto.\n</code></pre> <p>Note that even though words aren't cut, the context isn't adequate because sentences are left incomplete. To avoid this issue, we should split by sentences. Introducing the <code>SentenceSplitter</code>:</p>"},{"location":"examples/text/fixed_splitter/#23-split-by-sentences","title":"2.3. Split by Sentences","text":"<p>Analogously to the previous steps, we can define the <code>SentenceSplitter</code> object with the number of sentences to split on:</p> <pre><code>sentence_splitter = SentenceSplitter(chunk_size=5)\nsentence_splitter_output = sentence_splitter.split(reader_output)\n\nfor idx, chunk in enumerate(sentence_splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\nCap\u00edtulo Primero\n\nQue trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha\n\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv\u00eda un hidalgo de los de lanza en astillero, adarga antigua, roc\u00edn flaco y galgo corredor. Una olla de algo m\u00e1s vaca que carnero, salpic\u00f3n las m\u00e1s noches, duelos y quebrantos los s\u00e1bados, lentejas los viernes, alg\u00fan palomin\n...\nuien \u00e9l un tiempo anduvo enamorado, aunque seg\u00fan se entiende, ella jam\u00e1s lo supo ni se di\u00f3 cata de ello. Llam\u00e1base Aldonza Lorenzo, y a esta le pareci\u00f3 ser bien darle t\u00edtulo de se\u00f1ora de sus pensamientos; y busc\u00e1ndole nombre que no desdijese mucho del suyo, y que tirase y se encaminase al de princesa y gran se\u00f1ora, vino a llamarla DULCINEA DEL TOBOSO, porque era natural del Toboso, nombre a su parecer m\u00fasico y peregrino y significativo, como todos los dem\u00e1s que a \u00e9l y a sus cosas hab\u00eda puesto.\n</code></pre> <p>While the entire context is preserved when splitting by sentences, the varying chunk sizes suggest that chunking by paragraphs might be more beneficial. Introducing <code>ParagraphSplitter</code>.</p>"},{"location":"examples/text/fixed_splitter/#24-split-by-paragraphs","title":"2.4. Split by Paragraphs","text":"<p>We can select <code>3</code> as the desired number of paragraphs per chunk. The resulting chunks are the following:</p> <pre><code>paragraph_splitter = ParagraphSplitter(chunk_size=3)\nparagraph_splitter_output = paragraph_splitter.split(reader_output)\n\nfor idx, chunk in enumerate(paragraph_splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\nCap\u00edtulo Primero\nQue trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv\u00eda un hidalgo de los de lanza en astillero, adarga antigua, roc\u00edn flaco y galgo corredor. Una olla de algo m\u00e1s vaca que carnero, salpic\u00f3n las m\u00e1s noches, duelos y quebrantos los s\u00e1bados, lentejas los viernes, alg\u00fan palomino \n...\nuien \u00e9l un tiempo anduvo enamorado, aunque seg\u00fan se entiende, ella jam\u00e1s lo supo ni se di\u00f3 cata de ello. Llam\u00e1base Aldonza Lorenzo, y a esta le pareci\u00f3 ser bien darle t\u00edtulo de se\u00f1ora de sus pensamientos; y busc\u00e1ndole nombre que no desdijese mucho del suyo, y que tirase y se encaminase al de princesa y gran se\u00f1ora, vino a llamarla DULCINEA DEL TOBOSO, porque era natural del Toboso, nombre a su parecer m\u00fasico y peregrino y significativo, como todos los dem\u00e1s que a \u00e9l y a sus cosas hab\u00eda puesto.\n</code></pre>"},{"location":"examples/text/fixed_splitter/#25-add-overlapping-chunks","title":"2.5. Add Overlapping Chunks","text":"<p>Another strategy you can employ is to preserve some text between chunks. For this use case, you can optionally add overlap between chunks. Overlap can be defined as either a fraction (e.g., <code>chunk_overlap = 0.2</code> for 20% overlap) or an integer number (e.g., <code>chunk_overlap = 20</code>):</p> <pre><code>char_splitter_with_overlap = CharacterSplitter(chunk_size=100, chunk_overlap=0.2)\nchar_splitter_output = char_splitter_with_overlap.split(reader_output)\n\nfor idx, chunk in enumerate(char_splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\nCap\u00edtulo Primero\n\nQue trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha\n\n\n\n======================================== Chunk 2 ========================================\nijote de la Mancha\n\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo\n\n======================================== Chunk 3 ========================================\n, no ha mucho tiempo que v\n...\nunk 129 ========================================\nncaminase al de princesa y gran se\u00f1ora, vino a llamarla DULCINEA DEL TOBOSO, porque era natural del\n\n======================================== Chunk 130 ========================================\nque era natural del Toboso, nombre a su parecer m\u00fasico y peregrino y significativo, como todos los d\n\n======================================== Chunk 131 ========================================\nvo, como todos los dem\u00e1s que a \u00e9l y a sus cosas hab\u00eda puesto.\n</code></pre> <p>And that\u2019s it! With these splitters, you can flexibly chunk your text data however you need. Remember that you can visit the complete Developer Reference to have more information about specific examples, methods, attributes and more of these Splitter classes.</p>"},{"location":"examples/text/fixed_splitter/#complete-example-script","title":"Complete Example Script","text":"<p>Finally, we provide a full example script for reproducibility purposes:</p> <pre><code>from splitter_mr.reader import VanillaReader\nfrom splitter_mr.splitter import (CharacterSplitter, ParagraphSplitter,\n                                  SentenceSplitter, WordSplitter)\n\nreader = VanillaReader()\n\ndata = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/quijote_example.txt\"\nreader_output = reader.read(data)\n\nprint(reader_output.model_dump_json(indent=4)) # Visualize the ReaderOutput object\nprint(reader_output.text) # Get the text from the document\n\n# 1. Split by Characters\n\nchar_splitter = CharacterSplitter(chunk_size=100)\nchar_splitter_output = char_splitter.split(reader_output)\nprint(char_splitter_output) # Visualize Character Splitter output\n\nfor idx, chunk in enumerate(char_splitter_output.chunks): # Visualize chunks\n    print(\"=\"*40 + \" Chunk \" + str(idx + 1) + \" \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n\n# 2. Split by Words\n\nword_splitter = WordSplitter(chunk_size=20)\nword_splitter_output = word_splitter.split(reader_output)\n\nfor idx, chunk in enumerate(word_splitter_output.chunks):\n    print(\"=\"*40 + \" Chunk \" + str(idx + 1) + \" \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n\n# 3. Split by Sentences\n\nsentence_splitter = SentenceSplitter(chunk_size=5)\nsentence_splitter_output = sentence_splitter.split(reader_output)\n\nfor idx, chunk in enumerate(sentence_splitter_output.chunks):\n    print(\"=\"*40 + \" Chunk \" + str(idx + 1) + \" \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n\n# 4. Split by Paragraphs\n\nparagraph_splitter = ParagraphSplitter(chunk_size=3)\nparagraph_splitter_output = paragraph_splitter.split(reader_output)\n\nfor idx, chunk in enumerate(paragraph_splitter_output.chunks):\n    print(\"=\"*40 + \" Chunk \" + str(idx + 1) + \" \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n\n# 5. Add overlapping words between chunks\n\nchar_splitter_with_overlap = CharacterSplitter(chunk_size=100, chunk_overlap=0.2)\nchar_splitter_output = char_splitter_with_overlap.split(reader_output)\n\nfor idx, chunk in enumerate(char_splitter_output.chunks):\n    print(\"=\"*40 + \" Chunk \" + str(idx + 1) + \" \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n</code></pre>"},{"location":"examples/text/keyword_splitter/","title":"Example: Split Pinocchio Chapters Using <code>KeywordSplitter</code>","text":"<p>The <code>KeywordSplitter</code> is a highly versatile text-splitting utility that divides documents based on custom regular-expression (regex) patterns. Unlike simpler splitters that break text by fixed lengths (characters, words, or sentences), the KeywordSplitter allows you to identify semantic boundaries\u2014such as headings, section markers, or unique keywords\u2014and chunk your text accordingly.</p> <p>This splitter is particularly useful when working with structured or semi-structured text where meaningful sections are defined by repeated markers. Examples include:</p> <ul> <li>Splitting classic books or plays by chapter headings (e.g., \"CHAPTER I\", \"ACT II\").</li> <li>Parsing logs by timestamps or error codes.</li> <li>Extracting sections from Markdown or plain text documents where a specific keyword (e.g., \"TODO\" or \"NOTE\") separates ideas.</li> <li>Segmenting transcripts or interview notes at speaker identifiers.</li> </ul> <p>This notebook demonstrates splitting Pinocchio by chapter markers such as \u201cCHAPTER I\u201d, \u201cCHAPTER II\u201d, etc., using the <code>KeywordSplitter</code>.</p>"},{"location":"examples/text/keyword_splitter/#step-1-read-the-text-using-a-reader-component","title":"Step 1: Read the text using a Reader component","text":"<p>We will use the <code>VanillaReader</code> to fetch the text from Project Gutenberg:</p> <pre><code>from splitter_mr.reader import VanillaReader\n\nFILE_PATH = \"https://www.gutenberg.org/cache/epub/16865/pg16865.txt\"\n\nreader = VanillaReader()\nreader_output = reader.read(file_path=FILE_PATH)\n\nprint(reader_output.model_dump_json(indent=4))\nprint(reader_output.text[:1000])  # preview first 1000 characters\n</code></pre> <pre><code>{\n    \"text\": \"\ufeffThe Project Gutenberg eBook of Pinocchio: The Tale of a Puppet\\r\\n    \\r\\nThis ebook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this ebook or online\\r\\nat www.gutenberg.org. If you are not located in the United States,\\r\\nyou will have to check the laws of the country\n...\nyou are located\nbefore using this eBook.\n\nTitle: Pinocchio: The Tale of a Puppet\n\nAuthor: Carlo Collodi\n\nIllustrator: Alice Carsey\n\nRelease date: October 13, 2005 [eBook #16865]\n                Most recently updated: December 12, 2020\n\nLanguage: English\n\nCredits: Produced by Mark C. Orton, Melissa Er-Raqabi and the Online\n        Distributed Proofreading Team at https://www.pgdp.net.\n\n\n*** START OF THE PROJECT GUTENBERG EBOOK PINOCCHIO: THE TALE OF A PUPPET ***\n\n\n\n\nProduced by Mark C. Orton, Me\n</code></pre>"},{"location":"examples/text/keyword_splitter/#step-2-split-the-text-using-keywordsplitter","title":"Step 2: Split the text using <code>KeywordSplitter</code>","text":"<p>We define the regex pattern for Roman numeral chapters and instantiate the splitter:</p> <pre><code>import re\nfrom splitter_mr.splitter import KeywordSplitter\n\nchapter_pattern = r\"CHAPTER\\s+[IVXLCDM]+\"\nsplitter = KeywordSplitter(\n    patterns={\"chapter\": chapter_pattern},\n    include_delimiters=\"after\",\n    flags=re.IGNORECASE,\n)\n\nsplitter_output = splitter.split(reader_output)\nprint(splitter_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"chunks\": [\n        \"\ufeffThe Project Gutenberg eBook of Pinocchio: The Tale of a Puppet\\r\\n    \\r\\nThis ebook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this ebook or online\\r\\nat www.gutenberg.org. If you are not located in the United States,\\r\\nyou will have to check the laws of\n...\n\n                ],\n                [\n                    203258,\n                    203271\n                ],\n                [\n                    214484,\n                    214496\n                ],\n                [\n                    222566,\n                    222579\n                ]\n            ],\n            \"include_delimiters\": \"after\",\n            \"flags\": 2,\n            \"pattern_names\": [\n                \"chapter\"\n            ],\n            \"chunk_size\": 100000\n        }\n    }\n}\n</code></pre> <p>As you can see, all the chunks start with <code>CHAPTER</code> + a roman number, so the Splitter has divided the text by chapters correctly.</p>"},{"location":"examples/text/keyword_splitter/#step-3-visualize-the-chunks","title":"Step 3: Visualize the chunks","text":"<p>To visualize the chunks, you can execute the following code:</p> <pre><code>for idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40)\n    print(chunk[:800] + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\n\ufeffThe Project Gutenberg eBook of Pinocchio: The Tale of a Puppet\n\nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United \n...\ne\nfever.\n\nWas he trembling from cold or from fear. Perhaps a little from both the\none and the other. But Pinocchio, thinking it was from fear, said, to\ncomfort him:\n\n\"Courage, papa! In a few minutes we shall be safely on shore.\"\n\n\"But where is this blessed shore?\" asked the little old man, becoming\nstill more frightened, and screwing up his eyes as tailors do when they\nwish to thread a needle. \"I have been looking in every direction and I\nsee nothing but the sky and the sea.\"\n\n\"But I see the s\n</code></pre>"},{"location":"examples/text/keyword_splitter/#step-4-analyze-metadata","title":"Step 4: Analyze metadata","text":"<pre><code>meta = splitter_output.metadata[\"keyword_matches\"]\nprint(\"Chapter matches:\", meta[\"counts\"])\nprint(\"Spans:\", meta[\"spans\"][:5])\nprint(\"Splitter parameters:\", splitter_output.split_params)\n</code></pre> <pre><code>Chapter matches: {'chapter': 36}\nSpans: [(7063, 7072), (10320, 10330), (14085, 14096), (20136, 20146), (23487, 23496)]\nSplitter parameters: {'include_delimiters': 'after', 'flags': re.IGNORECASE, 'chunk_size': 100000, 'pattern_names': ['chapter']}\n</code></pre> <p>You can also verify the configuration:</p> <pre><code>print(splitter_output.split_params)\n</code></pre> <pre><code>{'include_delimiters': 'after', 'flags': re.IGNORECASE, 'chunk_size': 100000, 'pattern_names': ['chapter']}\n</code></pre> <p>Note</p> <p>You can experiment with: * <code>include_delimiters</code> = <code>\"after\"</code>, <code>\"both\"</code>, or <code>\"none\"</code> to control delimiter placement. * Adjust <code>chunk_size</code> for soft wrapping inside chapters. * Use multiple patterns (dict or list) to split by different keywords simultaneously.</p> <p>And that\u2019s it! This notebook demonstrates how to segment Pinocchio into chapters interactively with <code>KeywordSplitter</code>.</p>"},{"location":"examples/text/keyword_splitter/#complete-script","title":"Complete script","text":"<pre><code>import re\nfrom splitter_mr.reader import VanillaReader\nfrom splitter_mr.splitter import KeywordSplitter\n\nFILE_PATH = \"https://www.gutenberg.org/cache/epub/16865/pg16865.txt\"\n\n# Step 1: Read Pinocchio\nreader = VanillaReader()\nreader_output = reader.read(file_path=FILE_PATH)\n\nprint(reader_output.model_dump_json(indent=4))  # inspect ReaderOutput\n\n# Step 2: Split by chapter markers\nchapter_pattern = r\"CHAPTER\\s+[IVXLCDM]+\"\nsplitter = KeywordSplitter(\n    patterns={\"chapter\": chapter_pattern},\n    include_delimiters=\"before\",\n    flags=re.IGNORECASE,\n    chunk_size=3000,\n)\nsplitter_output = splitter.split(reader_output)\n\nprint(splitter_output.model_dump_json(indent=4))  # inspect SplitterOutput\n\n# Step 3: Visualize chunks\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chapter {idx + 1} \" + \"=\" * 40)\n    print(chunk[:800] + \"\\n\")\n\n# Step 4: Explore metadata\nmeta = splitter_output.metadata[\"keyword_matches\"]\nprint(\"Counts:\", meta[\"counts\"])\nprint(\"First spans:\", meta[\"spans\"][:5])\nprint(\"Splitter parameters:\", splitter_output.split_params)\n</code></pre> <pre><code>{\n    \"text\": \"\ufeffThe Project Gutenberg eBook of Pinocchio: The Tale of a Puppet\\r\\n    \\r\\nThis ebook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this ebook or online\\r\\nat www.gutenberg.org. If you are not located in the United States,\\r\\nyou will have to check the laws of the country\n...\nresses. Donations are accepted in a number of other\nways including checks, online payments and credit card donations. To\ndonate, please visit: www.gutenberg.org/donate.\n\nSection 5. General Information About Project Gutenberg\u2122 electronic works\n\nProfessor\n\nCounts: {'chapter': 36}\nFirst spans: [(7063, 7072), (10320, 10330), (14085, 14096), (20136, 20146), (23487, 23496)]\nSplitter parameters: {'include_delimiters': 'before', 'flags': re.IGNORECASE, 'chunk_size': 3000, 'pattern_names': ['chapter']}\n</code></pre>"},{"location":"examples/text/paged_splitter/","title":"Example: Splitting Files by pages using <code>PagedSplitter</code>","text":"<p>For some documents, one of the best splitting strategies can be divide them by pages. To do so, you can use the <code>PagedSplitter</code>.</p> <p>For this example, we will read the file using `VanillaReader. The file can be found on the GitHub repository, and it consists of a scientific paper (Attention is All You Need) with 15 pages. Let's see how to split it.</p>"},{"location":"examples/text/paged_splitter/#step-1-read-the-file","title":"Step 1. Read the file","text":"<p>You can read the file using <code>VanillaReader</code> or <code>DoclingReader</code>. In case that you use <code>MarkItDownReader</code>, you should pass the parameter <code>split_by_pages = True</code>, since MarkItDown by default does not provide any placeholder to split by pages.</p> Show Python examples for all Readers <pre><code>from splitter_mr.reader import VanillaReader\n\nFILE_PATH = \"data/attention.pdf\"\n\nreader = VanillaReader()\nreader_output = reader.read(file_path=FILE_PATH)\n</code></pre> <pre><code>from splitter_mr.reader import DoclingReader\n\nFILE_PATH = \"data/attention.pdf\"\n\nreader = DoclingReader()\nreader_output = reader.read(file_path=FILE_PATH)\n</code></pre> <pre><code>from splitter_mr.reader import MarkItDownReader\n\nFILE_PATH = \"data/attention.pdf\"\n\nreader = MarkItDownReader()\nreader_output = reader.read(file_path=FILE_PATH, split_by_pages=True)\n</code></pre> <p>The output will be the following:</p> <pre><code>import warnings\n\nwarnings.filterwarnings(\"ignore\", message=\".*pin_memory.*MPS.*\")\n</code></pre> <pre><code>from splitter_mr.reader import DoclingReader\n\nFILE_PATH = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/attention.pdf\"\n\nreader = DoclingReader()\nreader_output = reader.read(file_path=FILE_PATH)\nprint(reader_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"text\": \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\\n\\n## Attention Is All You Need\\n\\nAshish Vaswani \u2217 Google Brain avaswani@google.com\\n\\nNoam Shazeer \u2217 Google Brain noam@google.com\\n\\nNiki Parmar \u2217 Google Research nikip@google.com\\n\\nJakob Uszkoreit \u2217 Google Research usz@google.com\\n\\nLlion Jones \u2217 Google Research llion@google.com\\n\\nAidan N. Gomez \u2217 \u2020 Unive\n...\n heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\\n\\n&lt;!-- image --&gt;\",\n    \"document_name\": \"attention.pdf\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/attention.pdf\",\n    \"document_id\": \"04aa7dd7-e397-464d-aa29-4f444be12514\",\n    \"conversion_method\": \"markdown\",\n    \"reader_method\": \"docling\",\n    \"ocr_method\": null,\n    \"page_placeholder\": \"&lt;!-- page --&gt;\",\n    \"metadata\": {}\n}\n</code></pre> <p>As you can see, the <code>ReaderOutput</code> object has an attribute named <code>page_placeholder</code> which allows to identify every page. </p>"},{"location":"examples/text/paged_splitter/#split-by-pages","title":"Split by pages","text":"<p>So, we can simply instantiate the <code>PageSplitter</code> object and use the <code>split</code> method to get the chunks page-by-page:</p> <pre><code>from splitter_mr.splitter import PagedSplitter\n\nsplitter = PagedSplitter()\nsplitter_output = splitter.split(reader_output=reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"\\n\" + \"*\" * 80 + f\" Chunk {idx} \" + \"*\" * 80 + \"\\n\")\n    print(chunk)\n</code></pre> <pre><code>******************************************************************************** Chunk 0 ********************************************************************************\n\nProvided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n\n## Attention Is All You Need\n\nAshish Vaswani \u2217 Google Brain avaswani@google.com\n\nNoam Shazeer \u2217 Google Brain noam@google.com\n\nNiki Parmar \u2217 Google Resear\n...\nord.\n\n&lt;!-- image --&gt;\n\n******************************************************************************** Chunk 14 ********************************************************************************\n\nInput-Input Layer5\n\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n\n&lt;!-- image --&gt;\n</code></pre> <p>Indeed, we have obtained a list of chunks with the extracted content, one per page.</p>"},{"location":"examples/text/paged_splitter/#experimenting-with-custom-parameteres","title":"Experimenting with custom parameteres","text":"<p>In case that we want to split by group of many pages (e.g., <code>3</code>), we can specify that value on the <code>PageSplitter</code> object. In addition, we can define an overlap between characters:</p> <pre><code>splitter = PagedSplitter(chunk_size=3, chunk_overlap=100)\nsplitter_output = splitter.split(reader_output=reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"\\n\" + \"*\" * 80 + f\" Chunk {idx} \" + \"*\" * 80 + \"\\n\")\n    print(chunk)\n</code></pre> <pre><code>******************************************************************************** Chunk 0 ********************************************************************************\n\nProvided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n\n## Attention Is All You Need\n\nAshish Vaswani \u2217 Google Brain avaswani@google.com\n\nNoam Shazeer \u2217 Google Brain noam@google.com\n\nNiki Parmar \u2217 Google Resear\n...\nlution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n\n&lt;!-- image --&gt;\nInput-Input Layer5\n\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n\n&lt;!-- image --&gt;\n</code></pre> <p>And that's it! Try to experiment which values are the best option for your use case. A full reference to this class is available on the API Reference. </p> <p>Thank you for reading! :)</p>"},{"location":"examples/text/paged_splitter/#complete-script","title":"Complete script","text":"<pre><code>from splitter_mr.reader import DoclingReader #, VanillaReader, MarkItDownReader\nfrom splitter_mr.splitter import PagedSplitter\n\nFILE_PATH = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/attention.pdf\"\n\nreader = DoclingReader()\nreader_output = reader.read(file_path=FILE_PATH)\n\nprint(reader_output.model_dump_json(indent=4))\n\nsplitter = PagedSplitter()\nsplitter_output = splitter.split(reader_output=reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"\\n\" + \"*\"*80 + f\" Chunk {idx} \" + \"*\"*80 + \"\\n\")\n    print(chunk)\n\nsplitter = PagedSplitter(chunk_size=3, chunk_overlap = 100)\nsplitter_output = splitter.split(reader_output=reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"\\n\" + \"*\"*80 + f\" Chunk {idx} \" + \"*\"*80 + \"\\n\")\n    print(chunk)\n</code></pre>"},{"location":"examples/text/recursive_character_splitter/","title":"Example: Read a basic document and chunk it with a <code>RecursiveCharacterSplitter</code>","text":"<p>As an example, we will use the first chapter of the book \"El ingenioso hidalgo Don Quijote de La Mancha\". The text of reference can be extracted from the GitHub project.</p> <p></p>"},{"location":"examples/text/recursive_character_splitter/#step-1-read-the-text-using-a-reader-component","title":"Step 1: Read the text using a Reader component","text":"<p>We will use the <code>VanillaReader</code> class, since there is no need to transform the text into a <code>markdown</code> format. </p> <p>Firstly, we will create a new Python file and instantiate our class as follows:</p> <pre><code>from splitter_mr.reader import VanillaReader\n\nreader = VanillaReader()\n</code></pre> <p>To read the file, we only need to call the <code>read</code> method from this class, which is inherited from the <code>BaseReader</code> class (see documentation).</p> <pre><code>url = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/quijote_example.txt\"\nreader_output = reader.read(file_url=url)\n</code></pre> <p>The <code>reader_output</code> variable now contains a <code>ReaderOutput</code> object, with the following fields:</p> <pre><code>print(reader_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"text\": \"Cap\u00edtulo Primero\\n\\nQue trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha\\n\\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv\u00eda un hidalgo de los de lanza en astillero, adarga antigua, roc\u00edn flaco y galgo corredor. Una olla de algo m\u00e1s vaca que carnero, salpic\u00f3n las m\u00e1s noches, duelos y quebrantos los s\u00e1bados, lentejas los viernes, alg\u00fan palomino de a\u00f1adidura los domingos, consum\u00edan las tres partes de su hacienda. \n...\ntural del Toboso, nombre a su parecer m\u00fasico y peregrino y significativo, como todos los dem\u00e1s que a \u00e9l y a sus cosas hab\u00eda puesto.\",\n    \"document_name\": \"quijote_example.txt\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/quijote_example.txt\",\n    \"document_id\": \"b22470b2-e72a-4b25-95fe-f29d092cb9ec\",\n    \"conversion_method\": \"txt\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"page_placeholder\": null,\n    \"metadata\": {}\n}\n</code></pre> <p>The <code>ReaderOutput</code> object contains both the document text and useful metadata for ETL pipelines and LLM traceability. In case of using another Reader component, the output will be similar.</p> <p>To get the text, simply access the <code>text</code> attribute:</p> <pre><code>print(reader_output.text)\n</code></pre> <pre><code>Cap\u00edtulo Primero\n\nQue trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha\n\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv\u00eda un hidalgo de los de lanza en astillero, adarga antigua, roc\u00edn flaco y galgo corredor. Una olla de algo m\u00e1s vaca que carnero, salpic\u00f3n las m\u00e1s noches, duelos y quebrantos los s\u00e1bados, lentejas los viernes, alg\u00fan palomino de a\u00f1adidura los domingos, consum\u00edan las tres partes de su hacienda. El resto della conc\n...\nquien \u00e9l un tiempo anduvo enamorado, aunque seg\u00fan se entiende, ella jam\u00e1s lo supo ni se di\u00f3 cata de ello. Llam\u00e1base Aldonza Lorenzo, y a esta le pareci\u00f3 ser bien darle t\u00edtulo de se\u00f1ora de sus pensamientos; y busc\u00e1ndole nombre que no desdijese mucho del suyo, y que tirase y se encaminase al de princesa y gran se\u00f1ora, vino a llamarla DULCINEA DEL TOBOSO, porque era natural del Toboso, nombre a su parecer m\u00fasico y peregrino y significativo, como todos los dem\u00e1s que a \u00e9l y a sus cosas hab\u00eda puesto.\n</code></pre>"},{"location":"examples/text/recursive_character_splitter/#step-2-split-the-text-using-a-splitting-strategy","title":"Step 2: Split the text using a splitting strategy","text":"<p>Before splitting, you have to choose a splitting strategy depending on your needs. </p> <p>In this case, we will use <code>RecursiveCharacterSplitter</code> since it is suitable for long, unstructured texts with an unknown number of words and stop words.</p> <p>We will split the chunks to have, at maximum, 1000 characters (<code>chunk_size = 1000</code>) with a 10% of overlapping between chunks (<code>chunk_overlap = 0.1</code>). Overlapping defines the number or percentage of common words between consecutive chunks.</p> <p>Instantiate the splitter:</p> <pre><code>from splitter_mr.splitter import RecursiveCharacterSplitter\n\nsplitter = RecursiveCharacterSplitter(chunk_size=1000, chunk_overlap=0.1)\n</code></pre> <p>Apply the <code>split</code> method to the <code>reader_output</code>. This returns a <code>SplitterOutput</code> object with:</p> <pre><code>splitter_output = splitter.split(reader_output)\n\nprint(splitter_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"chunks\": [\n        \"Cap\u00edtulo Primero\\n\\nQue trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha\",\n        \"En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv\u00eda un hidalgo de los de lanza en astillero, adarga antigua, roc\u00edn flaco y galgo corredor. Una olla de algo m\u00e1s vaca que carnero, salpic\u00f3n las m\u00e1s noches, duelos y quebrantos los s\u00e1bados, lentejas los viernes, alg\u00fan palomino de a\u00f1adidura los domingos, consum\u00edan las tres par\n...\n72a-4b25-95fe-f29d092cb9ec\",\n    \"conversion_method\": \"txt\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"split_method\": \"recursive_character_splitter\",\n    \"split_params\": {\n        \"chunk_size\": 1000,\n        \"chunk_overlap\": 0.1,\n        \"separators\": [\n            \"\\n\\n\",\n            \"\\n\",\n            \" \",\n            \".\",\n            \",\",\n            \"\u200b\",\n            \"\uff0c\",\n            \"\u3001\",\n            \"\uff0e\",\n            \"\u3002\",\n            \"\"\n        ]\n    },\n    \"metadata\": {}\n}\n</code></pre> <p>To visualize every chunk, we can simply perform the following operation:</p> <pre><code>for idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + \" Chunk \" + str(idx + 1) + \" \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\nCap\u00edtulo Primero\n\nQue trata de la condici\u00f3n y ejercicio del famoso hidalgo D. Quijote de la Mancha\n\n======================================== Chunk 2 ========================================\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv\u00eda un hidalgo de los de lanza en astillero, adarga antigua, roc\u00edn flaco y galgo corredor. Una olla de algo m\u00e1s vaca que carnero, sal\n...\nuien \u00e9l un tiempo anduvo enamorado, aunque seg\u00fan se entiende, ella jam\u00e1s lo supo ni se di\u00f3 cata de ello. Llam\u00e1base Aldonza Lorenzo, y a esta le pareci\u00f3 ser bien darle t\u00edtulo de se\u00f1ora de sus pensamientos; y busc\u00e1ndole nombre que no desdijese mucho del suyo, y que tirase y se encaminase al de princesa y gran se\u00f1ora, vino a llamarla DULCINEA DEL TOBOSO, porque era natural del Toboso, nombre a su parecer m\u00fasico y peregrino y significativo, como todos los dem\u00e1s que a \u00e9l y a sus cosas hab\u00eda puesto.\n</code></pre> <p>Note</p> <p>Remember that in case that we want to use custom separators or define another <code>chunk_size</code> or overlapping, we can do it when instantiating the class. </p> <p>And that's it! This is as simple as shown here.</p>"},{"location":"examples/text/recursive_character_splitter/#complete-script","title":"Complete script","text":"<pre><code>from splitter_mr.reader import VanillaReader\nfrom splitter_mr.splitter import RecursiveCharacterSplitter\n\nreader = VanillaReader()\n\nurl = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/quijote_example.txt\"\nreader_output = reader.read(file_url = url)\n\nprint(reader_output.model_dump_json(indent=4)) # Visualize the ReaderOutput object\nprint(reader_output.text) # Get the text from the document\n\nsplitter = RecursiveCharacterSplitter(\n    chunk_size = 1000,\n    chunk_overlap = 100)\nsplitter_output = splitter.split(reader_output)\n\nprint(splitter_output.model_dump_json(indent=4)) # Print the SplitterOutput object\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    # Visualize every chunk\n    print(\"=\"*40 + \" Chunk \" + str(idx + 1) + \" \" + \"=\"*40 + \"\\n\" + chunk + \"\\n\")\n</code></pre>"},{"location":"examples/text/semantic_splitter/","title":"Example: Splitting Files by Semantic Similarity using <code>SemanticSplitter</code>","text":"<p>For some documents, the best splitting strategy is to break them at semantic boundaries rather than fixed lengths or pages. This is exactly what the <code>SemanticSplitter</code> does, which uses cosine similarity to detect topic or meaning shifts between sentences. This allows to produce semantically coherent chunks.</p> <p>In this example, we will read the famous Pinocchio tale (<code>pinocchio_example.md</code>) using <code>VanillaReader</code>, and then split it into chunks using <code>SemanticSplitter</code>.</p>"},{"location":"examples/text/semantic_splitter/#step-1-read-the-file","title":"Step 1. Read the file","text":"<p>You can read the file using <code>VanillaReader</code> or any other <code>Reader</code> that outputs a <code>ReaderOutput</code> object.</p> <pre><code>from splitter_mr.reader import VanillaReader\n\nFILE_PATH = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/pinocchio_example.md\"\n\nreader = VanillaReader()\nreader_output = reader.read(file_path=FILE_PATH)\n</code></pre> <p>The output will look like:</p> <pre><code>print(reader_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"text\": \"# Pinocchio by Carlo Colodi (*The Tale of a Puppet*)\\n\\n## Chapter 1\\n\\n### THE PIECE OF WOOD THAT LAUGHED AND CRIED LIKE A CHILD\\n\\nThere was once upon a time a piece of wood in the shop of an old carpenter named Master Antonio. Everybody, however, called him Master Cherry, on account of the end of his nose, which was always as red and polished as a ripe cherry.\\n\\nNo sooner had Master Cherry set eyes on the piece of wood than his face beamed with delight, and, rubbing his hands \n...\n rest of their lives.\\n\\nGeppetto carried off his fine piece of wood and, thanking Master Antonio, returned limping to his house.\",\n    \"document_name\": \"pinocchio_example.md\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/pinocchio_example.md\",\n    \"document_id\": \"9ad18596-3dbc-49ba-b428-fbd2db3fd7f4\",\n    \"conversion_method\": \"txt\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"page_placeholder\": null,\n    \"metadata\": {}\n}\n</code></pre> <p>Note that this is a Pydantic object, so you can represent it as a JSON dictionary with the <code>.model_dump_json()</code> instruction.</p>"},{"location":"examples/text/semantic_splitter/#step-2-split-by-semantic-similarity","title":"Step 2. Split by semantic similarity","text":"<p>To split semantically, instantiate the <code>SemanticSplitter</code> object with an embedding backend and call to the <code>.split()</code> method:</p> <pre><code>from splitter_mr.embedding import (\n    AzureOpenAIEmbedding,\n)  # can be any other Embedding model.\nfrom splitter_mr.splitter import SemanticSplitter\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv\n\nparams = {\n    \"model_name\": os.getenv(\"AZURE_OPENAI_EMBEDDING\"),\n    \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_EMBEDDING\"),\n    \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n}\n\nembedding = AzureOpenAIEmbedding(**params)  # can be any other Embedding model.\n\nsplitter = SemanticSplitter(embedding)\nsplitter_output = splitter.split(reader_output)\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"\\n\" + \"*\" * 80 + f\" Chunk {idx} \" + \"*\" * 80 + \"\\n\")\n    print(chunk)\n</code></pre> <pre><code>******************************************************************************** Chunk 0 ********************************************************************************\n\n# Pinocchio by Carlo Colodi (*The Tale of a Puppet*)\n\n## Chapter 1\n\n### THE PIECE OF WOOD THAT LAUGHED AND CRIED LIKE A CHILD\n\nThere was once upon a time a piece of wood in the shop of an old carpenter named Master Antonio. Everybody, however, called him Master Cherry, on account of the end of his nose, which was always as red\n...\nn!\" \"Pudding!\" On hearing himself called Pudding for the third time Geppetto, mad with rage, fell upon the carpenter and they fought desperately. When the battle was over, Master Antonio had two more scratches on his nose, and his adversary had lost two buttons off his waistcoat. Their accounts being thus squared, they shook hands and swore to remain good friends for the rest of their lives. Geppetto carried off his fine piece of wood and, thanking Master Antonio, returned limping to his house.\n</code></pre>"},{"location":"examples/text/semantic_splitter/#how-it-works","title":"How it works","text":"<p>The SemanticSplitter algorithm:</p> <ol> <li>Sentence Splitting \u2014 Breaks text into individual sentences using <code>SentenceSplitter</code>.</li> <li>Sliding Window Context \u2014 Combines each sentence with <code>buffer_size</code> neighbors before and after for better semantic representation.</li> <li>Embedding Generation \u2014 Uses the provided <code>BaseEmbedding</code> model to get vector representations of each combined sentence.</li> <li>Distance Calculation \u2014 Computes cosine distances between consecutive embeddings.</li> <li>Breakpoint Detection \u2014 Finds points where the distance exceeds a threshold (based on percentile, standard deviation, interquartile range, or gradient).</li> <li>Chunk Assembly \u2014 Merges sentences between breakpoints into chunks, ensuring each meets the minimum <code>chunk_size</code>.</li> </ol> <p>To consult information more-in-depth about how this algorithm works, you can check the following section.</p>"},{"location":"examples/text/semantic_splitter/#customizing-parameters","title":"Customizing Parameters","text":"<p>You can adjust how chunks are detected and their minimum length:</p> <pre><code>splitter = SemanticSplitter(\n    embedding=embedding,\n    buffer_size=1,  # number of neighbor sentences to include\n    breakpoint_threshold_type=\"percentile\",  # method for determining breakpoints\n    breakpoint_threshold_amount=80.0,  # threshold value (percentile here)\n    chunk_size=1000,  # minimum characters per chunk\n)\nsplitter_output = splitter.split(reader_output)\nprint(splitter_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"chunks\": [\n        \"# Pinocchio by Carlo Colodi (*The Tale of a Puppet*)\\n\\n## Chapter 1\\n\\n### THE PIECE OF WOOD THAT LAUGHED AND CRIED LIKE A CHILD\\n\\nThere was once upon a time a piece of wood in the shop of an old carpenter named Master Antonio. Everybody, however, called him Master Cherry, on account of the end of his nose, which was always as red and polished as a ripe cherry. No sooner had Master Cherry set eyes on the piece of wood than his face beamed with delight, and, rubbing h\n...\n  \"document_id\": \"9ad18596-3dbc-49ba-b428-fbd2db3fd7f4\",\n    \"conversion_method\": \"txt\",\n    \"reader_method\": \"vanilla\",\n    \"ocr_method\": null,\n    \"split_method\": \"semantic_splitter\",\n    \"split_params\": {\n        \"buffer_size\": 1,\n        \"breakpoint_threshold_type\": \"percentile\",\n        \"breakpoint_threshold_amount\": 80.0,\n        \"number_of_chunks\": null,\n        \"chunk_size\": 1000,\n        \"model_name\": \"es-BPE_GENAI_CLASSIFIER_AGENT-llm-lab-embedding-3-large\"\n    },\n    \"metadata\": {}\n}\n</code></pre> <p>In this case, more chunks are extracted since the splitter becomes more sensitive to smaller semantic changes.</p> <p>Other available <code>breakpoint_threshold_type</code> values are:</p> <ul> <li><code>\"percentile\"</code> \u2014 Split at distances above a given percentile.</li> <li><code>\"standard_deviation\"</code> \u2014 Split at mean + (amount \u00d7 std deviation).</li> <li><code>\"interquartile\"</code> \u2014 Split at mean + (amount \u00d7 IQR).</li> <li><code>\"gradient\"</code> \u2014 Split at steep changes in distance.</li> </ul> <p>Alternatively, you can also directly control the number of chunks by setting <code>number_of_chunks</code>.</p>"},{"location":"examples/text/semantic_splitter/#complete-script","title":"Complete Script","text":"<pre><code>from splitter_mr.embedding import AzureOpenAIEmbedding\nfrom splitter_mr.reader import VanillaReader\nfrom splitter_mr.splitter import SemanticSplitter\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nFILE_PATH = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/pinocchio_example.md\"\n\nparams = {\n    \"model_name\": os.getenv(\"AZURE_OPENAI_EMBEDDING\"),\n    \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_EMBEDDING\"),\n    \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n}\n\nembedding = AzureOpenAIEmbedding(**params)  # can be any other Embedding model.\nreader = VanillaReader()\nreader_output = reader.read(file_path=FILE_PATH)\n\nprint(\"*\" * 40 + \"\\n Output from Reader: \\n\" + \"*\" * 40)\nprint(reader_output.model_dump_json(indent=4))\n\nsplitter = SemanticSplitter(embedding)\nsplitter_output = splitter.split(reader_output)\n\nprint(\"*\" * 40 + \"\\n Output from Splitter: \\n\" + \"*\" * 40)\nprint(splitter_output.model_dump_json(indent=4))\n\nfor idx, chunk in enumerate(splitter_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40)\n    print(chunk)\n</code></pre> <pre><code>****************************************\n Output from Reader: \n****************************************\n{\n    \"text\": \"# Pinocchio by Carlo Colodi (*The Tale of a Puppet*)\\n\\n## Chapter 1\\n\\n### THE PIECE OF WOOD THAT LAUGHED AND CRIED LIKE A CHILD\\n\\nThere was once upon a time a piece of wood in the shop of an old carpenter named Master Antonio. Everybody, however, called him Master Cherry, on account of the end of his nose, which was always as red and polished as a ripe cherry.\\n\\nNo sooner ha\n...\nn!\" \"Pudding!\" On hearing himself called Pudding for the third time Geppetto, mad with rage, fell upon the carpenter and they fought desperately. When the battle was over, Master Antonio had two more scratches on his nose, and his adversary had lost two buttons off his waistcoat. Their accounts being thus squared, they shook hands and swore to remain good friends for the rest of their lives. Geppetto carried off his fine piece of wood and, thanking Master Antonio, returned limping to his house.\n</code></pre>"},{"location":"examples/text/semantic_splitter/#annex-semantic-splitting-algorithm","title":"Annex: Semantic Splitting algorithm","text":"<p>The Semantic Splitter detects semantic shift points between adjacent sentence windows by:</p> <ol> <li>Splitting the text into sentences.</li> <li>Creating a sliding window around each sentence (<code>buffer_size</code> neighbors on both sides).</li> <li>Embedding each window with the configured <code>BaseEmbedding</code>.</li> <li>Computing cosine distances between consecutive windows.</li> <li>Selecting breakpoints where the signal exceeds a threshold (or meets a desired number of chunks).</li> <li>Emitting chunks between breakpoints, honoring a minimum <code>chunk_size</code> (in characters).</li> </ol> <p>Below, the full process will be detailed.</p>"},{"location":"examples/text/semantic_splitter/#notation","title":"Notation","text":"<ul> <li> <p>Let the document be split into sentences \\( S = [s_0, s_1, \\ldots, s_{n-1}] \\).</p> </li> <li> <p>For each index \\( i \\), define a window \\( w_i \\) as the concatenation of sentences</p> </li> </ul> <p>$$   w_i = \\text{concat}\\big(s_{\\max(0,\\, i-b)}, \\ldots, s_i, \\ldots, s_{\\min(n-1,\\, i+b)}\\big),   $$   where \\( b = \\texttt{buffer\\_size} \\).</p> <ul> <li> <p>Let \\( \\mathbf{e}_i = \\text{embed}(w_i) \\in \\mathbb{R}^d \\) be the embedding vector.</p> </li> <li> <p>Define cosine similarity and distance:</p> </li> </ul> \\[\\text{cos\\_sim}(\\mathbf{e}_i, \\mathbf{e}_{i+1})= \\frac{\\mathbf{e}_i \\cdot \\mathbf{e}_{i+1}}{\\lVert \\mathbf{e}_i \\rVert \\, \\lVert \\mathbf{e}_{i+1} \\rVert + \\varepsilon}, \\qquad d_i = 1 - \\operatorname{cos\\_sim}(\\mathbf{e}_i, \\mathbf{e}_{i+1}),\\] <p>with a small \\( \\varepsilon \\) for numerical stability.</p> <p>We obtain a distance vector \\( D = [d_0, d_1, \\ldots, d_{n-2}] \\) (length \\( n-1 \\)).</p>"},{"location":"examples/text/semantic_splitter/#breakpoint-selection","title":"Breakpoint Selection","text":"<p>Let $ R $ be the reference array used to select cut indices: - For most strategies, $ R = D $. - For <code>\"gradient\"</code>, compute $ G = \\nabla D $ and set $ R = G $.</p> <p>We then choose a threshold $ T $ and mark breakpoints at all indices $ i $ where $ R[i] &gt; T $.</p>"},{"location":"examples/text/semantic_splitter/#threshold-strategies","title":"Threshold Strategies","text":"<p>Percentile (default):</p> <ul> <li>$ T = \\text{percentile}(D, p) $, where $ p \\in [0, 100] $.</li> <li>Intuition: cut at the largest $ (100 - p)\\% $ jumps.</li> </ul> <p>Standard Deviation:</p> <ul> <li>$ \\mu = \\text{mean}(D),\\ \\sigma = \\text{std}(D) $</li> <li>$ T = \\mu + \\alpha \\sigma $ (e.g., $ \\alpha=3 $).</li> <li>Intuition: cut on statistical outliers.</li> </ul> <p>Interquartile (IQR):</p> <ul> <li>$ Q_1, Q_3 = \\text{percentile}(D, 25), \\text{percentile}(D, 75) $</li> <li>$ \\text{IQR} = Q_3 - Q_1,\\ \\mu = \\text{mean}(D) $</li> <li>$ T = \\mu + \\beta \\cdot \\text{IQR} $ (e.g., $ \\beta=1.5 $).</li> <li>Intuition: robust outlier detection vs. heavy tails.</li> </ul> <p>Gradient:</p> <ul> <li>Compute $ G = \\nabla D $ and threshold by percentile on $ G $:      $ T = \\text{percentile}(G, p) $.</li> <li>Intuition: cut at steep changes in the distance signal (useful when distances drift).</li> </ul> <p>Note</p> <p>For <code>\"percentile\"</code> and <code>\"gradient\"</code>, the class accepts <code>breakpoint_threshold_amount</code> in either \\([0, 1]\\) (interpreted as a fraction, converted internally to \\([0,100]\\)) or \\([0,100]\\) (as a percentile). For <code>\"standard_deviation\"</code> and <code>\"interquartile\"</code>, the amount is a multiplier (\\(\\alpha\\) or \\(\\beta\\)).</p>"},{"location":"examples/text/semantic_splitter/#targeting-a-desired-number-of-chunks","title":"Targeting a Desired Number of Chunks","text":"<p>If <code>number_of_chunks</code> is set, the splitter maps the requested count to an inverse percentile over $ D $:</p> <ul> <li> <p>Let $ m = |D| $. Requested chunks $ k $ are mapped to a percentile $ y \\in [0,100] $ such that:</p> </li> <li> <p>$ k=1 \\Rightarrow y \\approx 100 $ (almost no cuts),</p> </li> <li> <p>$ k=m \\Rightarrow y \\approx 0 $ (many cuts).</p> </li> <li> <p>The threshold is then $ T = \\text{percentile}(D, y) $.</p> </li> </ul> <p>This provides an approximate, monotonic control over chunk count without explicit clustering.</p>"},{"location":"examples/text/semantic_splitter/#chunk-assembly-and-chunk_size-enforcement","title":"Chunk Assembly and <code>chunk_size</code> Enforcement","text":"<p>After computing indices $ \\mathcal{B} = {i \\mid R[i] &gt; T} $, we sweep left-to-right:</p> <ol> <li>Maintain a <code>start_idx</code> (initially 0).</li> <li>For each $ i \\in \\mathcal{B} $ in ascending order, propose a cut after sentence $ i $ (i.e., up to index $ i $ inclusive).</li> <li>Build the candidate chunk $ C = \\text{concat}(s_{\\text{start}}, \\ldots, s_{i}) $.</li> <li>If <code>len(C) &gt;= chunk_size</code>, emit the chunk and set <code>start_idx = i + 1</code>.</li> <li>After the sweep, emit the tail (if non-empty).</li> </ol> <p>If no candidate passes <code>chunk_size</code>, the splitter falls back to a single chunk (all sentences concatenated).</p> <p><code>chunk_size</code> is a minimum only. Chunks can be larger depending on where valid cut points occur.</p>"},{"location":"examples/text/token_splitter/","title":"Example: Split a Document by Tokens with <code>TokenSplitter</code> (SpaCy, NLTK, tiktoken)","text":"<p>In this example, we will use several popular NLP libraries to split a text document into token-based chunks. A token is the lexical unit in which a text is divided into. Tokenization can be performed in many ways: by words, by characters, by lemmas, etc. One of the most common methods is by sub-words. </p> <p>Observe the following example:</p> <p></p> <p>Most of Large Language Model uses tokenizers to process a large text into comprehensive lexical units . Hence, split by tokens could be a suitable option to produce chunks of a fixed length compatible with the LLM context window. So, in this tutorial we show how to split the text using three tokenizers: SpaCy, NLTK, and tiktoken (OpenAI tokenization). Let's see!</p>"},{"location":"examples/text/token_splitter/#step-1-read-the-text-using-a-reader","title":"Step 1: Read the Text Using a Reader","text":"<p>We will start by reading a text file using the <code>MarkItDownReader</code>. Remember that you can use any other compatible Reader. Simply, instantiate a Reader object and use the <code>read</code> method. Provide as an argument the file to be read, which can be an URL, variable or path</p> <pre><code>from splitter_mr.reader import MarkItDownReader\n\nfile = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/my_wonderful_family.txt\"\nreader = MarkItDownReader()\nreader_output = reader.read(file)\n</code></pre> <p>The output is a <code>ReaderOutput</code> object:</p> <pre><code>print(reader_output.model_dump_json(indent=4))\n</code></pre> <pre><code>{\n    \"text\": \"My Wonderful Family\\nI live in a house near the mountains. I have two brothers and one sister, and I was born last. My father teaches mathematics, and my mother is a nurse at a big hospital. My brothers are very smart and work hard in school. My sister is a nervous girl, but she is very kind. My grandmother also lives with us. She came from Italy when I was two years old. She has grown old, but she is still very strong. She cooks the best food!\\n\\nMy family is very important to me\n...\nOn the weekends we all play board games together. We laugh and always have a good time. I love my family very much.\",\n    \"document_name\": \"my_wonderful_family.txt\",\n    \"document_path\": \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/my_wonderful_family.txt\",\n    \"document_id\": \"5dc57f34-8bd2-4cfb-9442-7324a0b814ec\",\n    \"conversion_method\": \"markdown\",\n    \"reader_method\": \"markitdown\",\n    \"ocr_method\": null,\n    \"page_placeholder\": null,\n    \"metadata\": {}\n}\n</code></pre> <p>To see only the document text, you can access to the <code>text</code> attribute of this object:</p> <pre><code>print(reader_output.text)\n</code></pre> <pre><code>My Wonderful Family\nI live in a house near the mountains. I have two brothers and one sister, and I was born last. My father teaches mathematics, and my mother is a nurse at a big hospital. My brothers are very smart and work hard in school. My sister is a nervous girl, but she is very kind. My grandmother also lives with us. She came from Italy when I was two years old. She has grown old, but she is still very strong. She cooks the best food!\n\nMy family is very important to me. We do lots of things together. My brothers and I like to go on long walks in the mountains. My sister likes to cook with my grandmother. On the weekends we all play board games together. We laugh and always have a good time. I love my family very much.\n</code></pre>"},{"location":"examples/text/token_splitter/#step-2-split-the-document-by-tokens","title":"Step 2: Split the Document by Tokens","text":"<p>As we have said, the <code>TokenSplitter</code> lets you pick the tokenization backend: SpaCy, NLTK, or tiktoken. Use one or another depending on your needs. For every tokenizer, it should be passed:</p> <ul> <li>A <code>chunk_size</code>, the maximum chunk size in characters for the tokenization process. It tries to never cut a sentence in two chunks.</li> <li>A <code>model_name</code>, the tokenizer model to use. It should always follows this structure: <code>{tokenizer}/{model_name}</code>, e.g., <code>tiktoken/cl100k_base</code>. </li> </ul> <p>Note</p> <p>For spaCy and tiktoken, the corresponding models must be installed in your environment.</p> <p>To see a complete list of available tokenizers, refer to Available models.</p>"},{"location":"examples/text/token_splitter/#21-split-by-tokens-using-spacy","title":"2.1. Split by Tokens Using SpaCy","text":"<p>To split using a spaCy tokenizer model, you firstly need to instantiate the <code>TokenSplitter</code> class and select the parameters. Then, call to the <code>split</code> method with the path, URL or variable to split on:</p> <pre><code>from splitter_mr.splitter import TokenSplitter\n\nspacy_splitter = TokenSplitter(\n    chunk_size=100,\n    model_name=\"spacy/en_core_web_sm\",  # Use the SpaCy model with \"spacy/{model_name}\" format\n)\nspacy_output = spacy_splitter.split(reader_output)\n\nprint(spacy_output)  # See the SplitterOutput object\n</code></pre> <pre><code>chunks=['My Wonderful Family\\nI live in a house near the mountains.', 'I have two brothers and one sister, and I was born last.', 'My father teaches mathematics, and my mother is a nurse at a big hospital.', 'My brothers are very smart and work hard in school.', 'My sister is a nervous girl, but she is very kind.\\n\\nMy grandmother also lives with us.', 'She came from Italy when I was two years old.\\n\\nShe has grown old, but she is still very strong.', 'She cooks the best food!\\n\\n\\n\\nMy family i\n...\n 'a3b56da5-c885-43d5-8846-b0329f232af7', '6cf64509-b3f8-4a60-92f4-5006a4693e8a'] document_name='my_wonderful_family.txt' document_path='https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/my_wonderful_family.txt' document_id='4798e9f9-4a0e-4453-8924-78748c6402d4' conversion_method='markdown' reader_method='markitdown' ocr_method=None split_method='token_splitter' split_params={'chunk_size': 100, 'model_name': 'spacy/en_core_web_sm', 'language': 'english'} metadata={}\n\n\n/Users/aherencia/Documents/Projects/Splitter_MR/.venv/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n  warnings.warn(Warnings.W108)\n</code></pre> <p>To see the resulting chunks, you can use the following code:</p> <pre><code># Visualize each chunk\nfor idx, chunk in enumerate(spacy_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\nMy Wonderful Family\nI live in a house near the mountains.\n\n======================================== Chunk 2 ========================================\nI have two brothers and one sister, and I was born last.\n\n======================================== Chunk 3 ========================================\nMy father teaches mathematics, and my mother is a nurse at a big hospital.\n\n=====================================\n...\n\n\n======================================== Chunk 8 ========================================\nMy brothers and I like to go on long walks in the mountains.\n\n======================================== Chunk 9 ========================================\nMy sister likes to cook with my grandmother.\n\nOn the weekends we all play board games together.\n\n======================================== Chunk 10 ========================================\nWe laugh and always have a good time.\n\nI love my family very much.\n</code></pre>"},{"location":"examples/text/token_splitter/#22-split-by-tokens-using-nltk","title":"2.2. Split by Tokens Using NLTK","text":"<p>Similarly, you can use a NLTK tokenizer. This library will always use <code>punkt</code> as the tokenizer, but you can customize the language through this parameter.</p> <pre><code>nltk_splitter = TokenSplitter(\n    chunk_size=100,\n    model_name=\"nltk/punkt\",  # Use the NLTK model as \"nltk/{model_name}\"\n    language=\"english\",  # Defaults to English\n)\nnltk_output = nltk_splitter.split(reader_output)\n\nfor idx, chunk in enumerate(nltk_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\nMy Wonderful Family\nI live in a house near the mountains.\n\n======================================== Chunk 2 ========================================\nI have two brothers and one sister, and I was born last.\n\n======================================== Chunk 3 ========================================\nMy father teaches mathematics, and my mother is a nurse at a big hospital.\n\n=====================================\n...\n\n\n======================================== Chunk 8 ========================================\nMy brothers and I like to go on long walks in the mountains.\n\n======================================== Chunk 9 ========================================\nMy sister likes to cook with my grandmother.\n\nOn the weekends we all play board games together.\n\n======================================== Chunk 10 ========================================\nWe laugh and always have a good time.\n\nI love my family very much.\n</code></pre> <p>As you can see, the results are basically the same.</p>"},{"location":"examples/text/token_splitter/#23-split-by-tokens-using-tiktoken-openai","title":"2.3. Split by Tokens Using tiktoken (OpenAI)","text":"<p>TikToken is one of the most extended tokenizer models. In this case, this tokenizer split by the number of tokens and chunks if <code>\\\\n\\\\n</code> is detected. Hence, the results are the following:</p> <pre><code>tiktoken_splitter = TokenSplitter(\n    chunk_size=100,\n    model_name=\"tiktoken/cl100k_base\",  # Use the tiktoken model as \"tiktoken/{model_name}\"\n    language=\"english\",\n)\ntiktoken_output = tiktoken_splitter.split(reader_output)\n\nfor idx, chunk in enumerate(tiktoken_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>======================================== Chunk 1 ========================================\nMy Wonderful Family\n\n======================================== Chunk 2 ========================================\nI live in a house near the mountains. I have two brothers and one sister, and I was born last. My father teaches mathematics, and my mother is a nurse at a big hospital. My brothers are very smart and work hard in school. My sister is a nervous girl, but she is very kind. My grandmother also lives \n...\nShe came from Italy when I was two years old. She has grown old, but she is still very strong. She cooks the best food!\n\n======================================== Chunk 3 ========================================\nMy family is very important to me. We do lots of things together. My brothers and I like to go on long walks in the mountains. My sister likes to cook with my grandmother. On the weekends we all play board games together. We laugh and always have a good time. I love my family very much.\n</code></pre>"},{"location":"examples/text/token_splitter/#extra-split-by-tokens-in-other-languages-eg-spanish","title":"Extra: Split by Tokens in Other Languages (e.g., Spanish)","text":"<p>In previous examples, we show you how to split the text by tokens, but these models were adapted to English. But in case that you have texts in other languages, you can use other Tokenizers. Here, there are two examples with SpaCy and NLTK (tiktoken is multilingual by default):</p> <pre><code>from splitter_mr.reader import DoclingReader\n\nsp_file = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/mi_nueva_casa.txt\"\nsp_reader = DoclingReader()\nsp_reader_output = sp_reader.read(sp_file)\nprint(sp_reader_output.text)\n</code></pre> <pre><code>Mi nueva casa\nYo vivo en Granada, una ciudad peque\u00f1a que tiene monumentos muy importantes como la Alhambra. Aqu\u00ed la comida es deliciosa y son famosos el gazpacho, el rebujito y el salmorejo.\n\nMi nueva casa est\u00e1 en una calle ancha que tiene muchos \u00e1rboles. El piso de arriba de mi casa tiene tres dormitorios y un despacho para trabajar. El piso de abajo tiene una cocina muy grande, un comedor con una mesa y seis sillas, un sal\u00f3n con dos sof\u00e1s verdes, una televisi\u00f3n y cortinas. Adem\u00e1s, tiene una peque\u00f1a terraza con piscina donde puedo tomar el sol en verano.\n\nMe gusta mucho mi casa porque puedo invitar a mis amigos a cenar o a ver el f\u00fatbol en mi televisi\u00f3n. Adem\u00e1s, cerca de mi casa hay muchas tiendas para hacer la compra, como panader\u00eda, carnicer\u00eda y pescader\u00eda.\n\n\n\n\n/Users/aherencia/Documents/Projects/Splitter_MR/src/splitter_mr/reader/readers/docling_reader.py:87: UserWarning: Unsupported extension 'txt'. Using VanillaReader.\n  warnings.warn(msg)\n</code></pre>"},{"location":"examples/text/token_splitter/#split-spanish-by-tokens-using-spacy","title":"Split Spanish by Tokens Using SpaCy","text":"<pre><code>spacy_sp_splitter = TokenSplitter(\n    chunk_size=100,\n    model_name=\"spacy/es_core_news_sm\",  # Use a Spanish SpaCy model\n)\nspacy_sp_output = spacy_sp_splitter.split(sp_reader_output)\n\nfor idx, chunk in enumerate(spacy_sp_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>Created a chunk of size 107, which is longer than the specified 100\nCreated a chunk of size 142, which is longer than the specified 100\n\n\n======================================== Chunk 1 ========================================\nMi nueva casa\nYo vivo en Granada, una ciudad peque\u00f1a que tiene monumentos muy importantes como la Alhambra.\n\n======================================== Chunk 2 ========================================\nAqu\u00ed la comida es deliciosa y son famosos el gazpacho, el rebujito y el salmorejo.\n\n======================================== Chunk 3 ========================================\nMi nueva casa est\u00e1 en una calle ancha\n...\n==========================\nAdem\u00e1s, tiene una peque\u00f1a terraza con piscina donde puedo tomar el sol en verano.\n\n======================================== Chunk 7 ========================================\nMe gusta mucho mi casa porque puedo invitar a mis amigos a cenar o a ver el f\u00fatbol en mi televisi\u00f3n.\n\n======================================== Chunk 8 ========================================\nAdem\u00e1s, cerca de mi casa hay muchas tiendas para hacer la compra, como panader\u00eda, carnicer\u00eda y pescader\u00eda.\n</code></pre>"},{"location":"examples/text/token_splitter/#split-spanish-by-tokens-using-nltk","title":"Split Spanish by Tokens Using NLTK","text":"<pre><code>nltk_sp_splitter = TokenSplitter(\n    chunk_size=100, model_name=\"nltk/punkt\", language=\"spanish\"\n)\nnltk_sp_output = nltk_sp_splitter.split(sp_reader_output)\n\nfor idx, chunk in enumerate(nltk_sp_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>Created a chunk of size 107, which is longer than the specified 100\nCreated a chunk of size 142, which is longer than the specified 100\n\n\n======================================== Chunk 1 ========================================\nMi nueva casa\nYo vivo en Granada, una ciudad peque\u00f1a que tiene monumentos muy importantes como la Alhambra.\n\n======================================== Chunk 2 ========================================\nAqu\u00ed la comida es deliciosa y son famosos el gazpacho, el rebujito y el salmorejo.\n\n======================================== Chunk 3 ========================================\nMi nueva casa est\u00e1 en una calle ancha\n...\n==========================\nAdem\u00e1s, tiene una peque\u00f1a terraza con piscina donde puedo tomar el sol en verano.\n\n======================================== Chunk 7 ========================================\nMe gusta mucho mi casa porque puedo invitar a mis amigos a cenar o a ver el f\u00fatbol en mi televisi\u00f3n.\n\n======================================== Chunk 8 ========================================\nAdem\u00e1s, cerca de mi casa hay muchas tiendas para hacer la compra, como panader\u00eda, carnicer\u00eda y pescader\u00eda.\n</code></pre> <p>And that\u2019s it! You can now tokenize and chunk text with precision, using the NLP backend and language that best fits your project.</p> <p>Note</p> <p>For best results, make sure to install any SpaCy/NLTK/tiktoken models needed for your language and task.</p>"},{"location":"examples/text/token_splitter/#complete-script","title":"Complete Script","text":"<pre><code>from splitter_mr.reader import DoclingReader, MarkItDownReader\nfrom splitter_mr.splitter import TokenSplitter\n\n# 1. Read the file using any Reader (e.g., MarkItDownReader)\n\nfile = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/my_wonderful_family.txt\"\n\nreader = MarkItDownReader()\nreader_output = reader.read(file)\nprint(reader_output.text)\n\n# 2. Split by Tokens\n\n## 2.1. Using SpaCy\n\nprint(\"*\" * 40 + \" spaCy \" + \"*\" * 40)\n\nspacy_splitter = TokenSplitter(\n    chunk_size=100,\n    model_name=\"spacy/en_core_web_sm\",  # Select a valid model with nomenclature spacy/{model_name}.\n)\n# Note that it is required to have the model installed in your execution machine.\n\nspacy_output = spacy_splitter.split(reader_output)  # Split the text\nprint(spacy_output.model_dump_json(indent=4))  # Print the SplitterOutput object\n\n# Visualize each chunk\nfor idx, chunk in enumerate(spacy_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n\n## 2.2. Using NLTK\n\nprint(\"*\" * 40 + \" NLTK \" + \"*\" * 40)\n\nnltk_splitter = TokenSplitter(\n    chunk_size=100,\n    model_name=\"nltk/punkt\",  # introduce the model as nltk/{model_name}\n    language=\"english\",  # defaults to this language\n)\n\nnltk_output = nltk_splitter.split(reader_output)\n\n# Visualize each chunk\nfor idx, chunk in enumerate(nltk_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n\n## 2.3. Using tiktoken\n\nprint(\"*\" * 40 + \" Tiktoken \" + \"*\" * 40)\n\ntiktoken_splitter = TokenSplitter(\n    chunk_size=100,\n    model_name=\"tiktoken/cl100k_base\",  # introduce the model as tiktoken/{model_name}\n    language=\"english\",\n)\n\ntiktoken_output = tiktoken_splitter.split(reader_output)\n\n# Visualize each chunk\nfor idx, chunk in enumerate(tiktoken_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n\n## 2.4. Split by tokens in other languages (e.g., Spanish)\n\nsp_file = \"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/data/mi_nueva_casa.txt\"\n\nsp_reader = DoclingReader()\nsp_reader_output = sp_reader.read(sp_file)\nprint(sp_reader_output.text)  # Visualize the text content\n\n### 2.4.1. Using SpaCy\n\nprint(\"*\" * 40 + \" Spacy in Spanish \" + \"*\" * 40)\n\nspacy_sp_splitter = TokenSplitter(\n    chunk_size=100,\n    model_name=\"spacy/es_core_news_sm\",  # Pick another model in Spanish\n)\nnltk_sp_output = spacy_sp_splitter.split(sp_reader_output)\n\nfor idx, chunk in enumerate(nltk_sp_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n\n### 2.4.2 Using NLTK\n\nprint(\"*\" * 40 + \" NLTK in Spanish \" + \"*\" * 40)\n\nnltk_sp_splitter = TokenSplitter(\n    chunk_size=100,\n    model_name=\"nltk/punkt\",\n    language=\"spanish\",  # select `spanish` as language for the tokenizer\n)\nnltk_sp_output = nltk_sp_splitter.split(sp_reader_output)\n\nfor idx, chunk in enumerate(nltk_sp_output.chunks):\n    print(\"=\" * 40 + f\" Chunk {idx + 1} \" + \"=\" * 40 + \"\\n\" + chunk + \"\\n\")\n</code></pre> <pre><code>My Wonderful Family\nI live in a house near the mountains. I have two brothers and one sister, and I was born last. My father teaches mathematics, and my mother is a nurse at a big hospital. My brothers are very smart and work hard in school. My sister is a nervous girl, but she is very kind. My grandmother also lives with us. She came from Italy when I was two years old. She has grown old, but she is still very strong. She cooks the best food!\n\nMy family is very important to me. We do lots of things together. My brothers and I like to go on long walks in the mountains. My sister likes to cook with my grandmother. On the weekends we all play board games together. We laugh and always have a good time. I love my family very much.\n**************************************** spaCy ****************************************\n\n\n/Users/aherencia/Documents/Projects/Splitter_MR/.venv/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n  warnings.warn(Warnings.W108)\n\n\nchunks=['My Wonderful Family\\nI live in a house near the mountains.', 'I have two brothers and one sister, and I was born last.', 'My father teaches mathematics, and my mother is a nurse at a big hospital.', 'My brothers are very smart and work hard in school.', 'My sister is a nervous girl, but she is very kind.\\n\\nMy grandmother also lives with us.', 'She came from Italy when I was two years old.\\n\\nShe has grown old, but she is still very strong.', 'She cooks the best food!\\n\\n\\n\\nMy family i\n...\nuy grande, un comedor con una mesa y seis sillas, un sal\u00f3n con dos sof\u00e1s verdes, una televisi\u00f3n y cortinas. Adem\u00e1s, tiene una peque\u00f1a terraza con piscina donde puedo tomar el sol en verano.\n\nMe gusta mucho mi casa porque puedo invitar a mis amigos a cenar o a ver el f\u00fatbol en mi televisi\u00f3n. Adem\u00e1s, cerca de mi casa hay muchas tiendas para hacer la compra, como panader\u00eda, carnicer\u00eda y pescader\u00eda.\n\n\n**************************************** Spacy in Spanish ****************************************\n\n\nCreated a chunk of size 107, which is longer than the specified 100\nCreated a chunk of size 142, which is longer than the specified 100\nCreated a chunk of size 107, which is longer than the specified 100\nCreated a chunk of size 142, which is longer than the specified 100\n\n\n======================================== Chunk 1 ========================================\nMi nueva casa\nYo vivo en Granada, una ciudad peque\u00f1a que tiene monumentos muy importantes como la Alhambra.\n\n======================================== Chunk 2 ========================================\nAqu\u00ed la comida es deliciosa y son famosos el gazpacho, el rebujito y el salmorejo.\n\n======================================== Chunk 3 ========================================\nMi nueva casa est\u00e1 en una calle ancha\n...\n==========================\nAdem\u00e1s, tiene una peque\u00f1a terraza con piscina donde puedo tomar el sol en verano.\n\n======================================== Chunk 7 ========================================\nMe gusta mucho mi casa porque puedo invitar a mis amigos a cenar o a ver el f\u00fatbol en mi televisi\u00f3n.\n\n======================================== Chunk 8 ========================================\nAdem\u00e1s, cerca de mi casa hay muchas tiendas para hacer la compra, como panader\u00eda, carnicer\u00eda y pescader\u00eda.\n</code></pre>"},{"location":"examples/text/token_splitter/#available-models","title":"Available models","text":"<p>There are several tokenizer models that you can use to split your text. In the following table is provided a summary of the models that you can currently use, among with some implementation examples:</p> Library Model identifier/template How to implement Reference Guide NLTK (Punkt) <code>&lt;language&gt;</code> See NLTK Example NLTK Tokenizers Tiktoken <code>&lt;encoder&gt;</code> See Tiktoken Example tiktoken spaCy <code>{CC}_core_web_sm</code>,<code>{CC}_core_web_md</code>,<code>{CC}_core_web_lg</code>,<code>{CCe}_core_web_trf</code>,<code>xx_ent_wiki_sm</code>,<code>xx_sent_ud_sm</code> See spaCy Example spaCy Models <p>spaCy Model Suffixes: - <code>sm</code> (small): Fastest, small in size, less accurate; good for prototyping and lightweight use-cases. - <code>md</code> (medium): Medium size and accuracy; balances speed and performance. - <code>lg</code> (large): Largest and most accurate pipeline with the most vectors; slower and uses more memory. - <code>trf</code> (transformer): Uses transformer-based architectures (e.g., BERT, RoBERTa); highest accuracy, slowest, and requires more resources.</p>"},{"location":"examples/text/token_splitter/#nltk-example","title":"NLTK Example","text":"<pre><code>language = \"english\"\nTokenSplitter(model_name=\"nltk/punkt\", language=language)\n</code></pre> <pre><code>&lt;splitter_mr.splitter.splitters.token_splitter.TokenSplitter at 0x3106e42c0&gt;\n</code></pre>"},{"location":"examples/text/token_splitter/#tiktoken-example","title":"Tiktoken Example","text":"<pre><code>encoder = \"cl100k_base\"\nTokenSplitter(model_name=f\"tiktoken/{encoder}\")\n</code></pre> <pre><code>&lt;splitter_mr.splitter.splitters.token_splitter.TokenSplitter at 0x315f1d7c0&gt;\n</code></pre>"},{"location":"examples/text/token_splitter/#spacy-example","title":"spaCy Example","text":"<pre><code>CC = \"en\"\next = \"sm\"\nencoder = f\"{CC}_core_web_{ext}\"\nTokenSplitter(model_name=f\"spacy/{encoder}\")\n</code></pre> <pre><code>&lt;splitter_mr.splitter.splitters.token_splitter.TokenSplitter at 0x3280d74a0&gt;\n</code></pre>"}]}